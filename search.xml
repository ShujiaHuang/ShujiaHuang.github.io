<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[从零开始完整学习全基因组测序（WGS）数据分析：第4节 构建WGS主流程]]></title>
      <url>/2017/09/19/2017-09-19-Begining-WGS-Data-Analysis-The-pipeline.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0400_cover.jpg" alt="wgs04"></p>
<p>这篇文章很长，超过1万字，是本系列中最重要的一篇，因为我并非只是在简单地告诉大家几条硬邦邦的操作命令。对于新手而言不建议碎片时间阅读，对于有一定经验的老手来说，相信依然可以有所收获。在开始之前，我想先说一句：<strong>流程的具体形式其实是次要的，WGS本质上只是一个技术手段，重要的是，我们要明白自己所要解决的问题是什么，所希望获取的结果是什么，然后再选择合适的技术</strong>。这是许多人经常忽视的一个重要问题。</p>
<p>好了，以下进入正文。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0401.pipeline.png" alt="wgs-pipeline"></p>
<p>这是WGS数据分析的流程图。流程的目的是准确检测出每个样本（这里特指人）基因组中的变异集合，也就是人与人之间存在差异的那些DNA序列。我把整个分析过程按照它们实际要完成的功能，将其分成了三个大的模块：</p>
<ul>
<li>原始数据质控</li>
<li>数据预处理</li>
<li>变异检测</li>
</ul>
<p>这或许和很多人看到的WGS分析流程，在结构梳理上有些差异（比如GATK的最佳实践），但过程中的各个步骤和所要完成的事情是一模一样的。</p>
<h2 id="0-准备阶段"><a href="#0-准备阶段" class="headerlink" title="0.准备阶段"></a>0.准备阶段</h2><p>在开始之前，我们需要做一些准备工作，主要是部署好相关的软件和工具。我们在这个WGS数据分析过程中用到的所有软件都是开源的，它们的代码全部都能够在github上找到，具体如下：</p>
<ul>
<li><a href="https://github.com/lh3/bwa" target="_blank" rel="external">BWA（Burrow-Wheeler Aligner）</a>: 这是最权威，使用最广的NGS数据比对软件，目前已经更新到0.7.16版本；</li>
<li><a href="https://github.com/samtools/samtools" target="_blank" rel="external">Samtools</a>: 是一个专门用于处理比对数据的工具，由BWA的作者（lh3）所编写；</li>
<li><a href="http://broadinstitute.github.io/picard/" target="_blank" rel="external">Picard</a>: 它是目前最著名的组学研究中心-Broad研究所开发的一款强大的NGS数据处理工具，功能方面和Samtools有些重叠，但更多的是互补，它是由java编写的，我们直接下载最新的.jar包就行了。</li>
<li><a href="https://software.broadinstitute.org/gatk/download/" target="_blank" rel="external">GATK</a>: 同样是Broad研究所开发的，是目前业内最权威、使用最广的基因数据变异检测工具。值得注意的是，目前GATK有3.x和4.x两个不同的版本，代码在github上也是分开的。<strong>4.x是今年新推出的，在核心算法层面并没太多的修改，但使用了新的设计模式，做了很多功能的整合，是更适合于大规模集群和云运算的版本，后续GATK团队也将主要维护4.x的版本，而且它的代码是100%开源的，这和3.x只有部分开源的情况不同。看得出GATK今年的这次升级是为了应对接下来越来越多的大规模人群测序数据而做出的改变，但现阶段4.x版本还不稳定，真正在使用的人和机构其实也还不多</strong>。短期来看，3.x版本还将在业内继续使用一段时间；其次，<strong>3.x对于绝大分部的分析需求来说是完全足够的</strong>。我们在这里也以GATK3.8（最新版本）作为流程的重要工具进行分析流程的构建。</li>
</ul>
<p>事实上，对于构造WGS分析流程来说，以上这个四个工具就完全足够了。它们的安装都非常简单，除了BWA和Samtools由C编写的，安装时需要进行编译之外，另外两个只要保证系统中的java是1.8.x版本及以上的，那么直接下载jar包就可以使用了。操作系统方面推荐linux（集群）或者Mac OS。</p>
<h2 id="1-原始数据质控"><a href="#1-原始数据质控" class="headerlink" title="1.原始数据质控"></a>1.原始数据质控</h2><p>数据的质控，由于我已经在<a href="http://www.huangshujia.me/2017/08/25/2017-08-25-Begining-WGS-Data-Analysis-Fastq-Data-Quality-Control.html" target="_blank" rel="external">上一节</a>的文章中讲的比较详细了，因此在本篇中就不再进行详细的讨论了。而且质控的处理方法都是比较一致的，基本不需要为特定的分析做定制化的改动，因此，我们可以把它作为WGS主流程之外的一环。但还是再强调一下，数据质控的地位同样重要，不然我也不必专门为其单独写一篇完整的文章。</p>
<h2 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2.数据预处理"></a>2.数据预处理</h2><h3 id="序列比对"><a href="#序列比对" class="headerlink" title="序列比对"></a>序列比对</h3><p>先问一个问题：为什么需要比对？</p>
<p>我们已经知道NGS测序下来的短序列（read）存储于FASTQ文件里面。虽然它们原本都来自于有序的基因组，但在经过DNA建库和测序之后，文件中不同read之间的前后顺序关系就已经全部丢失了。因此，FASTQ文件中紧挨着的两条read之间没有任何位置关系，它们都是随机来自于原本基因组中某个位置的短序列而已。</p>
<p>因此，我们需要先把这一大堆的短序列捋顺，一个个去跟该物种的 <strong>参考基因组【注】</strong>比较，找到每一条read在参考基因组上的位置，然后按顺序排列好，这个过程就称为测序数据的比对。这 <strong>也是核心流程真正意义上的第一步</strong>，只有完成了这个序列比对我们才有下一步的数据分析。</p>
<blockquote>
<p>【注】参考基因组：指该物种的基因组序列，是已经组装成的完整基因组序列，常作为该物种的标准参照物，比如人类基因组参考序列，fasta格式。</p>
</blockquote>
<p>序列比对本质上是一个寻找最大公共子字符串的过程。大家如果有学过生物信息学的话，应该或多或少知道BLAST，它使用的是动态规划的算法来寻找这样的子串，但在面对巨量的短序列数据时，类似BLAST这样的软件实在太慢了！因此，需要更加有效的数据结构和相应的算法来完成这个搜索定位的任务。</p>
<p>我们这里将用于流程构建的BWA就是其中最优秀的一个，它将BW(Burrows-Wheeler)压缩算法和后缀树相结合，能够让我们以较小的时间和空间代价，获得准确的序列比对结果。</p>
<p>以下我们就开始流程的搭建。</p>
<p>首先，我们需要为参考基因组的构建索引——这其实是在为参考序列进行Burrows Wheeler变换（wiki: 块排序压缩），以便能够在序列比对的时候进行快速的搜索和定位。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bwa index human.fasta</div></pre></td></tr></table></figure>
<p>以我们人类的参考基因组（3Gb长度）为例，这个构造过程需要消耗几个小时的时间（一般3个小时左右）。完成之后，你会看到类似如下几个以human.fasta为前缀的文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">.</div><div class="line">├── human.fasta.amb</div><div class="line">├── human.fasta.ann</div><div class="line">├── human.fasta.bwt</div><div class="line">├── human.fasta.pac</div><div class="line">└── human.fasta.sa</div></pre></td></tr></table></figure>
<p>这些就是在比对时真正需要被用到的文件。这一步完成之后，我们就可以将read比对至参考基因组了：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bwa mem -t 4 -R <span class="string">'@RG\tID:foo_lane\tPL:illumina\tLB:library\tSM:sample_name'</span> /path/to/human.fasta read_1.fq.gz read_2.fq.gz &gt; sample_name.sam</div></pre></td></tr></table></figure>
<p>大伙如果以前没使用过这个比对工具的话，那么可能不明白上面参数的含义。我们这里调用的是bwa的mem比对模块，在解释这样做之前，我们不妨先看一下bwa mem的官方用法说明，它就一句话：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Usage: bwa mem [options] &lt;idxbase&gt; &lt;in1.fq&gt; [in2.fq]</div></pre></td></tr></table></figure>
<p>其中，<strong>[options]</strong>是一系列可选的参数，暂时不多说。这里的 <strong>&lt; idxbase&gt;</strong>要输入的是参考基因组的BW索引文件，我们上面通过<code>bwa index</code>构建好的那几个以human.fasta为前缀的文件便是；<strong>&lt; in1.fq&gt;</strong>和 <strong>[in2.fq]</strong>输入的是质控后的fastq文件。但这里输入的时候为什么会需要两个fq（in1.fq和in2.fq）呢？我们上面的例子也是有两个：read_1.fq.gz和read_2.fq.gz。这是因为这是双末端测序（也称Pair-End）的情况，那什么是“双末端测序”呢？这两个fq之间的关系又是什么？这个我需要简单解析一下。</p>
<p>我们已经知道NGS是短读长的测序技术，一次测出来的read的长度都不会太长，那为了尽可能把一个DNA序列片段尽可能多地测出来，既然测一边不够，那就测两边，于是就有了一种从被测DNA序列两端各测序一次的模式，<strong>这就被称为双末端测序（Pair-End Sequencing，简称PE测序）</strong>。如下图是Pair-End测序的示意图，中间灰色的是被测序的DNA序列片段，左边黄色带箭头和右边蓝色带箭头的分别是测序出来的read1和read2序列，这里假定它们的长度都是100bp。虽然很多时候Pair-End测序还是无法将整个被测的DNA片段完全测通，但是它依然提供了极其有用的信息，比如，我们知道每一对的read1和read2都来自于同一个DNA片段，read1和read2之间的距离是这个DNA片段的长度，而且read1和read2的方向刚好是相反的（这里排除mate-pair的情形）等，这些信息对于后面的变异检测等分析来说都是非常有用的。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0402.pair-end.png" alt="Pair-End 测序"></p>
<center>Pair-End 测序</center>

<p>另外，在read1在fq1文件中位置和read2在fq2文件中的文件中的位置是相同的，而且read ID之间只在末尾有一个’/1’或者’/2’的差别。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0403.png" alt="read1 ID和read2 ID的差别"></p>
<center>read1 ID和read2 ID的差别</center>

<p>既然有双末端测序，那么与之对应的就有单末端测序（Single End Sequecing，简称SE测序），即只测序其中一端。因此，我们在使用bwa比对的时候，实际上，in2.fq是非强制性的（所以用方括号括起来），只有是双末端测序的数据时才需要添加。</p>
<p>回到上面我们的例子，大伙可以看到我这里除了用法中提到的参数之外，还多了2个额外的参数，分别是：-t，线程数，我们在这里使用4个线程；-R 接的是 <strong>Read Group的字符串信息，这是一个非常重要的信息</strong>，以@RG开头，它是用来将比对的read进行分组的。不同的组之间测序过程被认为是相互独立的，这个信息对于我们后续对比对数据进行错误率分析和Mark duplicate时非常重要。在Read Group中，有如下几个信息非常重要：</p>
<p>(1) ID，这是Read Group的分组ID，一般设置为测序的lane ID（不同lane之间的测序过程认为是独立的），下机数据中我们都能看到这个信息的，一般都是包含在fastq的文件名中；</p>
<p>(2) PL，<strong>指的是所用的测序平台，这个信息不要随便写！</strong>特别是当我们需要使用GATK进行后续分析的时候，更是如此！这是一个很多新手都容易忽视的一个地方，在GATK中，PL只允许被设置为：ILLUMINA，SLX，SOLEXA，SOLID，454，LS454，COMPLETE，PACBIO，IONTORRENT，CAPILLARY，HELICOS或UNKNOWN这几个信息。基本上就是目前市场上存在着的测序平台，当然，如果实在不知道，那么必须设置为UNKNOWN，名字方面不区分大小写。如果你在分析的时候这里没设置正确，那么在后续使用GATK过程中可能会碰到类似如下的错误：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ERROR MESSAGE: The platform (xx) associated with <span class="built_in">read</span> group GATKSAMReadGroupRecord @RG:xx is not a recognized platform.</div></pre></td></tr></table></figure>
<p>这个时候你需要对比对文件的header信息进行重写，就会稍微比较麻烦。</p>
<p>我们上面的例子用的是<code>PL:illumina</code>。如果你的数据是CG测序的那么记得不要写成CG！而要写<code>COMPLETE</code>。</p>
<p>(3) SM，样本ID，同样非常重要，有时候我们测序的数据比较多的时候，那么可能会分成多个不同的lane分布测出来，这个时候SM名字就是可以用于区分这些样本。</p>
<p>(4) LB，测序文库的名字，这个重要性稍微低一些，主要也是为了协助区分不同的group而存在。文库名字一般可以在下机的fq文件名中找到，如果上面的lane ID足够用于区分的话，也可以不用设置LB；</p>
<p>除了以上这四个之外，还可以自定义添加其他的信息，不过如无特殊的需要，对于序列比对而言，这4个就足够了。这些信息设置好之后，<strong>在RG字符串中要用制表符（\t）将它们分开</strong>。</p>
<p>最后在我们的例子中，我们将比对的输出结果直接重定向到一份sample_name.sam文件中，这类文件是BWA比对的标准输出文件，它的具体格式我会在下一篇文章中进行详细说明。但SAM文件是文本文件，一般整个文件都非常巨大，因此，为了有效节省磁盘空间，一般都会用samtools将它转化为BAM文件（SAM的特殊二进制格式），而且BAM会更加方便于后续的分析。所以我们上面比对的命令可以和samtools结合并改进为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ bwa mem -t 4 -R <span class="string">'@RG\tID:foo_lane\tPL:illumina\tLB:library\tSM:sample_name'</span> /path/to/human.fasta read_1.fq.gz read_2.fq.gz | samtools view -S -b - &gt; sample_name.bam</div></pre></td></tr></table></figure>
<p>我们通过管道(“|”)把比对的输出如同引导水流一样导流给samtools去处理，上面<code>samtools view</code>的-b参数指的就是输出为BAM文件，这里需要注意的地方是-b后面的’-‘，它代表就是上面管道引流过来的数据，经过samtools转换之后我们再重定向为sample_name.bam。</p>
<p>关于BWA的其他参数，我这里不打算对其进行一一解释，在绝大多数情况下，采用默认是合适的做法。</p>
<blockquote>
<p>[Tips] BWA MEM比对模块是有一定适用范围的：它是专门为长read比对设计的，目的是为了解决，第三代测序技术这种能够产生长达几十kb甚至几Mbp的read情况。一般只有当read长度≥70bp的时候，才推荐使用，如果比这个要小，建议使用BWA ALN模块。</p>
</blockquote>
<h3 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h3><p>以上，我们就完成了read比对的步骤。接下来是排序：</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0404.preprogress.sorted.png" alt="排序"></p>
<p>排序这一步我们也是通过使用samtools来完成的，命令很简单：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Usage: samtools sort [options...] [in.bam]</div></pre></td></tr></table></figure>
<p>但在执行之前，我们有必要先搞明白为什么需要排序，为什么BWA比对后输出的BAM文件是没顺序的！原因就是FASTQ文件里面这些被测序下来的read是随机分布于基因组上面的，第一步的比对是按照FASTQ文件的顺序把read逐一定位到参考基因组上之后，随即就输出了，它不会也不可能在这一步里面能够自动识别比对位置的先后位置重排比对结果。因此，比对后得到的结果文件中，每一条记录之间位置的先后顺序是乱的，我们后续去重复等步骤都需要在比对记录按照顺序从小到大排序下来才能进行，所以这才是需要进行排序的原因。对于我们的例子来说，这个排序的命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ time samtools sort -@ 4 -m 4G -O bam -o sample_name.sorted.bam sample_name.bam</div></pre></td></tr></table></figure>
<p>其中，-@，用于设定排序时的线程数，我们设为4；-m，限制排序时最大的内存消耗，这里设为4GB；-O 指定输出为bam格式；-o 是输出文件的名字，这里叫sample_name.sorted.bam。我会比较建议大伙在做类似分析的时候在文件名字将所做的关键操作包含进去，因为这样即使过了很长时间，当你再去看这个文件的时候也能够立刻知道当时对它做了什么；最后就是输入文件——sample_name.bam。</p>
<blockquote>
<p>【注意】排序后如果发现新的BAM文件比原来的BAM文件稍微小一些，不用觉得惊讶，这是压缩算法导致的结果，文件内容是没有损失的。</p>
</blockquote>
<h3 id="去除重复序列（或者标记重复序列）"><a href="#去除重复序列（或者标记重复序列）" class="headerlink" title="去除重复序列（或者标记重复序列）"></a>去除重复序列（或者标记重复序列）</h3><p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0404.preprogress.rmdup.png" alt="去除重复序列"></p>
<p>在排序完成之后我们就可以开始执行去除重复（准确来说是 <strong>去除PCR重复序列</strong>）的步骤了。</p>
<p>首先，我们需要先理解什么是重复序列，它是如何产生的，以及为什么需要去除掉？要回答这几个问题，我们需要再次理解在建库和测序时到底发生了什么。</p>
<p>我们在<a href="http://www.huangshujia.me/2017/08/04/2017-08-04-Begining-WGS-Data-Analysis-Sequecing-Tech.html" target="_blank" rel="external">第1节</a>中已经知道，在NGS测序之前都需要先构建测序文库：通过物理（超声）打断或者化学试剂（酶切）切断原始的DNA序列，然后选择特定长度范围的序列去进行PCR扩增并上机测序。</p>
<p>因此，这里重复序列的来源实际上就是由PCR过程中所引入的。因为所谓的PCR扩增就是把原来的一段DNA序列复制多次。<strong>可是为什么需要PCR扩增呢？如果没有扩增不就可以省略这一步了吗？</strong></p>
<p>情况确实如此，但是很多时候我们构建测序文库时能用的细胞量并不会非常充足，而且在打断的步骤中也会引起部分DNA的降解，这两点会使整体或者局部的DNA浓度过低，这时如果直接从这个溶液中取样去测序就很可能漏掉原本基因组上的一些DNA片段，导致测序不全。<strong>而PCR扩增的作用就是为了把这些微弱的DNA多复制几倍乃至几十倍，以便增大它们在溶液中分布的密度，使得能够在取样时被获取到</strong>。所以这里大家需要记住一个重点，PCR扩增原本的目的是为了增大微弱DNA序列片段的密度，但由于整个反应都在一个试管中进行，因此其他一些密度并不低的DNA片段也会被同步放大，那么这时在取样去上机测序的时候，这些DNA片段就很可能会被重复取到相同的几条去进行测序（下图为PCR扩增示意图）。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0405.pcr.png" alt="PCR扩增示意图"></p>
<center><em>PCR扩增示意图：PCR扩增是一个指数扩增的过程，图中原本只有一段双链DNA序列，在经过3轮PCR后就被扩增成了8段</em></center>

<p>看到这里，你或许会觉得，那没必要去除不也应该可以吗？因为即便扩增了多几次，不也同样还是原来的那一段DNA吗？直接用来分析对结果也不会有影响啊！难道不是吗？</p>
<p><strong>会有影响，而且有时影响会很大！</strong>最直接的后果就是同时增大了变异检测结果的假阴和假阳率。主要有几个原因：</p>
<ul>
<li>DNA在打断的那一步会发生一些损失，主要表现是会引发一些碱基发生颠换变换（嘌呤-变嘧啶或者嘧啶变嘌呤），带来假的变异。PCR过程会扩大这个信号，导致最后的检测结果中混入了假的结果；</li>
<li>PCR反应过程中也会带来新的碱基错误。发生在前几轮的PCR扩增发生的错误会在后续的PCR过程中扩大，同样带来假的变异；</li>
<li>对于真实的变异，PCR反应可能会对包含某一个碱基的DNA模版扩增更加剧烈（这个现象称为PCR Bias）。如果反应体系是对含有reference allele的模板扩增偏向强烈，那么变异碱基的信息会变小，从而会导致假阴。</li>
</ul>
<p><strong>PCR对真实的变异检测和个体的基因型判断都有不好的影响</strong>。GATK、Samtools、Platpus等这种利用贝叶斯原理的变异检测算法都是认为所用的序列数据都不是重复序列（即将它们和其他序列一视同仁地进行变异的判断，所以带来误导），因此必须要进行标记（去除）或者使用PCR-Free的测序方案（这个方案目前正变得越来越流行，特别是对于RNA-Seq来说尤为重要，现在著名的基因组学研究所——Broad Institute，基本都是使用PCR-Free的测序方案）。</p>
<p>那么具体是如何做到去除这些PCR重复序列的呢？我们可以抛开任何工具，仔细想想，既然PCR扩增是把同一段DNA序列复制出很多份，那么这些序列在经过比对之后它们一定会定位到基因组上相同的位置，比对的信息看起来也将是一样的！于是，我们就可以根据这个特点找到这些重复序列了！</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0406.dup.png" alt="重复性序列"></p>
<p>事实上，现有的工具包括Samtools和Picard中去除重复序列的算法也的确是这么做的。不同的地方在于，samtools的rmdup是直接将这些重复序列从比对BAM文件中删除掉，而Picard的MarkDuplicates默认情况则只是在BAM的FLAG信息中标记出来，而不是删除，因此这些重复序列依然会被留在文件中，只是我们可以在变异检测的时候识别到它们，并进行忽略。</p>
<p>考虑到尽可能和现在主流的做法一致（但我并不是说主流的做法就一定是对的，要分情况看待，只是主流的做法容易被做成生产流程而已），我们这里也用Picard来完成这个事情：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">java -jar picard.jar MarkDuplicates \ </div><div class="line">  I=sample_name.sorted.bam \</div><div class="line">  O=sample_name.sorted.markdup.bam \</div><div class="line">  M=sample_name.markdup_metrics.txt</div></pre></td></tr></table></figure>
<p>这里只把重复序列在输出的新结果中标记出来，但不删除。如果我们非要把这些序列完全删除的话可以这样做：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">java -jar picard.jar MarkDuplicates \ </div><div class="line">  REMOVE_DUPLICATES=<span class="literal">true</span> \</div><div class="line">  I=sample_name.sorted.bam \</div><div class="line">  O=sample_name.sorted.markdup.bam \</div><div class="line">  M=sample_name.markdup_metrics.txt</div></pre></td></tr></table></figure></p>
<p>把参数<code>REMOVE_DUPLICATES</code>设置为ture，那么重复序列就被删除掉，不会在结果文件中留存。我比较建议使用第一种做法，只是标记出来，并留存这些序列，以便在你需要的时候还可以对其做分析。</p>
<p>这一步完成之后，我们需要为sample_name.sorted.markdup.bam创建索引文件，它的作用能够让我们可以随机访问这个文件中的任意位置，而且后面的“局部重比对”步骤也要求这个BAM文件一定要有索引，命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ samtools index sample_name.sorted.markdup.bam</div></pre></td></tr></table></figure>
<p>完成之后，会生成一份sample_name.sorted.markdup.bam.bai文件，这就是上面这份BAM的index。</p>
<h3 id="局部重比对"><a href="#局部重比对" class="headerlink" title="局部重比对"></a>局部重比对</h3><p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0407.realign.png" alt="局部重比对"></p>
<p>接下来是局部区域重比对，通常也叫Indel局部区域重比对。有时在进行这一步骤之前还有一个merge的操作，将同个样本的所有比对结果合并成唯一一个大的BAM文件【注】，merge的例子如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ samtools merge &lt;out.bam&gt; &lt;in1.bam&gt; [&lt;in2.bam&gt; ... &lt;inN.bam&gt;]</div></pre></td></tr></table></figure>
<blockquote>
<p>【注意】之所以会有这种情况，是因为有些样本测得非常深，其测序结果需要经过多次测序（或者分布在多个不同的测序lane中）才全部获得，这个时候我们一般会先分别进行比对并去除重复序列后再使用samtools进行合并。</p>
</blockquote>
<p>局部重比对的目的是将BWA比对过程中所发现有 <strong>潜在序列插入或者序列删除（insertion和deletion，简称Indel）的区域进行重新校正</strong>。这个过程往往还会把一些已知的Indel区域一并作为重比对的区域，但为什么需要进行这个校正呢？</p>
<p><strong>其根本原因来自于参考基因组的序列特点和BWA这类比对算法本身，注意这里不是针对BWA，而是针对所有的这类比对算法，包括bowtie等</strong>。这类在全局搜索最优匹配的算法在存在Indel的区域及其附近的比对情况往往不是很准确，特别是当一些存在长Indel、重复性序列的区域或者存在长串单一碱基（比如，一长串的TTTT或者AAAAA等）的区域中更是如此。</p>
<p><strong>另一个重要的原因是在这些比对算法中，对碱基错配和开gap的容忍度是不同的</strong>。具体体现在罚分矩阵的偏向上，例如，在read比对时，如果发现碱基错配和开gap都可以的话，它们会更偏向于错配。但是这种偏向错配的方式，有时候却还会反过来引起错误的开gap！<strong>这就会导致基因组上原本应该是一个长度比较大的Indel的地方，被错误地切割成多个错配和短indel的混合集，这必然会让我们检测到很多错误的变异</strong>。而且，这种情况还会随着所比对的read长度的增长（比如三代测序的Read，通常都有几十kbp）而变得越加严重。</p>
<p>因此，我们需要有一种算法来对这些区域进行局部的序列重比对。这个算法通常就是大名鼎鼎的Smith-Waterman算法，它非常适合于这类场景，可以极其有效地实现对全局比对结果的校正和调整，最大程度低地降低由全局比对算法的不足而带来的错误。<strong>而且GATK的局部重比对模块，除了应用这个算法之外，还会对这个区域中的read进行一次局部组装，把它们连接成为长度更大的序列，这样能够更进一步提高局部重比对的准确性。</strong></p>
<p>下图给大家展示一个序列重比对之前和之后的结果，其中灰色的横条指的是read，空白黑线指的是deletion，有颜色的碱基指的是错配碱基。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0408.realign.png" alt="Indel局部重比对的前后的对比"></p>
<center>Indel局部重比对的前后的对比</center>

<p>相信大家都可以明显地看到在序列重比对之前，在这个区域的比对数据是多么的糟糕，如果就这样进行变异检测，那么一定会得到很多假的结果。而在经过局部重比对之后，这个区域就变得非常清晰而分明，它原本发生的就只是一个比较长的序列删除（deletion）事件，但在原始的比对结果中却被错误地用碱基错配和短的Indel所代替。</p>
<p>说到这里，那么具体该怎么做呢？我们的WGS分析流程从这个步骤开始就需要用到GATK (GenomeAnalysisTK.jar)了，我们的执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">java -jar /path/to/GenomeAnalysisTK.jar \</div><div class="line"> -T RealignerTargetCreator \</div><div class="line"> -R /path/to/human.fasta \</div><div class="line"> -I sample_name.sorted.markdup.bam \</div><div class="line"> -known /path/to/gatk/bundle/1000G_phase1.indels.b37.vcf \</div><div class="line"> -known /path/to/gatk/bundle/Mills_and_1000G_gold_standard.indels.b37.vcf \</div><div class="line"> -o sample_name.IndelRealigner.intervals</div><div class="line"> </div><div class="line">java -jar /path/to/GenomeAnalysisTK.jar \</div><div class="line"> -T IndelRealigner \</div><div class="line"> -R /path/to/human.fasta \</div><div class="line"> -I sample_name.sorted.markdup.bam \</div><div class="line"> -known /path/to/gatk/bundle/1000G_phase1.indels.b37.vcf \</div><div class="line"> -known /path/to/gatk/bundle/Mills_and_1000G_gold_standard.indels.b37.vcf \</div><div class="line"> -o sample_name.sorted.markdup.realign.bam \</div><div class="line"> --targetIntervals sample_name.IndelRealigner.intervals</div></pre></td></tr></table></figure>
<p>这里包含了两个步骤：</p>
<ul>
<li>第一步，RealignerTargetCreator ，目的是定位出所有需要进行序列重比对的目标区域（如下图）；</li>
<li>第二步，IndelRealigner，对所有在第一步中找到的目标区域运用算法进行序列重比对，最后得到捋顺了的新结果。</li>
</ul>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0409.realign.png" alt="IndelRealigner.intervals文件内容示例"></p>
<center>IndelRealigner.intervals文件内容示例</center>

<p>以上这两个步骤是缺一不可的，顺序也是固定的。而且，需要指出的是，<strong>这里的-R参数输入的human.fasta不是BWA比对中的索引文件前缀，而是参考基因组序列（FASTA格式）文件，下同。</strong></p>
<p>另外，在重比对步骤中，我们还看到了两个陌生的VCF文件，分别是：1000G_phase1.indels.b37.vcf和Mills_and_1000G_gold_standard.indels.b37.vcf。这两个文件来自于千人基因组和Mills项目，里面记录了那些项目中检测到的人群Indel区域。我上面其实也提到了，<strong>候选的重比对区除了要在样本自身的比对结果中寻找之外，还应该把人群中已知的Indel区域也包含进来，而这两个是我们在重比对过程中最常用到的。</strong>这些文件你可以很方便地在<a href="ftp://ftp.broadinstitute.org/bundle/" target="_blank" rel="external">GATK bundle ftp</a>中下载，注意一定要选择和你的参考基因组对应的版本，我们这里用的是b37版本。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0410.gatk-bundle.png" alt="GATK bundle"></p>
<center>GATK bundle</center>

<p><strong>那么既然Indel局部重比对这么好，这么重要，似乎看起来在任何情况下都应该是必须的。然鹅，我的回答是否定的！</strong>惊讶吗！</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0411.png" alt="友谊的小船"></p>
<p>但否定是有前提的！<strong>那就是我们后面的变异检测必须是使用GATK，而且必须使用GATK的HaplotypeCaller模块，仅当这个时候才可以减少这个Indel局部重比对的步骤</strong>。原因是GATK的HaplotypeCaller中，会对潜在的变异区域进行相同的局部重比对！但是其它的变异检测工具或者GATK的其它模块就没有这么干了！所以切记！</p>
<h3 id="重新校正碱基质量值（BQSR）"><a href="#重新校正碱基质量值（BQSR）" class="headerlink" title="重新校正碱基质量值（BQSR）"></a>重新校正碱基质量值（BQSR）</h3><p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0412.BQSR.png" alt="BQSR"></p>
<p><strong>在WGS分析中，变异检测是一个极度依赖测序碱基质量值的步骤。因为这个质量值是衡量我们测序出来的这个碱基到底有多正确的重要（甚至是唯一）指标</strong>。它来自于测序图像数据的base calling。因此，基本上是由测序仪和测序系统来决定的。但不幸的是，影响这个值准确性的系统性因素有很多，包括物理和化学等对测序反应的影响，甚至连仪器本身和周围环境都是其重要的影响因素。当把所有这些东西综合在一起之后，往往会发现计算出来的碱基质量值要么高于真实结果，要么低于真实结果。那么，我们到底该如何才能获得符合真实情况的碱基质量值？</p>
<p>BQSR（Base Quality Score Recalibration）这个步骤就是为此而存在的，这一步同样非常重要。它主要是通过机器学习的方法构建测序碱基的错误率模型，然后对这些碱基的质量值进行相应的调整。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0413.BQSR.png" alt="BQSR质量校正对比"></p>
<center>BQSR质量校正对比</center>

<p>图中，横轴（Reported quality score）是测序结果在Base calling之后报告出来的质量值，也就是我们在FASTQ文件中看到的那些；纵轴（Empirical quality score）代表的是“真实情况的质量值”。</p>
<p>但是且慢，这个“真实情况的质量值”是怎么来的？因为实际上我们并没有办法直接测得它们啊！没错，确实没办法直接测量到，但是我们可以通过统计学的技巧获得极其接近的分布结果（因此我加了引号）。试想一下，<strong>如果我们在看到某一个碱基报告的质量值是20时，那么它的预期错误率是1%，反过来想，就等于是说如果有100个质量值都是20的碱基，那么从统计上讲它们中将只有1个是错的！做了这个等效变换之后，我们的问题就可以转变成为寻找错误碱基的数量了</strong>。</p>
<p>这时问题就简单多了。我们知道人与人之间的差异其实是很小的，那么在一个群体中发现的已知变异，在某个人身上也很可能是同样存在的。因此，这个时候我们可以对比对结果进行直接分析，首先排除掉所有的已知变异位点，<strong>然后计算每个（报告出来的）质量值下面有多少个碱基在比对之后与参考基因组上的碱基是不同的，这些不同碱基就被我们认为是错误的碱基，它们的数目比例反映的就是真实的碱基错误率</strong>，换算成Phred score（Phred score的定义可以参考<a href="http://www.huangshujia.me/2017/08/12/2017-08-12-Begining-WGS-Data-Analysis-Fasta-And-Fastq.html" target="_blank" rel="external">第2节</a>的相关内容）之后，就是纵轴的Empirical quality score了。</p>
<p>上面‘BQSR质量校正对比’的图中左边是原始质量值与真实质量值的比较，在这个图的例子中我们可以发现，base calling给出的质量值并没有正确地反映真实的错误率情况，测序报告出来的碱基质量值大部分被高估了，换句话说，就是错误率被低估了。</p>
<p>在我们的流程中，BQSR的具体执行命令如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">java -jar /path/to/GenomeAnalysisTK.jar \</div><div class="line"> -T BaseRecalibrator \</div><div class="line"> -R /path/to/human.fasta \</div><div class="line"> -I sample_name.sorted.markdup.realign.bam \</div><div class="line"> --knownSites /path/to/gatk/bundle/1000G_phase1.indels.b37.vcf \</div><div class="line"> --knownSites /path/to/gatk/bundle/Mills_and_1000G_gold_standard.indels.b37.vcf \</div><div class="line"> --knownSites /path/to/gatk/bundle/dbsnp_138.b37.vcf \</div><div class="line"> -o sample_name.recal_data.table</div><div class="line"> </div><div class="line">java -jar /path/to/GenomeAnalysisTK.jar \</div><div class="line"> -T PrintReads \</div><div class="line"> -R /path/to/human.fasta \</div><div class="line"> -I sample_name.sorted.markdup.realign.bam \</div><div class="line"> --BQSR sample_name.recal_data.table \</div><div class="line"> -o sample_name.sorted.markdup.realign.BQSR.bam</div></pre></td></tr></table></figure>
<p>这里同样包含了两个步骤：</p>
<ul>
<li>第一步，BaseRecalibrator，这里计算出了所有需要进行重校正的read和特征值，然后把这些信息输出为一份校准表文件（sample_name.recal_data.table）</li>
<li>第二步，PrintReads，这一步利用第一步得到的校准表文件（sample_name.recal_data.table）重新调整原来BAM文件中的碱基质量值，并使用这个新的质量值重新输出一份新的BAM文件。</li>
</ul>
<p><strong>注意，因为BQSR实际上是为了（尽可能）校正测序过程中的系统性错误，因此，在执行的时候是按照不同的测序lane或者测序文库来进行的，这个时候@RG信息（BWA比对时所设置的）就显得很重要了，算法就是通过@RG中的ID来识别各个独立的测序过程，这也是我开始强调其重要性的原因。</strong></p>
<h2 id="变异检测"><a href="#变异检测" class="headerlink" title="变异检测"></a>变异检测</h2><p><img src="http://blog-fungenomics-com.qiniudn.com/wgs0414.variant.png" alt="变异检测功能组合"></p>
<p>事实上，这是目前所有WGS数据分析流程的一个目标——获得样本准确的变异集合。这里变异检测的内容一般会包括：SNP、Indel，CNV和SV等，这个流程中我们只做其中最主要的两个：SNP和Indel。我们这里使用GATK HaplotypeCaller模块对样本中的变异进行检测，它也是目前最适合用于对二倍体基因组进行变异（SNP+Indel）检测的算法。</p>
<p>HaplotypeCaller和那些直接应用贝叶斯推断的算法有所不同，它会先推断群体的单倍体组合情况，计算各个组合的几率，然后根据这些信息再反推每个样本的基因型组合。因此它不但特别适合应用到群体的变异检测中，而且还能够依据群体的信息更好地计算每个个体的变异数据和它们的基因型组合。</p>
<p>一般来说，在实际的WGS流程中对HaplotypeCaller的应用有两种做法，差别只在于要不要在中间生成一个gVCF：</p>
<p>（1）直接进行HaplotypeCaller，这适合于单样本，或者那种固定样本数量的情况，也就是执行一次HaplotypeCaller之后就老死不相往来了。否则你会碰到仅仅只是增加一个样本就得重新运行这个HaplotypeCaller的坑爹情况（即，N+1难题），而这个时候算法需要重新去读取所有人的BAM文件，这将会是一个很费时间的痛苦过程；</p>
<p>（2）每个样本先各自生成gVCF，然后再进行群体joint-genotype。这其实就是GATK团队为了解决（1）中的N+1难题而设计出来的模式。gVCF全称是genome VCF，是每个样本用于变异检测的中间文件，格式类似于VCF，它把joint-genotype过程中所需的所有信息都记录在这里面，文件无论是大小还是数据量都远远小于原来的BAM文件。这样一旦新增加样本也不需要再重新去读取所有人的BAM文件了，只需为新样本生成一份gVCF，然后重新执行这个joint-genotype就行了。</p>
<p>我们先以第一种（直接HaplotypeCaller）做法为例子：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">java -jar /path/to/GenomeAnalysisTK.jar \</div><div class="line"> -T HaplotypeCaller \</div><div class="line"> -R /path/to/human.fasta \</div><div class="line"> -I sample_name.sorted.markdup.realign.BQSR.bam \</div><div class="line"> -D /path/to/gatk/bundle/dbsnp_138.b37.vcf \</div><div class="line"> -stand_call_conf 50 \ </div><div class="line"> -A QualByDepth \ </div><div class="line"> -A RMSMappingQuality \ </div><div class="line"> -A MappingQualityRankSumTest \ </div><div class="line"> -A ReadPosRankSumTest \ </div><div class="line"> -A FisherStrand \ </div><div class="line"> -A StrandOddsRatio \ </div><div class="line"> -A Coverage \</div><div class="line"> -o sample_name.HC.vcf</div></pre></td></tr></table></figure>
<p>这里我特别提一下-D参数输入的dbSNP同样可以再GATK bundle目录中找到，这份文件汇集的是目前几乎所有的公开人群变异数据集。另外，由于我们的例子只有一个样本因此只输入一个BAM文件就可以了，如果有多个样本那么可以继续用-I参数输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">java -jar GenomeAnalysisTK.jar \</div><div class="line">    -T HaplotypeCaller \</div><div class="line">     -R reference.fasta \</div><div class="line">     -I sample1.bam [-I sample2.bam ...] \</div><div class="line">     ...</div></pre></td></tr></table></figure>
<p>以上的命令是直接对全基因组做变异检测，这个过程会消耗很长的时间，通常需要几十个小时甚至几天。</p>
<p>然而，基因组上各个不同的染色体之间其实是可以理解为相互独立的（结构性变异除外），也就是说，为了提高效率我们可以按照染色体一条条来独立执行这个步骤，最后再把结果合并起来就好了，这样的话就能够节省很多的时间。下面我给出一个按照染色体区分的例子：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">java -jar /path/to/GenomeAnalysisTK.jar \</div><div class="line"> -T HaplotypeCaller \</div><div class="line"> -R /path/to/human.fasta \</div><div class="line"> -I sample_name.sorted.markdup.realign.BQSR.bam \</div><div class="line"> -D /path/to/gatk/bundle/dbsnp_138.b37.vcf \</div><div class="line"> -L 1 \</div><div class="line"> -stand_call_conf 50 \ </div><div class="line"> -A QualByDepth \ </div><div class="line"> -A RMSMappingQuality \ </div><div class="line"> -A MappingQualityRankSumTest \ </div><div class="line"> -A ReadPosRankSumTest \ </div><div class="line"> -A FisherStrand \ </div><div class="line"> -A StrandOddsRatio \ </div><div class="line"> -A Coverage \</div><div class="line"> -o sample_name.HC.1.vcf</div></pre></td></tr></table></figure>
<p>注意到了吗？其它参数都没任何改变，就只增加了一个 -L 参数，通过这个参数我们可以指定特定的染色体（或者基因组区域）！我们这里指定的是 1 号染色体，有些地方会写成chr1，具体看human.fasta中如何命名，与其保持一致即可。其他染色体的做法也是如此，就不再举例了。最后合并：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">java -jar /path/to/GenomeAnalysisTK.jar \</div><div class="line"> -T CombineVariants \</div><div class="line"> -R /path/to/human.fasta \</div><div class="line"> --genotypemergeoption UNSORTED \</div><div class="line"> --variant sample_name.HC.1.vcf \</div><div class="line"> --variant sample_name.HC.2.vcf \</div><div class="line"> ...</div><div class="line"> --variant sample_name.HC.MT.vcf \</div><div class="line"> -o sample_name.HC.vcf</div></pre></td></tr></table></figure>
<p>第二种，先产生gVCF，最后再joint-genotype的做法：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">java -jar /path/to/GenomeAnalysisTK.jar \</div><div class="line"> -T HaplotypeCaller \</div><div class="line"> -R /path/to/human.fasta \</div><div class="line"> -I sample_name.sorted.markdup.realign.BQSR.bam \</div><div class="line"> --emitRefConfidence GVCF \</div><div class="line"> -o sample_name.g.vcf</div><div class="line"> </div><div class="line"><span class="comment">#调用GenotypeGVCFs完成变异calling</span></div><div class="line">java -jar /path/to/GenomeAnalysisTK.jar \</div><div class="line"> -T GenotypeGVCFs \</div><div class="line"> -R /path/to/human.fasta \</div><div class="line"> --variant sample_name.g.vcf \</div><div class="line"> -o sample_name.HC.vcf</div></pre></td></tr></table></figure>
<p>其实，就是加了–emitRefConfidence GVCF的参数。而且，假如嫌慢，同样可以按照染色体或者区域去产生一个样本的gVCF，然后在GenotypeGVCFs中把它们全部作为输入文件完成变异calling。<strong>也许你会担心同个样本被分成多份gVCF之后，是否会被当作不同的多个样本？回答是不会！</strong>因为生成gVCF文件的过程中，GATK会根据@RG信息中的SM（也就是sample name）来判断这些gVCF是否来自同一个样本，如果名字相同，那么就会被认为是同一个样本，不会产生多样本问题。</p>
<h2 id="变异检测质控和过滤（VQSR）"><a href="#变异检测质控和过滤（VQSR）" class="headerlink" title="变异检测质控和过滤（VQSR）"></a>变异检测质控和过滤（VQSR）</h2><p>这是我们这个流程中最后的一步了。在获得了原始的变异检测结果之后，我们还需要做的就是质控和过滤。这一步或多或少都有着一些个性化的要求，我暂时就不做太多解释吧（一旦解释恐怕同样是一篇万字长文）。只用一句话来概括，VQSR是通过构建GMM模型对好和坏的变异进行区分，从而实现对变异的质控，具体的原理暂时不展开了。</p>
<p>下面就直接给出例子吧：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## SNP Recalibrator</span></div><div class="line">java -jar /path/to/GenomeAnalysisTK.jar \</div><div class="line">   -T VariantRecalibrator \</div><div class="line">   -R reference.fasta \</div><div class="line">   -input sample_name.HC.vcf \</div><div class="line">   -resource:hapmap,known=<span class="literal">false</span>,training=<span class="literal">true</span>,truth=<span class="literal">true</span>,prior=15.0 /path/to/gatk/bundle/hapmap_3.3.b37.vcf \ </div><div class="line">   -resource:omini,known=<span class="literal">false</span>,training=<span class="literal">true</span>,truth=<span class="literal">false</span>,prior=12.0 /path/to/gatk/bundle/1000G_omni2.5.b37.vcf \</div><div class="line">   -resource:1000G,known=<span class="literal">false</span>,training=<span class="literal">true</span>,truth=<span class="literal">false</span>,prior=10.0 /path/to/gatk/bundle/1000G_phase1.snps.high_confidence.b37.vcf \ </div><div class="line">   -resource:dbsnp,known=<span class="literal">true</span>,training=<span class="literal">false</span>,truth=<span class="literal">false</span>,prior=6.0 /path/to/gatk/bundle/dbsnp_138.b37.vcf \ </div><div class="line">   -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP \ </div><div class="line">   -mode SNP \ </div><div class="line">   -recalFile sample_name.HC.snps.recal \</div><div class="line">   -tranchesFile sample_name.HC.snps.tranches \ </div><div class="line">   -rscriptFile sample_name.HC.snps.plots.R</div><div class="line"></div><div class="line">java -jar /path/to/GenomeAnalysisTK.jar -T ApplyRecalibration \</div><div class="line">   -R  human_g1k_v37.fasta \</div><div class="line">   -input sample_name.HC.vcf \ </div><div class="line">   --ts_filter_level 99.5 \ </div><div class="line">   -tranchesFile sample_name.HC.snps.tranches \ </div><div class="line">   -recalFile sample_name.HC.snps.recal \</div><div class="line">   -mode SNP \</div><div class="line">   -o sample_name.HC.snps.VQSR.vcf</div><div class="line"></div><div class="line"><span class="comment">## Indel Recalibrator</span></div><div class="line">java -jar /path/to/GenomeAnalysisTK.jar -T VariantRecalibrator \</div><div class="line">   -R  human_g1k_v37.fasta \</div><div class="line">   -input sample_name.HC.snps.VQSR.vcf \</div><div class="line">   -resource:mills,known=<span class="literal">true</span>,training=<span class="literal">true</span>,truth=<span class="literal">true</span>,prior=12.0 /path/to/gatk/bundle/Mills_and_1000G_gold_standard.indels.b37.vcf \</div><div class="line">   -an QD -an DP -an FS -an SOR -an ReadPosRankSum -an MQRankSum \</div><div class="line">   -mode INDEL \</div><div class="line">   -recalFile sample_name.HC.snps.indels.recal \</div><div class="line">   -tranchesFile sample_name.HC.snps.indels.tranches \</div><div class="line">   -rscriptFile sample_name.HC.snps.indels.plots.R</div><div class="line"></div><div class="line">java -jar /path/to/GenomeAnalysisTK.jar -T ApplyRecalibration \ </div><div class="line">   -R human_g1k_v37.fasta\</div><div class="line">   -input sample_name.HC.snps.VQSR.vcf \</div><div class="line">   --ts_filter_level 99.0 \</div><div class="line">   -tranchesFile sample_name.HC.snps.indels.tranches \</div><div class="line">   -recalFile sample_name.HC.snps.indels.recal \</div><div class="line">   -mode INDEL \</div><div class="line">   -o sample_name.HC.snps.indels.VQSR.vcf</div></pre></td></tr></table></figure>
<p>最后，<code>sample_name.HC.snps.indels.VQSR.vcf</code>便是我们最终的变异检测结果。对于人类而言，一般来说，每个人最后检测到的变异数据大概在400万左右（包括SNP和Indel）。</p>
<p>这篇文章已经很长了，在变异检测的这个过程中GATK应用了很多重要的算法，包括如何构建模型、如何进行局部组装和比对、如何应用贝叶斯、PariHMM、GMM、参数训练、特征选择等等，这些只能留在后面介绍GATK的专题文章中再进行展开了。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>在这里，整篇文章就结束了。如你所见，文章非常长，这里基本包含了WGS最佳实践中的所有内容，但其实我想说的还远不止如此（包括CNV和SV的检测），只是暂时只能作罢了，否则恐怕就没人愿意看下去了，呵呵。<strong>在这个WGS主流程的构建过程中，我并非只是硬邦邦地告诉大家几条简单的命令就了事了，因为我认为那种做法要么是极其不负责任的，要么就是作者并非真的懂。而且如果都觉得只要懂得几条命令就可以了的话，那么我们就活该被机器和人工智能所取替，它们一定会操作得更好更高效</strong>。我想掌握工具和技术的目的是为了能够更好地发现并解决问题（包括科研和生产），所有的数据分析流程本质上是要服务于我们所要解决的问题的。</p>
<p>毕竟工具是死的，人是活的，需求总是会变的。理解我们所要处理的问题的本质，选择合适的工具，而不是反过来被工具所束缚，这一点很重要。个人的能力不能只是会跑一个流程，或者只是会创建流程，因为那都是一个“术”的问题，我觉得我们真正要去掌握的应该是如何分析数据的能力，如何发现问题和解决数据问题等的能力。</p>
<hr>
<p>本文首发于我的个人公众号：<strong>解螺旋的矿工</strong>，欢迎扫码关注，更及时了解更多信息</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/helixminer_wechat_qr.png" alt="解螺旋的矿工"></p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
            <category> 基因组学 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NGS </tag>
            
            <tag> WGS </tag>
            
            <tag> 流程 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[从零开始完整学习全基因组测序（WGS）数据分析：第3节 数据质控]]></title>
      <url>/2017/08/25/2017-08-25-Begining-WGS-Data-Analysis-Fastq-Data-Quality-Control.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs_03_cover_small.png" alt="数据质控"></p>
<p>从这一节开始详细讲述正式流程的搭建，我将结合具体的例子努力争取将这个系列写成比GATK最佳实践更加具体、更具有实践价值的入门指南。整个完整的流程分为以下6部分：</p>
<ol>
<li>原始测序数据的质控</li>
<li>read比对，排序和去除重复序列</li>
<li>Indel区域重（“重新”的“重”）比对</li>
<li>碱基质量值重校正</li>
<li>变异检测</li>
<li>变异结果质控和过滤</li>
</ol>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig1.png" alt="WGS流程图"></p>
<p>在这个图中，我把WGS数据分析流程的各个步骤和关系都画下来了。这个流程虽然只针对于人，但对于其它二倍体生物来说，同样具有借鉴价值。这6个步骤，接下来我也会进行详细介绍，在本篇文章中我们首先介绍原始测序数据的质控。</p>
<h2 id="认识测序数据——数据质控的意义"><a href="#认识测序数据——数据质控的意义" class="headerlink" title="认识测序数据——数据质控的意义"></a>认识测序数据——数据质控的意义</h2><p>在<a href="http://www.huangshujia.me/2017/08/04/2017-08-04-Begining-WGS-Data-Analysis-Sequecing-Tech.html" target="_blank" rel="external">第1节测序技术</a>中，我们已经知道现在的NGS测序，以illumina为首基本都是运用边合成边测序的技术。碱基的合成依靠的是化学反应，这使得碱基链可以不断地从5’端一直往3’端合成并延伸下去。但在这个合成的过程中随着合成链的增长，DNA聚合酶的效率会不断下降，特异性也开始变差，这就会带来一个问题——越到后面碱基合成的错误率就会越高【注】，这也是为何当前NGS测序读长普遍偏短的一个原因。</p>
<blockquote>
<p>【注】：有时候测序仪在刚开始进行合成反应的时候也会由于反应还不够稳定，同样会带来质量值的波动，不过这个波动一般都在高质量值区域（如下图）。</p>
</blockquote>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig2.png" alt="序列开头的波动"></p>
<p>测序数据的质量好坏会影响我们的下游分析。但不同的测序平台其测序错误率的图谱都是有差别的。因此，非常建议在我们分析测序数据之前先搞清楚如下两个地方：</p>
<ul>
<li>原始数据是通过哪种测序平台产生的，它们的错误率分布是怎么样的，是否有一定的偏向性和局限性，是否会显著受GC含量的影响等；</li>
<li>评估它们有可能影响哪些方面的分析；</li>
</ul>
<p>第一点是我们认识数据质量的第一步，也是我们一定要去知道的地方。除了看官方的资料之外，最好的做法是自己分析。</p>
<p>虽然随着NGS测序数据变得越来越普遍，整体的测序质量和错误率分布情况大家也都了解一些。在实际的工作中，我也常常发现很多人其实并不十分关心这个数据到底长啥样，拿到之后，就直接跑过滤流程，也不管这些参数或者工具是否真的是合适的，更加不看看过滤后的数据和过滤前到底有什么不同。尽管，大多数情况下问题不大，但是我想跟大家说的是： <strong>认识你的数据，不要相信你的工具！</strong> 这样也能够更好地避开很多不必要的坑。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/%E5%A6%88%E8%9B%8B%E8%AF%B4%E7%9A%84%E6%98%AF%E7%9C%9F%E7%90%86.png" alt="真理"></p>
<p>因此，在本文中我将谈谈该如何更好地去认识一个测序数据，而不只是简单地告诉大家一个质控流程，当然，这也是本篇文章将要进行介绍的内容。至于第二点其实需要视情况而定，例如你的测序深度是多少，检测变异的时候，变异的位点是否过于集中在read的末尾，比对的时候是否会出现了一定的正反链偏向性等诸如此类的问题；又或者我们在进行基因组序列组装的时候，由于对read的中出错的碱基更加敏感，因此往往需要进行更严格的切除，不然会由于这些错误的碱基消耗更大的计算资源和时间。</p>
<p>那么说大地该如何认识一个原始的测序数据（fastq data）呢？一般我们可以从如下几个方面来分析：</p>
<ul>
<li>read各个位置的碱基质量值分布</li>
<li>碱基的总体质量值分布</li>
<li>read各个位置上碱基分布比例，目的是为了分析碱基的分离程度</li>
<li>GC含量分布</li>
<li>read各位置的N含量</li>
<li>read是否还包含测序的接头序列</li>
<li>read重复率，这个是实验的扩增过程所引入的</li>
</ul>
<p>以上，这几个地方都弄明白了，那么这个数据的基本情况也就差不多都清楚了。</p>
<p>我们首先来说说read各位置的碱基质量分布。在<a href="http://www.huangshujia.me/2017/08/12/2017-08-12-Begining-WGS-Data-Analysis-Fasta-And-Fastq.html" target="_blank" rel="external">第2节</a>里面，我们已经知道该如何通过简单的Python代码计算出read的质量值和碱基的测序错误率了。但对于成千上万的read来说，这样做并不合适，我们需要更直观的表达方式——画出来，正所谓 <strong>一图胜千言</strong>！目前也有很多现成的工具可以高效地来完成这样的事情，比如用得最广的<a href="http://www.bioinformatics.babraham.ac.uk/projects/fastqc/" target="_blank" rel="external">FastQC</a>，它是一个java程序，能够用于给出测序数据的QC报告，报告中会同时给出上述几个方面的数据图，并提示原来的数据可能还存在着哪些问题。它可以很好地帮助我们理解测序数据的质量情况，但缺点就是 <strong>图！太！丑！</strong></p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/%E6%8D%82%E8%84%B8.png" alt="太丑，看不下去"></p>
<p>在做read质量值分析的时候，FastQC并不单独查看具体某一条read中碱基的质量值，而是将Fastq文件中所有的read数据都综合起来一起分析。下图是一个测序质量非常好的read各位置碱基质量分布图（如下图）。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig3.good.png" alt="好的测序结果"></p>
<p>这个图的横轴是read上碱基的位置，纵轴是碱基质量值。在这个例子中，read的长度是126bp（来自HiSeq X10的测序结果），这应该算是比较长的二代测序序列了。我们可以看到read上的每一个位置都有一个黄色的箱型图表示在该位置上所有碱基的质量分布情况。除了最后一个碱基之外，其他的碱基质量值都基本都在大于30，而且波动很小，说明质量很稳定，这其实是一个非常高质量的结果。而且我们可以看到图中质量值的分布都在绿色背景（代表高质量）的区域。</p>
<p>那如果是质量很差的结果看起来会是怎么样的呢？我手边一时找不到这样的数据，就在网上找到了一个代替品，样子如下：</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig4.bad.png" alt="差的测序结果"></p>
<p>在这个图中我们可以明显看到，read各个位置上的碱基质量分布波动都比较大，特别从第18个碱基往后全部出现了大幅度的波动，而且很多read的碱基质量值都掉到非常低（红色）的区域中了，说明这个数据的测序结果真的非常差，有着大量不及格的read。最好的情况是重新测序，但如果不得不使用这个数据，就要把这些低质量的数据全都去除掉才行，同时还需留意是否还存在其他的问题，但不管如何都一定会丢掉很大一部分的数据。</p>
<p>除了上面read各位置的碱基质量值分布之外，FastQC还会为我们计算其他几个非常有价值的统计结果，包括：</p>
<p>1）碱基总体质量值分布，只要大部分都高于20，那么就比较正常。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig5.quality.png" alt="碱基总体质量值分布"></p>
<p>在<a href="http://www.huangshujia.me/2017/08/12/2017-08-12-Begining-WGS-Data-Analysis-Fasta-And-Fastq.html" target="_blank" rel="external">第2节</a>里面我也提到了关于Q20和Q30的比例是我们衡量测序质量的一个重要指标。这其实也是从这里来进行体现的，一般来说，对于二代测序，<strong>最好是达到Q20的碱基要在95%以上（最差不低于90%），Q30要求大于85%（最差也不要低于80%）</strong>。</p>
<p>2）read各个位置上碱基比例分布</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig6.baserate.png" alt="碱基含量分布"></p>
<p>这个是为了分析碱基的分离程度。何为碱基分离？我们知道AT配对，CG配对，假如测序过程是比较随机的话（随机意味着好），那么在每个位置上A和T比例应该差不多，C和G的比例也应该差不多，如上图所示，两者之间即使有偏差也不应该太大，<strong>最好平均在1%以内</strong>，如果过高，除非有合理的原因，比如某些特定的捕获测序所致，否则都需要注意是不是测序过程有什么偏差。</p>
<p>3）GC含量分布图</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig7.GC_content.png" alt="GC含量分布"></p>
<p>GC含量指的是G和C这两种碱基占总碱基的比例。二代测序平台或多或少都存在一定的测序偏向性，我们可以通过查看这个值来协助判断测序过程是否足够随机。对于人类来说，我们基因组的GC含量一般在40%左右。因此，如果发现GC含量的图谱明显偏离这个值那么说明测序过程存在较高的序列偏向性，结果就是基因组中某些特定区域被反复测序的几率高于平均水平，除了覆盖度会有偏离之后，将会影响下游的变异检测和CNV分析。</p>
<p>4）N含量分布图</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig8.N.png" alt="N含量分布图"></p>
<p>N在测序数据中一般是不应该出现的，如果出现则意味着，测序的光学信号无法被清晰分辨，如果这种情况多的话，往往意味着测序系统或者测序试剂的错误。</p>
<p>5）接头序列</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig9.adapter.png" alt="接头序列比较"></p>
<p>在<a href="http://www.huangshujia.me/2017/08/04/2017-08-04-Begining-WGS-Data-Analysis-Sequecing-Tech.html" target="_blank" rel="external">第1节 测序技术</a>里面我们提到了在测序之前需要构建测序文库，测序接头就是在这个时候加上的，其目的一方面是为了能够结合到flowcell上，另一方面是当有多个样本同时测序的时候能够利用接头信息进行区分。当测序read的长度大于被测序的DNA片段【注】时，就会在read的末尾测到这些接头序列（如下图）。一般的WGS测序是不会测到这些接头序列的，因为构建WGS测序的文库序列（插入片段）都比较长，约几百bp，而read的测序长度都在100bp-150bp这个范围。不过在进行一些RNA测序的时候，由于它们的序列本来就比较短，很多只有几十bp长（特别是miRNA），<strong>那么就很容易会出现read测通的现象，这个时候就会在read的末尾测到这些接头序列</strong>。</p>
<blockquote>
<p>【注】这些DNA片段也常被我们称之为“插入片段”</p>
</blockquote>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig10.seq_adapter.png" alt="read测通的情况"></p>
<p>最后，这些被测到的接头序列和低质量碱基一样都是需要在正式分析之前进行切除的read片段。</p>
<p>当我们看完了上面的这些结果之后就可以比较清楚地了解一个测序数据的概况了。</p>
<p><strong>那么，说了这么多，上述提到的FastQC该怎么用呢？</strong></p>
<p>FastQC的安装非常简单，我们可以通过网页搜索或者直接到<a href="https://www.bioinformatics.babraham.ac.uk/projects/fastqc/" target="_blank" rel="external">它的主页</a>上下载最新的版本。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig11.fastqc.png" alt="FastQC"></p>
<p>也可以在终端通过wget命令下载：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.5.zip ./</div></pre></td></tr></table></figure>
<p>解压之后，修改文件夹中fastqc的权限，就可以直接运行了：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ unzip fastqc_v0.11.5.zip</div><div class="line">$ <span class="built_in">cd</span> FastQC </div><div class="line">$ chmod 755 fastqc</div></pre></td></tr></table></figure>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig12.fastqc.png" alt="FastQC的目录"></p>
<p>FastQC的运行非常简单，直接在终端通过命令行是最有效直接的，下面我给出一个例子：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ /path_to_fastqc/FastQC/fastqc untreated.fq -o fastqc_out_dir/</div></pre></td></tr></table></figure>
<p>命令比较简单，这里 <strong>唯一值得注意的地方就是 -o 参数用于指定FastQC报告的输出目录</strong>，这个目录需要事先创建好，如果不指定特定的目录，那么FastQC的结果会默认输出到文件untreated.fq的同一个目录下。它输出结果只有两个，一个html和一个.zip压缩包。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ tree fastqc_out_dir/</div></pre></td></tr></table></figure>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig13.fastqc.png" alt="输出目录"></p>
<p>我们可以直接通过浏览器打开html，就可以看到FastQC给出的所有结果，zip压缩包解压后，从中我们也可以在对应的目录下找到所有的QC图表和Summary数据。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig14.fastqc_fold.png" alt="zip解压目录结构"></p>
<p>除了上述用法之外，FastQC支持同时输入多个fq文件（或者以通配符的形式输入fq），当我们的fq文件比较多时，这种用法会比较方便，如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ /path_to_fastqc/FastQC/fastqc /path_to_fq/*.fq -o fastqc_out_dir/</div></pre></td></tr></table></figure>
<p>这个<a href="http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/" target="_blank" rel="external">链接</a>是FastQC官网给出的一个Online报告的模板，方便参考。</p>
<h2 id="切除测序接头序列和read的低质量序列"><a href="#切除测序接头序列和read的低质量序列" class="headerlink" title="切除测序接头序列和read的低质量序列"></a>切除测序接头序列和read的低质量序列</h2><p>前面关于如何认识fq数据的事情已经说完了，接下来是我们本篇文章中最后的一个重点——去除测序接头和低质量序列！</p>
<p>当我们理解了fq数据之后，做这些过滤就不会很难，你也完全可以自己编写工具来进行个性化的过滤。目前也已有很多工具用来切除接头序列和低质量碱基，比如SOAPnuke、cutadapt、untrimmed等不下十个，但这其中比较方便好用的是Trimmomatic（也是一个java程序）、sickle和seqtk。Trimmomatic的好处在于，它不但可以用来切除illumina测序平台的接头序列，还可以去除由我们自己指定的特定接头序列，而且同时也能够过滤read末尾的低质量序列，sickle和seqtk只能去除低质量碱基。具体的原理就是通过滑动一定长度的窗口，计算窗口内的碱基平均质量，如果过低，就直接 <strong>往后全部切除，注！意！不是挖掉read中的这部分低质量序列，而是像切菜一样，直接从低质量区域开始把这条read后面的所有其它碱基全！部！剁！掉！否则就是在人为改变实际的基因组序列情况</strong>。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/%E5%88%87%E8%8F%9C%E7%9A%84.png" alt="切菜哥"></p>
<p>如果下机的fq数据中不含有这些测序接头，那么我们除了trimmomatic之外，也可以直接使用sickle（同时支持PE和SE数据）或者seqtk（仅支持SE），这两个处理起来会更快，消耗的计算资源也更少。</p>
<p><strong>现在我们说回如何用Trimmomatic构造序列过滤流程。</strong></p>
<p>首先是安装Trimmomatic。我们可以到<a href="http://www.usadellab.org/cms/?page=trimmomatic" target="_blank" rel="external">它的官网</a>上获取最新的版本，下载打包好的binary即可，如果打算看它具体的代码，可以在github上找到。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig15.Trimmomatic.png" alt="下载Trimmomatic"></p>
<p>下载后，直接解压，目录下的trimmomatic-*.jar（我下载的是0.36版本）就是执行程序，可以直接使用java来运行。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs03.fig16.Trimmomatic.png" alt="Trimmomatic目录"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -jar trimmomatic-0.36.jar</div></pre></td></tr></table></figure>
<p>同个目录下还有一个名为adapters的文件夹，<strong>这个文件夹中的内容对于我们去除接头序列来说非常重要</strong>。其中默认存放的是illumina测序平台的接头序列（fasta格式），在实际的使用过程中，如果需要去除接头，我们需要明确指定对应的序列作为输入参数。</p>
<p>那么这些接头序列具体该如何选择呢？一般来说，<strong>目前的HiSeq系列和MiSeq系列用的都是TruSeq3，TruSeq2是以前GA2系列的测序仪所用的，已经很少见了</strong>。这些信息都可以在illumina的官网上找到，至于具体该用PE（Pair End）还是SE（Single End）就按照具体的测序类型进行选择就ok了。如果用的不是illumina测序平台，那么我们也可以按照adapters文件夹下的这些文件的格式做一个新的接头序列，然后再作为参数传入。不过在自定义接头序列的时候，命名时有一些小的细节需要注意，可以参考Trimmomatic的<a href="http://www.usadellab.org/cms/?page=trimmomatic" target="_blank" rel="external">主页文档（The Adapter Fasta）</a>，这里就不展开了。</p>
<p>Trimmomatic有两种运行模式：PE和SE。顾名思义，PE就是对应Pair End测序的，SE则是对应Single End测序的。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ java -jar trimmomatic-0.36.jar</div><div class="line">Usage:</div><div class="line">       PE [-version] [-threads &lt;threads&gt;] [-phred33|-phred64] [-trimlog &lt;trimLogFile&gt;] [-quiet] [-validatePairs] [-basein &lt;inputBase&gt; | &lt;inputFile1&gt; &lt;inputFile2&gt;] [-baseout &lt;outputBase&gt; | &lt;outputFile1P&gt; &lt;outputFile1U&gt; &lt;outputFile2P&gt; &lt;outputFile2U&gt;] &lt;trimmer1&gt;...</div><div class="line">   or:</div><div class="line">       SE [-version] [-threads &lt;threads&gt;] [-phred33|-phred64] [-trimlog &lt;trimLogFile&gt;] [-quiet] &lt;inputFile&gt; &lt;outputFile&gt; &lt;trimmer1&gt;...</div><div class="line">   or:</div><div class="line">       -version</div></pre></td></tr></table></figure>
<p>下面我分别给出例子来进行说明：</p>
<p><strong>PE模式，HiSeq PE测序：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -jar /path/Trimmomatic/trimmomatic-0.36.jar PE -phred33 -trimlog logfile reads_1.fq.gz reads_2.fq.gz out.read_1.fq.gz out.trim.read_1.fq.gz out.read_2.fq.gz out.trim.read_2.fq.gz ILLUMINACLIP:/path/Trimmomatic/adapters/TruSeq3-PE.fa:2:30:10 SLIDINGWINDOW:5:20 LEADING:5 TRAILING:5 MINLEN:50</div></pre></td></tr></table></figure>
<p><strong>SE模式，HiSeq SE测序：</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ java -jar /path/Trimmomatic/trimmomatic-0.36.jar SE -phred33 -trimlog se.logfile raw_data/untreated.fq out.untreated.fq.gz ILLUMINACLIP:/path/Trimmomatic/adapters/TruSeq3-SE.fa:2:30:10 SLIDINGWINDOW:5:20 LEADING:5 TRAILING:5 MINLEN:50</div></pre></td></tr></table></figure></p>
<p>我们可以看到PE和SE，顾名思义，分别代表了 ‘PE模式’和‘SE’模式。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/%E5%BA%9F%E8%AF%9D%E8%BF%98%E7%94%A8%E4%BD%A0%E8%AF%B4.png" alt="废话也要说"></p>
<p>同时需要明确指明质量值体系是Phred33还是Phred64，<strong>默认是Phred64，这需要特别注意，因为我们现在的测序数据基本都是Phred33的了，所以一定要指定这个参数</strong>。</p>
<p>剩下的就是输入的fq和输出的fq，可以用-basein和-baseout指定，也可以不用（如上例子），以及被过滤掉的fq要输出到文件。细心的读者可能已经发现这里PE和SE有一个区别：</p>
<p>在SE模式中，是不需要指定文件来存放被过滤掉的read信息的，后面直接就接Trimmer信息！这是需要注意到的一个地方。</p>
<p>关于后面的Trimmer信息，规定了很多切除接头序列和低质量序列的细节，我挑重点的说，具体如下：</p>
<ul>
<li>ILLUMINACLIP，接头序列切除参数。LLUMINACLIP:TruSeq3-PE.fa:2:30:10（省掉了路径）意思分别是：TruSeq3-PE.fa是接头序列，2是比对时接头序列时所允许的最大错配数；30指的是要求PE的两条read同时和PE的adapter序列比对，匹配度加起来超30%，那么就认为这对PE的read含有adapter，并在对应的位置需要进行切除【注】。10和前面的30不同，它指的是，我就什么也不管，反正只要这条read的某部分和adpater序列有超过10%的匹配率，那么就代表含有adapter了，需要进行去除；</li>
</ul>
<blockquote>
<p>【注】测序的时候一般只会测到一部分的adapter，因此read和adaper对比的时候肯定是不需要要求百分百匹配率的，上述30%和10%其实是比较推荐的值。<br>SLIDINGWINDOW，滑动窗口长度的参数，SLIDINGWINDOW:5:20代表窗口长度为5，窗口中的平均质量值至少为20，否则会开始切除；</p>
</blockquote>
<ul>
<li>LEADING，规定read开头的碱基是否要被切除的质量阈值；</li>
<li>TRAILING，规定read末尾的碱基是否要被切除的质量阈值；</li>
<li>MINLEN，规定read被切除后至少需要保留的长度，如果低于该长度，会被丢掉。</li>
</ul>
<p>此外，另一个值得注意的地方是，Trimmomatic的报错给出的提示信息都比较难以定位错误问题（如下图），但这往往都只是参数用没设置正确所致。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Exception <span class="keyword">in</span> thread <span class="string">"main"</span> java.lang.RuntimeException: Unknown trimmer: u.trim.txt</div><div class="line">        at org.usadellab.trimmomatic.trim.TrimmerFactory.makeTrimmer(TrimmerFactory.java:70)</div><div class="line">        at org.usadellab.trimmomatic.Trimmomatic.createTrimmers(Trimmomatic.java:59)</div><div class="line">        at org.usadellab.trimmomatic.TrimmomaticSE.run(TrimmomaticSE.java:303)</div><div class="line">        at org.usadellab.trimmomatic.Trimmomatic.main(Trimmomatic.java:85)</div></pre></td></tr></table></figure></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>数据质控的内容终于都讲完了，接下来第4节就是主流程的构建。</p>
<hr>
<p>欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/helixminer_wechat_qr.png" alt="解螺旋的矿工"></p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
            <category> 基因组学 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NGS </tag>
            
            <tag> WGS </tag>
            
            <tag> 数据质控 </tag>
            
            <tag> fastq </tag>
            
            <tag> FastQC </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[从零开始完整学习全基因组测序（WGS）数据分析：第2节 FASTA和FASTQ]]></title>
      <url>/2017/08/12/2017-08-12-Begining-WGS-Data-Analysis-Fasta-And-Fastq.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/wgs_s2_cover.png" alt="wgs_02_cover"></p>
<p>在WGS数据的分析过程中，我们会接触到许多生物信息学/基因组学领域所特有的数据文件和它们特殊的格式，在这一节中将要介绍的FASTA和FASTQ便是其中之一二。这是我们存储核苷酸序列信息（就是DNA序列）或者蛋白质序列信息最常使用的两种 <strong>文本文件</strong>，虽然看起来名字有些古怪，但它们完全是纯文本文件（如同.txt）！名字的发音分别是fast-A和fast-Q。这一篇文章内容虽然比较简单，但还是比较长，我在这里详细介绍了这两类文件的格式特点和一些在分析的时候需要考虑的地方。</p>
<h2 id="FASTA"><a href="#FASTA" class="headerlink" title="FASTA"></a>FASTA</h2><p>我相信许多人（包括生物信息工程师们）一定不知道FASTA这个文件的来源，竟然是一款名叫“FASTA”的比对软件！名字中最后一个字母A，其实就是Alignment的意思！但这已经是上个世纪的事情了，最初是由William. R. Pearson 和 David. J. Lipman在1988年所编写，目的是用于生物序列数据的处理。</p>
<p>自那之后，生物学家和遗传学家们也没做过多的考虑，就草率地决定（其实类似的‘草率’行为在组学领域经常碰到）把FASTA作为这种存储 <strong>有顺序的</strong>序列数据的文件后缀【注】，这包括我们常用的参考基因组序列、蛋白质序列、编码DNA序列（coding DNA sequence，简称CDS）、转录本序列等文件都是如此，文件后缀除了.fasta之外，也常用.fa或者.fa.gz（gz压缩）。</p>
<blockquote>
<p>【注】这里的序列、序列数据，指的其实就是表示DNA或者蛋白质的一条字符串。</p>
</blockquote>
<p>这里再特别强调三个字：有！顺！序！说的是从1开始一个个按顺序往下排列的意思——这不也正是序列这个词的含义！</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/interesting.png" alt="1"></p>
<p>因此，我们可以通过数个数，就知道某个DNA碱基在某个基因组上的准确位置，这个位置会用所在序列的名字和所在位置来表达，比如基因数据比对的结果（下一篇会介绍），方便后续数据分析。</p>
<p>FASTA文件主要由两个部分构成：<strong>序列头信息（有时包括一些其它的描述信息）和具体的序列数据。头信息独占一行，以大于号（&gt;）开头作为识别标记</strong>，其中除了记录该条序列的名字之外，有时候还会接上其它的信息。紧接的下一行是具体的序列内容，直到另一行碰到另一个大于号（&gt;）开头的新序列或者文件末尾。下面给出一个FASTA文件的例子，这是我们人类一个名为EGFR基因的部分序列。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;ENSMUSG00000020122|ENSMUST00000138518</div><div class="line">CCCTCCTATCATGCTGTCAGTGTATCTCTAAATAGCACTCTCAACCCCCGTGAACTTGGT</div><div class="line">TATTAAAAACATGCCCAAAGTCTGGGAGCCAGGGCTGCAGGGAAATACCACAGCCTCAGT</div><div class="line">TCATCAAAACAGTTCATTGCCCAAAATGTTCTCAGCTGCAGCTTTCATGAGGTAACTCCA</div><div class="line">GGGCCCACCTGTTCTCTGGT</div><div class="line">&gt;ENSMUSG00000020122|ENSMUST00000125984</div><div class="line">GAGTCAGGTTGAAGCTGCCCTGAACACTACAGAGAAGAGAGGCCTTGGTGTCCTGTTGTC</div><div class="line">TCCAGAACCCCAATATGTCTTGTGAAGGGCACACAACCCCTCAAAGGGGTGTCACTTCTT</div><div class="line">CTGATCACTTTTGTTACTGTTTACTAACTGATCCTATGAATCACTGTGTCTTCTCAGAGG</div><div class="line">CCGTGAACCACGTCTGCAAT</div></pre></td></tr></table></figure>
<p>可以看到，FASTA其实很简单，但它往往都很大，比如人类基因组有30亿个碱基，就是30亿个字符存储在这样的一个文本文件中，就算是压缩也要占用约1GB的存储空间。</p>
<p>另外，有两个地方，我觉得有必要提及：</p>
<p><strong>第一，除了序列内容之外，FASTA的头信息并没有被严格地限制</strong>。这个特点有时会带来很多麻烦的事情，比如有时我们会看到相同的序列被不同的人处理之后、甚至是在不同的网站上或者数据库中它们的头信息都不尽相同，比如以下的几种情况都是可能存在的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;ENSMUSG00000020122|ENSMUST00000125984</div><div class="line">&gt; ENSMUSG00000020122|ENSMUST00000125984</div><div class="line">&gt;ENSMUSG00000020122|ENSMUST00000125984|epidermal growth factor receptor</div><div class="line">&gt;ENSMUSG00000020122|ENSMUST00000125984|Egfr</div><div class="line">&gt;ENSMUSG00000020122|ENSMUST00000125984|11|ENSFM00410000138465</div></pre></td></tr></table></figure>
<p>这对于程序处理来说，凌乱的格式显然是不合适的。因此后来在业内也慢慢地有一些不成文的规则被大家所使用，那就是，<strong>用一个空格把头信息分为两个部分：第一部分是序列名字，它和大于号（&gt;）紧接在一起；第二部分是注释信息，这个可以没有，就看具体需要</strong>，比如下面这个序列例子，除了前面gene_00284728这个名字之外，注释信息（length=231;type=dna）给出这段序列的长度和它所属的序列类型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;gene_00284728 length=231;type=dna</div><div class="line">GAGAACTGATTCTGTTACCGCAGGGCATTCGGATGTGCTAAGGTAGTAATCCATTATAAGTAACATG</div><div class="line">CGCGGAATATCCGGGAGGTCATAGTCGTAATGCATAATTATTCCCTCCCTCAGAAGGACTCCCTTGC</div><div class="line">GAGACGCCAATACCAAAGACTTTCGTAAGCTGGAACGATTGGACGGCCCAACCGGGGGGAGTCGGCT</div><div class="line">ATACGTCTGATTGCTACGCCTGGACTTCTCTT</div></pre></td></tr></table></figure>
<p>这对于程序处理来说，凌乱的格式显然是不合适的。因此后来在业内也慢慢地有一些不成文的规则被大家所使用，那就是，用一个空格把头信息分为两个部分：第一部分是序列名字，它和大于号（&gt;）紧接在一起；第二部分是注释信息，这个可以没有，就看具体需要，比如下面这个序列例子，除了前面gene_00284728这个名字之外，注释信息（length=231;type=dna）给出这段序列的长度和它所属的序列类型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;gene_00284728 length=231;type=dna</div><div class="line">GAGAACTGATTCTGTTACCGCAGGGCATTCGGATGTGCTAAGGTAGTAATCCATTATAAGTAACATG</div><div class="line">CGCGGAATATCCGGGAGGTCATAGTCGTAATGCATAATTATTCCCTCCCTCAGAAGGACTCCCTTGC</div><div class="line">GAGACGCCAATACCAAAGACTTTCGTAAGCTGGAACGATTGGACGGCCCAACCGGGGGGAGTCGGCT</div><div class="line">ATACGTCTGATTGCTACGCCTGGACTTCTCTT</div></pre></td></tr></table></figure>
<p>虽然这样的格式还不算是真正的标准，但却非常有助于我们的数据分析和处理，很多生信软件（如：BWA，samtools，bcftools，bedtools等）都是将第一个空格前面的内容认定为序列名字来进行操作的。</p>
<p><strong>第二，FASTA由于是文本文件，它里面的内容是否有重复是无法自检的，在使用之前需要我们进行额外的检查</strong>。这个检查倒不用很复杂，只需检查序列名字是否有重复即可。但对于那些已经成为标准使用的参考序列来说，都有专门的团队进行维护，因此不会出现这种内容重复的情况，可以直接使用，但对于其它的一些序列来说，谨慎起见，最好进行检查。</p>
<h2 id="FASTQ"><a href="#FASTQ" class="headerlink" title="FASTQ"></a>FASTQ</h2><p>这是目前存储测序数据最普遍、最公认的一个数据格式，另一个是uBam格式，但这篇文章中不打算对其进行介绍。上面所讲的FASTA文件，它所存的都是已经排列好的序列（如参考序列），FASTQ存的则是产生自测序仪的原始测序数据，它由测序的图像数据转换过来，也是文本文件，文件大小依照不同的测序量（或测序深度）而有很大差异，小的可能只有几M，大的则常常有几十G上百G，文件后缀通常都是.fastq，.fq或者.fq.gz（gz压缩），以下是它的一个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">@DJB775P1:248:D0MDGACXX:7:1202:12362:49613</div><div class="line">TGCTTACTCTGCGTTGATACCACTGCTTAGATCGGAAGAGCACACGTCTGAA</div><div class="line">+</div><div class="line">JJJJJIIJJJJJJHIHHHGHFFFFFFCEEEEEDBD?DDDDDDBDDDABDDCA</div><div class="line">@DJB775P1:248:D0MDGACXX:7:1202:12782:49716</div><div class="line">CTCTGCGTTGATACCACTGCTTACTCTGCGTTGATACCACTGCTTAGATCGG</div><div class="line">+</div><div class="line">IIIIIIIIIIIIIIIHHHHHHFFFFFFEECCCCBCECCCCCCCCCCCCCCCC</div></pre></td></tr></table></figure>
<p>你可以看到它有着自己独特的格式：<strong>每四行成为一个独立的单元，我们称之为read</strong>。具体的格式描述如下：</p>
<ul>
<li><p>第一行：以‘@’开头，是这一条read的名字，这个字符串是根据测序时的状态信息转换过来的，中间不会有空格，它是 <strong>每一条read的唯一标识符</strong>，同一份FASTQ文件中不会重复出现，甚至不同的FASTQ文件里也不会有重复； </p>
</li>
<li><p>第二行：测序read的序列，由A，C，G，T和N这五种字母构成，这也是我们真正关心的DNA序列，N代表的是测序时那些无法被识别出来的碱基；</p>
</li>
<li><p>第三行：以‘+’开头，在旧版的FASTQ文件中会直接重复第一行的信息，但现在一般什么也不加（节省存储空间）；</p>
</li>
<li><p>第四行：测序read的质量值，这个和第二行的碱基信息一样重要，它描述的是每个测序碱基的可靠程度，用ASCII码表示。</p>
</li>
</ul>
<p>那么，重点说一下什么是质量值？顾名思义，碱基质量值就是能够用来定量描述碱基 <strong>好坏程度</strong>的一个数值。它该如何才能恰当地描述这个结果呢？我们试想一下，如果测序测得越准确，这个碱基的质量就应该越高；反之，测得越不准确，质量值就应该越低。也就是说可以利用碱基被测错的概率来描述它的质量值，错误率越低，质量值就越高！如下图，红线代表错误率，蓝线代表质量值，这便是我们希望达到的效果：</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/fq_seqerror.png" alt="测序错误率"></p>
<p>这里我们假定碱基的测序错误率为: $${P}_{error}$$质量值为Q，它们之间的关系如下：</p>
<p>$$Q=-10log_{10}{P}_{error}$$</p>
<p>即，<strong>质量值是测序错误率的对数（10为底数）乘以-10（并取整）</strong>。这个公式也是目前测序质量值的计算公式，它非常简单，p_error的值和测序时的多个因素有关，体现为测序图像数据点的清晰程度，并由测序过程中的base calling 算法计算出来；公式右边的Q我们称之为Phred quality score，就是用它来描述测序碱基的靠谱程度。比如，如果该碱基的测序错误率是0.01，那么质量值就是20（俗称Q20），如果是0.001，那么质量值就是30（俗称Q30）。Q20和Q30的比例常常被我们用来评价某次测序结果的好坏，比例越高就越好。下面我也详细给出一个表，更进一步地解释质量值高低的含义：</p>
<table>
<thead>
<tr>
<th>测序平台</th>
<th>ASCII码范围</th>
<th>下限</th>
<th>质量值类型</th>
<th>质量值范围</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sanger, Illumina(版本1.8及以上)</td>
<td>33-126</td>
<td>33</td>
<td>Phred quality score</td>
<td>0-93</td>
<td>现在沿用</td>
</tr>
<tr>
<td>Solexa, Illumina早期版本(&lt;1.3版本)</td>
<td>59-126</td>
<td>64</td>
<td>Solexa quality score</td>
<td>5-62</td>
<td>除了已测序数据之外，不再使用</td>
</tr>
<tr>
<td>Illumina(版本1.3-1.7)</td>
<td>64-126</td>
<td>64</td>
<td>Phred quality score</td>
<td>0-62</td>
<td>除了已测序数据之外，不再使用</td>
</tr>
</tbody>
</table>
<p>现在回过头来说说为什么要用ASCII码来代表，直接用数字不行吗？行！但很难看，而且数字不能直接连起来，还得在中间加一个分隔符，长度也对不齐，还占空间，又不符合美学设计，真！麻！烦！</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/tired.png" alt="2"></p>
<p>因此，也是为了格式存储以及处理时的方便，这个数字被直接转换成了ASCII码，并与第二行的read序列构成一一对应的关系——每一个ASCII码都和它正上面的碱基对应，这就很完美。</p>
<p>不过，值得一提的是，ASCII码虽然能够从小到大表示0-127的整数，但是并非所有的ASCII码都是 <strong>可见的字符</strong>，比如所有小于33的ASCII码值所表示的都是不可见字符，比如空格，换行符等，因此 <strong>为了能够让碱基的质量值表达出来，必须避开所有这些不可见字符</strong>。最简单的做法就是加上一个固定的整数！也的确是这么干的。</p>
<p>但一开始对于要加哪一个整数，并没有什么指导标准，这就导致了在刚开始的时候，不同的测序平台加的整数也不同，总的来说有以下3种质量体系，演变到现在也基本只剩下第一种了，如下表：</p>
<table>
<thead>
<tr>
<th>Phred Quality Score</th>
<th>Probability of incorrect base call</th>
<th>Base call accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>1 in 10</td>
<td>90%</td>
</tr>
<tr>
<td>20</td>
<td>1 in 100</td>
<td>99%</td>
</tr>
<tr>
<td>30</td>
<td>1 in 1000</td>
<td>99.9%</td>
</tr>
<tr>
<td>40</td>
<td>1 in 10,000</td>
<td>99.99%</td>
</tr>
<tr>
<td>50</td>
<td>1 in 100,000</td>
<td>99.999%</td>
</tr>
<tr>
<td>60</td>
<td>1 in 1,000,000</td>
<td>99.9999%</td>
</tr>
</tbody>
</table>
<p>从表中可以看到下限有33和64两个值，我们把加33的的质量值体系称之为Phred33，加64的称之为Phred64（Solexa的除外，它叫Solexa64）。不过，现在一般都是使用Phred33这个体系，而且33也恰好是ASCII的第一个可见字符（’!’），完美+2。</p>
<p>如果你在实际做项目的过程不知道所用的质量体系（经验丰富者是可以直接看出来的），那么可以用我下面这一段代码，简单地做个检查:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">less <span class="variable">$1</span> | head -n 1000 | awk <span class="string">'&#123;if(NR%4==0) printf("%s",$0);&#125;'</span> \</div><div class="line">| od -A n -t u1 -v \</div><div class="line">| awk <span class="string">'BEGIN&#123;min=100;max=0;&#125; \</span></div><div class="line"><span class="string">  &#123;for(i=1;i&lt;=NF;i++) &#123;if($i&gt;max) max=$i; if($i&lt;min) min=$i;&#125;&#125;END \</span></div><div class="line"><span class="string">  &#123;if(max&lt;=126 &amp;&amp; min&lt;59) print "Phred33"; \</span></div><div class="line"><span class="string">  else if(max&gt;73 &amp;&amp; min&gt;=64) print "Phred64"; \</span></div><div class="line"><span class="string">  else if(min&gt;=59 &amp;&amp; min&lt;64 &amp;&amp; max&gt;73) print "Solexa64"; \</span></div><div class="line"><span class="string">  else print "Unknown score encoding"; \</span></div><div class="line"><span class="string">  print "( " min ", " max, ")";&#125;'</span></div></pre></td></tr></table></figure>
<p>将上面这段代码复制到任意一份shell文件中（比如：fq_qual_type.sh），就可以用它来进行质量值类型的检查了。代码的思路其实比较简单，就是截取FASTQ文件的前1000行数据，并抽取出质量值所在的行，分别计算出其中最小和最大的ASCII值，再比较一下就判断出来了。下面给出一个例子，这是我们在本文中用到的FASTQ文件，它是Phred33的：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sh fq_qual_type.sh untreated.fq</div><div class="line">Phred33</div><div class="line">( 34, 67 )</div></pre></td></tr></table></figure>
<p>另外，在查看碱基质量值的过程中，如果你心中存有ASCII码表当然可以直接“看”出各个碱基的质量值，但在实际的场景中都是通过程序直接进行转换处理。下面我就用Python的ord()函数举个转换的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">1</span>]: qual=<span class="string">'JJJJJIIJJJJJJHIHHHGHFFFFFFCEEEEEDBD'</span></div><div class="line">In [<span class="number">2</span>]: [ord(q)<span class="number">-33</span> <span class="keyword">for</span> q <span class="keyword">in</span> qual]</div><div class="line">Out[<span class="number">2</span>]:</div><div class="line">[<span class="number">35</span>, <span class="number">20</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">24</span>, <span class="number">34</span>, <span class="number">35</span>, <span class="number">35</span>, <span class="number">35</span>, <span class="number">34</span>, <span class="number">35</span>, <span class="number">34</span>, <span class="number">29</span>, <span class="number">29</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">34</span>, <span class="number">34</span>, <span class="number">33</span>, </div><div class="line"> <span class="number">29</span>, <span class="number">33</span>, <span class="number">33</span>, <span class="number">32</span>, <span class="number">35</span>, <span class="number">35</span>, <span class="number">35</span>, <span class="number">34</span>, <span class="number">34</span>, <span class="number">34</span>, <span class="number">34</span>, <span class="number">35</span>, <span class="number">35</span>, <span class="number">34</span>, <span class="number">35</span>, <span class="number">34</span>, <span class="number">35</span>, <span class="number">34</span>, <span class="number">35</span>, </div><div class="line"> <span class="number">34</span>, <span class="number">34</span>, <span class="number">34</span>, <span class="number">35</span>, <span class="number">35</span>, <span class="number">35</span>, <span class="number">35</span>, <span class="number">34</span>, <span class="number">33</span>, <span class="number">33</span>, <span class="number">30</span>, <span class="number">33</span>, <span class="number">24</span>, <span class="number">27</span>]</div></pre></td></tr></table></figure>
<p>这里的ord()函数会将字符转换为ASCII对应的数字，减掉33后就得到了该碱基最后的质量值（即，Phred quality score）。</p>
<p>另外，根据上面phred quality score的计算公式，我们可以很方便地获得每个测序碱基的错误率，这个错误率在我们的比对和变异检测中都十分重要，后续文章中我将会讲述该部分的具体内容，以下先给出一个转换的例子，还是以上述qual为例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">In [<span class="number">1</span>]: qual=<span class="string">'JJJJJIIJJJJJJHIHHHGHFFFFFFCEEEEEDBD'</span></div><div class="line">In [<span class="number">2</span>]: phred_score = [ord(q)<span class="number">-33</span> <span class="keyword">for</span> q <span class="keyword">in</span> qual]</div><div class="line">In [<span class="number">3</span>]: [<span class="number">10</span>**(-q/<span class="number">10.0</span>) <span class="keyword">for</span> q <span class="keyword">in</span> phred_score]</div><div class="line">Out[<span class="number">3</span>]:</div><div class="line">[<span class="number">3e-04</span>, <span class="number">1e-02</span>, <span class="number">2e-02</span>, <span class="number">2e-02</span>, <span class="number">4e-03</span>, <span class="number">4e-04</span>, <span class="number">3e-04</span>, <span class="number">3e-04</span>, <span class="number">3e-04</span>, </div><div class="line"> <span class="number">4e-04</span>, <span class="number">3e-04</span>, <span class="number">4e-04</span>, <span class="number">1e-03</span>, <span class="number">1e-03</span>, <span class="number">6e-04</span>, <span class="number">6e-04</span>, <span class="number">4e-04</span>, <span class="number">4e-04</span>, </div><div class="line"> <span class="number">5e-04</span>, <span class="number">1e-03</span>, <span class="number">5e-04</span>, <span class="number">5e-04</span>, <span class="number">6e-04</span>, <span class="number">3e-04</span>, <span class="number">3e-04</span>, <span class="number">3e-04</span>, <span class="number">4e-04</span>, </div><div class="line"> <span class="number">4e-04</span>, <span class="number">4e-04</span>, <span class="number">4e-04</span>, <span class="number">3e-04</span>, <span class="number">3e-04</span>, <span class="number">4e-04</span>, <span class="number">3e-04</span>, <span class="number">4e-04</span>, <span class="number">3e-04</span>, </div><div class="line"> <span class="number">4e-04</span>, <span class="number">3e-04</span>, <span class="number">4e-04</span>, <span class="number">4e-04</span>, <span class="number">4e-04</span>, <span class="number">3e-04</span>, <span class="number">3e-04</span>, <span class="number">3e-04</span>, <span class="number">3e-04</span>, </div><div class="line"> <span class="number">4e-04</span>, <span class="number">5e-04</span>, <span class="number">5e-04</span>, <span class="number">1e-03</span>, <span class="number">5e-04</span>, <span class="number">4e-03</span>, <span class="number">2e-03</span>]</div></pre></td></tr></table></figure>
<p>这其实就是根据phred quality score的定义进行简单的指数运算。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>到这里就说完了，虽然一开始只不过是想介绍两个普通的文件格式，但写着写着就变得很长，可见，越是看似简单的东西，其实越不容易说明白。关于FASTQ还有很多需要说的内容，我打算将其留到该系列的第四篇文章里，到时我会讲述该如何构造流程对其进行有效的数据质控等，这都是构造WGS分析流程之前非常重要的内容。</p>
<p>我一直觉得，生物信息学（或者说基因组学）中的许多数据文件，它们的格式都有着比较特殊的一面，为了能够真正有效地进行数据分析，多花些时间搞清楚它们的细节和来龙去脉是非常重要的。不然，你有可能在后续的数据分析过程掉入意想不到的陷阱，从而浪费大量宝贵的时间去寻找可能出错的地方。</p>
<p>欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/helixminer_wechat_qr.png" alt="解螺旋的矿工"></p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
            <category> 基因组学 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NGS </tag>
            
            <tag> WGS </tag>
            
            <tag> 数据格式 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[从零开始完整学习全基因组测序（WGS）数据分析：第1节 测序技术]]></title>
      <url>/2017/08/04/2017-08-04-Begining-WGS-Data-Analysis-Sequecing-Tech.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/coverge.png" alt="sequencing_cover"></p>
<p><strong>前言</strong></p>
<p>基因测序已是时下热门，目前除了华大基因之外，其他分布于全中国的大型测序平台（HiSeq X 10）还有约10个，每个每年大概能完成1.8万人的高深度全基因组测序，加起来就是18万人，如果加上华大，可能需要翻倍！而且随着新技术的快速发展和成本的下降，WGS正变得越来越普遍！再加上国家十三五规划已经提出了构建大规模中国人群遗传队列图谱的要求，全基因组测序技术正在逐渐替代其它测序手段，这也是我打算写这一个系列的原因。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/01.hiseqX10_distribution.png" alt="HiSeq X 10分布（来源：转化医学网）">                                           </p>
<p align="center"><a>HiSeq X 10分布（来源：转化医学网）</a></p>     

<p>首先，全基因组测序的英文是Whole Genome Sequencing，简称WGS，目前默认指的是人类的全基因组测序。所谓全（Whole），指的就是 <strong>把物种细胞里面完整的基因组序列从第1个DNA开始一直到最后一个DNA，完完整整地检测出来，并排列好，因此这个技术几乎能够鉴定出基因组上任何类型的突变。对于人类来说，全基因组测序的价值是极大的，它的信息包含了所有基因和生命特征之间的内在关联性，当然也意味着更大的数据解读和更高的技术挑战</strong>。但，没关系，在这个系列中，我将从测序技术、常用文件解析，数据质控和流程构建等各个方面结合实际的例子，详细阐述什么是全基因组测序以及 <strong>该如何构造流程</strong> 分析全基因组测序（WGS）数据。</p>
<p>这是这一组学入门技术系列的第一篇（这篇文章修改自我以前的<a href="http://www.huangshujia.me/2013/08/02/2013-08-02-An-Introduction-of-NGS-Sequence.html" target="_blank" rel="external">一篇博客</a>，该文也已被各种形式转载），我首先将介绍当前的基因组测序原理及其发展历程。</p>
<p><strong>第一节 NGS测序技术</strong></p>
<p>在真正开始数据分析之前先知道我们是如何将那些原本存在于细胞中的DNA信息获取出来的——也就是测序的原理，总是有益的。</p>
<p>测序，简单来说就是将DNA化学信号转变为计算机可处理的数字信号。</p>
<p>它从1977年的第一代Sanger技术发展至今，已经足有40年时间。在这个技术发展的更迭历程中，<strong>测序读长从长到短，再从短到长</strong>。虽然就当前形势看第二代短读长测序技术在全球范围内上占有着绝对的垄断位置，但第三测序技术也已在这几年快速地发展着。测序技术的每一次变革和突破，都对基因组学研究，疾病医疗研究，药物研发，育种等领域产生巨大的推动作用。所以在这个系列的第一篇里我将对当前最主流的测序技术以及它们的测序原理做一个全面的介绍。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/02.ngs_history.png" alt="图1. 测序技术发展历程"></p>
<p align="center"><a>图1. 测序技术发展历程</a></p>     

<p><strong>第一代测序技术</strong></p>
<p>第一代DNA测序技术用的是1975年由桑格（Sanger）和考尔森（Coulson）开创的链终止法或者是1976-1977年由马克西姆（Maxam）和吉尔伯特（Gilbert）发明的化学法（链降解）. 并在1977年，由桑格老人家测定了第一个基因组序列——噬菌体phiX-174，全长只有5,375个碱基。虽然与今日的技术比起来根本不算什么，但自此之后，人类获得了窥探生命本质的能力，并以此为开端真正步入了基因组学时代。</p>
<p>研究人员在Sanger法的多年实践之中不断对其进行改进。在2001年，完成的首个人类基因组图谱就是以改进了的Sanger法为基础进行测序的。<strong>Sanger法的核心原理是：由于ddNTP（4种带有荧光标记的A,C,G,T碱基）的2’和3’都不含羟基，其在DNA的合成过程中不能形成磷酸二酯键，因此可以用来中断DNA的合成反应，在4个DNA合成反应体系中分别加入一定比例带有放射性同位素标记的ddNTP（分别为：ddATP,ddCTP,ddGTP和ddTTP），然后利用凝胶电泳和放射自显影后可以根据电泳带的位置确定待测分子的DNA序列（图2）</strong>。这个<a href="http://smcg.cifn.unam.mx/enp-unam/03-EstructuraDelGenoma/animaciones/secuencia.swf" target="_blank" rel="external">网址</a>为Sanger测序法制作了一个小短片，形象而生动。<br>值得注意的是，在测序技术起步发展的这一时期中，除了Sanger法之外还出现了一些其他的测序技术，如焦磷酸测序法、连接酶法等。其中，焦磷酸测序法是后来Roche公司454技术所使用的测序方法，而连接酶测序法是后来ABI公司SOLID使用的测序方法，但他们的核心手段都是利用了Sanger中可中断DNA合成反应的dNTP。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/03.sanger.png" alt="图2. Sanger测序发原理"></p>
<p align="center"><a>图2. Sanger测序发原理</a></p>     

<p><strong>第二代测序技术</strong></p>
<p>总的来说，第一代测序技术的主要特点是测序读长可达1,000bp，准确性高达99.999%，但其测序成本高，通量低等方面的缺点，严重影响了其真正大规模的应用。因而第一代测序技术并不是理想的测序方法。经过不断的技术开发和改进，以Roche公司的454技术、illumina公司的Solexa/HiSeq技术和ABI公司的SOLID技术为标记的第二代测序技术诞生了。第二代测序技术在大幅提高了测序速度的同时，还大大地降低了测序成本，并且保持了高准确性，<strong>以前完成一个人类基因组的测序需要3年时间，而使用二代测序技术则仅仅需要1周，但其序列读长方面比起第一代测序技术则要短很多，大多只有100bp-150bp</strong>。图3. 是第一代和第二代测序技术测序成本作了一个简单的比较，可以看出自第二代测序技术发展出来之后，历史开始发生根本性的改变，测序的成本开始快速实现断崖式下降，也就是业内经常提到的 <strong>超摩尔定律</strong> 现象。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/04.sequencing_cost.png" alt="图3. 测序成本比较（来源：NIH网站）">                                          </p>
<p align="center"><a>图3. 测序成本比较（来源：NIH网站）</a></p>     

<p>接下来我以illumina（目前最大、最成功的NGS测序仪公司）的技术为基础简要单介绍第二代测序测序技术的原理和特点。</p>
<p>目前illumina的测序仪占全球75%以上，以HiSeq系列为主。它的机器采用的都是边合成边测序的方法，主要分为以下4个步骤：</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/05.illumina_1.png" alt="图4. illumina测序原理（来源：illumina官网）"></p>
<p align="center"><a>图4. illumina测序原理（来源：illumina官网）</a></p>     

<p>1）构建DNA测序文库，图4-1</p>
<p>简单来说就是把一堆乱糟糟的DNA分子用超声波打断成一定长度范围的小片段。目前除了一些特殊的需求之外，基本都是打断为300bp-800bp长的序列片段，并在这些小片段的两端添加上不同的接头【注】，构建出单链DNA文库，以备测序之用；</p>
<blockquote>
<p>【注】接头在illumina中一般分为P5和P7接头，其中一个带有和flowcell上的探针反向互补的序列，以完成待测序列和探针结合的作用，另外一个接头带有barcord序列以区分不同的样本。</p>
</blockquote>
<p>2）测序流动槽（flowcell），图4-2</p>
<p>flowcell是用于吸附流动DNA片段的槽道，也是核心的测序反应容器——所有的测序过程就发生在这里。当文库建好后，这些文库中的DNA在通过flowcell的时候会随机附着在flowcell表面的槽道（称为lane）上。每个flowcell有8个lane（图5），每个lane的表面都附有很多接头，这些接头能和建库过程中加在DNA片段两端的接头相互配对，这就是为什么flowcell能吸附建库后的DNA的原因，并能支持DNA在其表面进行桥式PCR的扩增，理论上这些lane之间是不会相互影响的。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/06.flowcell.png" alt="图5. flowcell（实物 VS 示意图）"></p>
<p align="center"><a>图5. flowcell（实物 VS 示意图）</a></p>     

<p>3）桥式PCR扩增与变性</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/07.illumina_2.png" alt="图6. 桥式PCR扩增（来源：illumina官网）"></p>
<p align="center"><a>图6. 桥式PCR扩增（来源：illumina官网）</a></p>     

<p>这是NGS技术的一个核心特点。桥式PCR以flowcell表面所固定的序列为模板，进行桥形扩增，如图6所示。经过不断的扩增和变性循环，最终每个DNA片段都将在各自的位置上集中成束，每一个束都含有单个DNA模板的很多分拷贝，这一过程的目的在于实现将单一碱基的信号强度进行放大，以达到测序所需的信号要求。</p>
<p>4）测序，如图4-4和图7所示</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/08.illumina_3.png" alt="图7. 边合成边测序（来源：illumina官网）"></p>
<p align="center"><a>图7. 边合成边测序（来源：illumina官网）</a></p>     

<p>测序方法采用边合成边测序的方法。向反应体系中同时添加DNA聚合酶、接头引物和带有碱基特异荧光标记的4中dNTP（如同Sanger测序法）。<strong>这些dNTP的3’-OH被化学方法所保护，因而每次只能添加一个dNTP，这就确保了在测序过程中，一次只会被添加一个碱基。同时在dNTP被添加到合成链上后，所有未使用的游离dNTP和DNA聚合酶会被洗脱掉。接着，再加入激发荧光所需的缓冲液，用激光激发荧光信号（图7），并有光学设备完成荧光信号的记录，最后利用计算机分析将光学信号转化为测序碱基</strong>。这样荧光信号记录完成后，再加入化学试剂淬灭荧光信号并去除dNTP 3’-OH保护基团，以便能进行下一轮的测序反应。</p>
<p>Illumina的这种每次只添加一个dNTP的技术特点能够很好的地解决同聚物长度的准确测量问题，它的主要测序错误来源是碱基的替换，目前它的测序错误率在1%-1.5%左右。测序周期以人类基因组重测序为例，30x-50x测序深度对于Hisq系列需要3-5天时间，而对于2017年初最新推出的NovaSeq系列则只需要40个小时！</p>
<p>表1. 测序量比较（双流动槽为例，如为单流动槽则测序量减少为下表的一半，时间不变）<br><img src="http://blog-fungenomics-com.qiniudn.com/09.novaseq_vs_hiseq.png" alt="novaseq_vs_hiseq"><br><em>一次测序的数据总产量的单位Gb，不是计算机字节，而是测序碱基的数目（Giga base）</em></p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/09.NovaSeq_cost.png" alt="图8. NovaSeq与其他测序仪测序通量的比较（来源：illumina官网）"></p>
<p align="center"><a>图8. NovaSeq与其他测序仪测序通量的比较（来源：illumina官网）</a></p>     

<p>上面表1和图8是NovaSeq和其他测序系列的比较，数据相当好。按照这个数据量估算，一台NovaSeq 6000（S4）在跑满的情况下，<strong>一年就可以测序6400多人！</strong>而且按照以往的经验，illumina的官方公布的数据都是偏于保守的，我们在实际的使用过程中发现 <strong>高质量（Q30）的read其实占到了总数据的90%以上，远高于官方公布的75%，数据的总产量也同样更高</strong>。</p>
<p><strong>第三代测序技术</strong></p>
<p>这是一个新的里程碑。以PacBio公司的SMRT和Oxford Nanopore Technologies的纳米孔单分子测序技术为标志，被称之为第三代测序技术。与前两代相比，最大的特点就是 <strong>单分子测序，测序过程无需进行PCR扩增，超长读长</strong>，以下图9是PacBio SMRT技术的测序读长分布情况，平均达到10Kb-15Kb，是二代测序技术的100倍以上，值得注意的是在测序过程中这些序列的读长也不再是相等的，下文有解析！</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/10.pacbio_seqlength.png" alt="图9. PacBio SMRT 测序read读长分布（来源：PacBio官网）"></p>
<p align="center"><a>图9. PacBio SMRT 测序read读长分布（来源：PacBio官网）</a></p>     

<p><strong>PacBio SMRT</strong></p>
<p>PacBio SMRT技术其实也应用了边合成边测序的思想，并以SMRT芯片为测序载体（如同flowcell）。基本原理是： DNA聚合酶和模板结合，用4色荧光标记A,C,G,T这4种碱基（即是dNTP）。在碱基的配对阶段，不同的碱基加入，会发出不同的光，根据光的波长与峰值可判断进入的碱基类型。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/11.pacbio_smrt.png" alt="图10. PacBio SMRT 测序原理"></p>
<p align="center"><a>图10. PacBio SMRT 测序原理</a></p>     

<p>这个DNA聚合酶是实现超长读长的关键之一，<strong>读长主要跟酶的活性保持有关</strong>，它主要受激光对其造成的损伤所影响。PacBio SMRT技术的一个关键点是在于如何将反应信号与周围游离碱基的强大荧光背景区别出来。他们利用的是ZMW（零模波导孔）原理：如同微波炉壁上可看到的很多密集小孔。这些小孔的直径是有严格要求的，如果直径大于微波波长，能量就会在衍射效应的作用下穿透面板从而泄露出来（光波的衍射效应），从而与周围小孔相互干扰（光波的干涉）。如果孔径能够小于波长，那么能量就不会辐射到周围，而是保持直线状态，从而可起到保护的作用。同理，<strong>在一个反应管(SMRTCell:单分子实时反应孔)中有许多这样的圆形纳米小孔,，即 ZMW(零模波导孔)，外径100多纳米，比检测激光波长小(数百纳米)，激光从底部打上去后不会穿透小孔进入上方的溶液区，能量会被限制在一个小范围(体积20X 10-21 L)里（图10-A），正好足够覆盖需要检测的部分，使得信号仅仅只是来自于这个小反应区域，孔外过多的游离核苷酸单体依然留在黑暗中，从而实现将背景噪音降到最低的目的</strong>。</p>
<p>PacBio SMRT技术除了能够检测普通的碱基之外，还可以通过检测相邻两个碱基之间的测序时间，来检测碱基的表观修饰情况，如甲基化。因为假设某个碱基存在表观修饰，则通过聚合酶时的速度会减慢，那么相邻两峰之间的距离会增大，我们可以通过这个时间上的差异来检测表观甲基化修饰等信息（图11）。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/12.pacbio_methy.png" alt="图11. PacBio SMRT 检测甲基化修饰（来源：PacBio官网）"></p>
<p align="center"><a>图11. PacBio SMRT 检测甲基化修饰（来源：PacBio官网）</a></p>     

<p>SMRT技术的测序速度很快，每秒约10个dNTP。但这么快的测序速度也带来了一些明显的缺点——测序错误率比较高（这几乎是目前单分子测序技术的通病），可以达到10%-15%，而且以缺失序列和错位居多，但好在它的出错是随机的，并不会像第二代测序技术那样存在一定的碱基偏向，因此可以通过多次测序来进行有效纠错。</p>
<p><strong>Oxford Nanopore</strong></p>
<p>Oxford Nanopore 的MinION是另一个比较受关注的第三代测序仪，俗称U盘测序仪，<strong><em>它真的很小，我亲手拿过，并拆过，图12（左）</em></strong>！这家公司开发的纳米单分子测序技术与以往的测序技术相比都不一样，它是基于电信号而不是光信号的测序技术！</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/13.minION.png" alt="图12. Oxford Nanopore MinION"></p>
<p align="center"><a>图12. Oxford Nanopore MinION</a></p>     


<p>这个技术的关键点在于他们所设计的一种特殊纳米孔，孔内共价结合分子接头。当DNA分子通过纳米孔时，它们使电荷发生变化，从而短暂地影响流过纳米孔的电流强度（每种碱基所影响的电流变化幅度是不同的），最后高灵敏度的电子设备检测到这些变化从而鉴定所通过的碱基（图13）。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/13.MinION2.png" alt="图13. MinION 测序原理"></p>
<p align="center"><a>图13. MinION 测序原理</a></p>     

<p>纳米孔测序以及其他第三代测序技术，有可能会彻底地解决目前第二代测序平台的诸多不足。另外，MinION的主要特点是：读长很长，而且比PacBio的都长得多，基本都是在几十kb上百kb以上，最新的数据显示可以达到900 kb！错误率是5%-15%，也是随机错误，MinION最大的特点除了极小的体积之外，就是数据将是可实时读取的，并且起始DNA在测序过程中不被破坏！这真是个可以上天的能力。然鹅，遗憾地多说几句，目前还没真正公布，细节也不知，自从2012开过一次发布会之后，就没什么声响了。</p>
<p>这种纳米孔单分子测序仪还有另一大特点，它能够 <strong>直接</strong> 读取出甲基化的胞嘧啶，而不必像二代测序方法那样需要事先对基因组进行bisulfite处理。这对于在基因组水平直接研究表观遗传相关现象有极大的帮助。下面是对PacBio和Oxford Nanopore这两家第三代测序技术公司的测序仪做的一个简单比较，可以看出其实成本还是蛮高的，质量也只是还行，期待他们的下一次进化吧。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/14.compare.png" alt="cost_compare"></p>
<p><strong>总结</strong></p>
<p>以上，便是对各代测序技术的原理做了简要的阐述。在这个比较的过程中，可以看到测序成本，读长和通量是该测序技术先进与否的三个重要指标。其实第一代和第二代测序技术除了通量和成本上的差异之外，测序的核心原理都来自于边合成边测序的思想。第二代测序技术的优点是通量大大提升，成本大大减低，<strong>使得昔日王榭堂前燕，可以飞入寻常百姓家。总之，只有变成白菜价，才能真正对大众有意义</strong>；但它的缺点是所引入PCR过程会在一定程度上增加测序的错误率，并且具有系统偏向性，同时读长也比较短。第三代测序技术是为了解决第二代所存在的缺点而开发的，它的根本特点是单分子测序，不需要任何PCR的过程，这是为了能有效避免因PCR偏向性而导致的系统错误，同时提高读长，但这个技术还不是很成熟，需要再进化，成本也偏高。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/15.sequencing_map.png" alt="图14. 全球测序仪数量分布"></p>
<p align="center"><a>图14. 全球测序仪数量分布</a></p>

<p>参考文献</p>
<blockquote>
<ol>
<li>Sanger, F. &amp; Nicklen, S. DNA sequencing with chain-terminating. <strong>74</strong>, 5463–5467 (1977).</li>
<li>Mardis, E. R. Next-generation DNA sequencing methods. <em>Annual review of genomics and human genetics</em> <strong>9</strong>, 387–402 (2008).</li>
<li>Shendure, J. &amp; Ji, H. Next-generation DNA sequencing. <em>Nature biotechnology</em> <strong>26</strong>, 1135–45 (2008).</li>
<li>Metzker, M. L. Sequencing technologies - the next generation. <em>Nature reviews. Genetics</em> <strong>11</strong>, 31–46 (2010).</li>
<li>Niedringhaus, T. P., Milanova, D., Kerby, M. B., Snyder, M. P. &amp; Barron, A. E. Landscape of Next-Generation Sequencing Technologies. 4327–4341 (2011).</li>
<li>Rothberg, J. M. <em>et al.</em> An integrated semiconductor device enabling non-optical genome sequencing. <em>Nature</em> <strong>475</strong>, 348–52 (2011).</li>
</ol>
</blockquote>
<p>欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/helixminer_wechat_qr.png" alt="解螺旋的矿工"></p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
            <category> 基因组学 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NGS </tag>
            
            <tag> 测序技术 </tag>
            
            <tag> WGS </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于深度学习的序列模型预测非编码区变异的功能效应]]></title>
      <url>/2017/05/30/Predicting-effects-of-noncoding-variants-with-deep-learning-based-sequence-model.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/dna.png" alt="cover"></p>
<p>Deep Learning，现在几乎到处都能看到它的应用。看！紧随DeepBind，在基因组学应用中又来了一个DeepSEA——这是一个适用于表观遗传研究和应用的工具，它只从DNA序列出发，并没用其他有关于表观研究的实验或者测序技术，通过直接输入fasta sequence，vcf或者bed文件，就可以预测转录因子结合位点(Transcription factors binding site), DNase I超敏感位点（DNase I hypersensitive sites）和组蛋白靶点（histone marks），这么多年来，这样的做法还是头一回。下面这张示意图展示的是各个主要的表观修饰在染色体中的位置和相关实验测定技术。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/epi.png" alt="EPI"></p>
<p>为什么要有这么个东西呢？</p>
<p>众所周知，人类基因组上绝大部分的序列都是非编码序列——不直接编码蛋白质的序列，这些序列在很长的一段时间里都被误解为所谓的“垃圾DNA”！但其实它们各自都有着独特的作用——调控着机体的正常运作，只是要想正确地理解它们确实不是一个容易的事情。DeepSEA想要干的就是尝试从序列的基础功能预测着手去解决这么一个难题。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/deepsea.png" alt="deepsea"></p>
<p>它先通过学习大量已知的染色质修饰数据——主要来自于<a href="https://www.encodeproject.org/" target="_blank" rel="external">ENCODE</a>和<a href="http://www.roadmapepigenomics.org/" target="_blank" rel="external">Roadmap Epigenomics</a>等大型项目，经过不断的训练，学习到了许多种在非编码区域中序列调控的序列模式或者说是序列特征（注意是序列模式，不是功能模式），之后，便可以通过这些模式和特征去预测序列上单碱基的突变会如何影响染色质的修饰功能。从发表的文章来看，其精确程度是目前所有方案中最高也是在同等数据下最有效的了。</p>
<blockquote>
<p>DeepSEA 在Nature Method的原文<a href="http://www.nature.com/nmeth/journal/v12/n10/full/nmeth.3547.html" target="_blank" rel="external">http://www.nature.com/nmeth/journal/v12/n10/full/nmeth.3547.html</a><br>更赞的是它的代码和相关训练数据都一起公开在网站上：<a href="http://deepsea.princeton.edu/" target="_blank" rel="external">http://deepsea.princeton.edu/</a> 可以尝试玩起来了。</p>
</blockquote>
<p>【注】本文首发于<a href="http://www.fungenomics.com/article/31" target="_blank" rel="external">泛基因</a>和公众号。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 非编码区 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 功能预测 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[GATK中如何计算Inbreeding coefficient（近交系数）]]></title>
      <url>/2017/05/28/how-to-calculate-inbreading-coefficient-in-GATK.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/ic_cover.jpg" alt="cover"></p>
<p>关于近交系数是什么的定义，除了英文资料，中文上也给出了清晰的定义，这里引用一下：</p>
<blockquote>
<p>近交系数（inbreeding coefficient）是指根据近亲交配的世代数，将基因的纯化程度用百分数来表示即为近交系数，也指个体由于近交而造成异质基因减少时，同质基因或纯合子所占的百分比也叫近交系数，普遍以F或f来表示。</p>
</blockquote>
<p>GATK近交系数的计算程序在github上可以找到：<a href="https://github.com/broadgsa/gatk-protected/blob/f185a75e1c49fb4c039511e61254da0509833ee9/protected/gatk-tools-protected/src/main/java/org/broadinstitute/gatk/tools/walkers/annotator/AS_InbreedingCoeff.java" target="_blank" rel="external">AS_InbreedingCoeff.java</a></p>
<p>代码不短，但计算很简单，我主要说展示一下这个计算的核心部分并在代码中做些注释，如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">double</span> <span class="title">calculateIC</span><span class="params">(<span class="keyword">final</span> VariantContext vc, <span class="keyword">final</span> Allele altAllele)</span> </span>&#123;</div><div class="line">    <span class="keyword">final</span> <span class="keyword">int</span> AN = vc.getCalledChrCount();</div><div class="line">    <span class="keyword">final</span> <span class="keyword">double</span> altAF;</div><div class="line"></div><div class="line">    <span class="keyword">final</span> <span class="keyword">double</span> hetCount = heterozygosityUtils.getHetCount(vc, altAllele);</div><div class="line"></div><div class="line">    <span class="keyword">final</span> <span class="keyword">double</span> F;</div><div class="line">    <span class="comment">//shortcut to get a value closer to the non-alleleSpecific value for bialleleics</span></div><div class="line">    <span class="keyword">if</span> (vc.isBiallelic()) &#123;</div><div class="line">        <span class="keyword">double</span> refAC = heterozygosityUtils.getAlleleCount(vc, vc.getReference());</div><div class="line">        <span class="keyword">double</span> altAC = heterozygosityUtils.getAlleleCount(vc, altAllele);</div><div class="line">        <span class="keyword">double</span> refAF = refAC/(altAC+refAC);</div><div class="line">        altAF = <span class="number">1</span> - refAF;</div><div class="line">        F = <span class="number">1.0</span> - (hetCount / (<span class="number">2.0</span> * refAF * altAF * (<span class="keyword">double</span>) heterozygosityUtils.getSampleCount())); <span class="comment">// inbreeding coefficient</span></div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">//compare number of hets for this allele (and any other second allele) with the expectation based on AFs</span></div><div class="line">        <span class="comment">//derive the altAF from the likelihoods to account for any accumulation of fractional counts from non-primary likelihoods,</span></div><div class="line">        <span class="comment">//e.g. for a GQ10 variant, the probability of the call will be ~0.9 and the second best call will be ~0.1 so adding up those 0.1s for het counts can dramatically change the AF compared with integer counts</span></div><div class="line">        altAF = heterozygosityUtils.getAlleleCount(vc, altAllele)/ (<span class="keyword">double</span>) AN;</div><div class="line"></div><div class="line">        <span class="comment">// 计算inbreeding coefficient</span></div><div class="line">        F = <span class="number">1.0</span> - (hetCount / (<span class="number">2.0</span> * (<span class="number">1</span> - altAF) * altAF * (<span class="keyword">double</span>) heterozygosityUtils.getSampleCount())); <span class="comment">// heterozygosityUtils.getSampleCount() 获取总样本数 </span></div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="keyword">return</span> F;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>总的来说，是利用哈迪温伯格定律来计算的。 1.0 - (hetCount / (2.0 <em> (1 - altAF) </em> altAF(double)N ，N是人数。这个值给出的是期望的杂合变异的个数。所以参数F说的就是”实际的hetCount”除以”期望的hetCount”再与1.0取差。当F值越接近0，就意味着实际的hetCount与理论的hetCount越接近。</p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> GATK </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[癌症基因组学研究全程回顾]]></title>
      <url>/2017/01/02/2017-01-02-cancer-genomics-review.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/dna-epigenome-breast-cancer-early-warning-2.jpg" alt="cover"><br>【注】本文同时发于<a href="http://blog.fungenomics.com/" target="_blank" rel="external">泛基因fungenomics</a>以及微信公众号。</p>
<p>癌症，一种慢性的基因病。</p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在癌症研究中，每个癌症样品呈现在研究人员眼前的已经是一个发生了改变的基因组，其中包含着独特且难以预测的诸多点突变、序列的插入缺失、易位、融合以及其他畸变。并且，这些发生的变异中，许多往往都是之前所未观察到的（Novel mutations），它们也不会只存在于基因组的编码区域中，因而为了能够真正做到全面研究癌症基因组本身所发生的所有突变事件，全基因组测序已被视为肿瘤基因变异研究中 <strong>唯一</strong> 严谨的方法。</p>
<p>然而，在所有这些变异中，却只有少数的几个主导着癌症这一疾病的发展和演变。要有效揭示这一演变和发展的过程，就需要监控基因表达水平上的变化，那么RNA-Seq便是用以确定这些遗传改变是否会影响疾病发展有用技术。但遗传改变有可能影响所有的细胞过程，包括染色质结构、DNA甲基化、RNA剪接异构体、RNA编辑和microRNA（miRNA）等。这就意味着，只有对所有这些独立的过程都进行检测和综合分析，才能在癌症研究中取得真正的突破。这些内容我们都将在下文一一展开。</p>
<p>当前基因组测序技术的一大特点是在于能够在很短的时间内并行测得数十亿（甚至数百亿）的独立序列片段——read，而每个read来源于单个的DNA分子。由此产生的数据我们可将其视为是对DNA分子的随机抽样，这反过来代表肿瘤样品中每个细胞的基因组的情况。这是我们解开癌症的原因和机制的一种强大工具。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_1.png" alt="cancer_review_1"></p>
<h3 id="肿瘤异质性"><a href="#肿瘤异质性" class="headerlink" title="肿瘤异质性"></a>肿瘤异质性</h3><p>癌症基因组的突变是复杂的。</p>
<p>每个人都携带一套独特的来自父母遗传的胚系突变（germline mutations）信息。但随着癌症的发展，体细胞突变（Somatic mutations）和基因组重排（genomics rearrangements）会逐渐增加。这些改变往往会引发耐药性以及转移。越来越多的研究证据表明，这些过程竟然可以是 <strong>有意的！</strong>——它们其实可以认为是癌细胞面临药物刺激的过程中不断进化的结果。我们想要全面地理解这一复发和耐药性的原因，就有必要进行纵向实验，按照癌症的发展过程，分阶段采集样品来进行研究。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_2.png" alt="cancer_review_2"></p>
<p>如上图，这是处于正常组织背景下的多克隆肿瘤（polyclone tumor）。大多数的肿瘤样品都会同时包含肿瘤细胞和正常细胞，如基质细胞、血管和免疫细胞。并且肿瘤本身也常常包含几种不同的克隆亚型（clone types），每一种都有着不同的治疗反应和复发可能。</p>
<p>根据传统病理学的估计，大部分研究的结果其实都只集中在那些肿瘤细胞比例 <em>&gt;60%</em> 的区域中。并且为了确定哪些突变是肿瘤特异的，通常都要包含来自同一个体的正常组织样本作为参考。</p>
<p>然而，肿瘤本身也往往是异质性的。在癌症发展的过程中，个别细胞会发生新的突变，包含这些新突变的细胞会继续增殖，形成克隆亚型。因此，对于晚期癌症我们通常检测到的都是一个多克隆肿瘤，其中每一个克隆有一套独特的突变信息、独特的病理学和药物反应机制。这其实也正是癌症难以完全治疗的原因，而它的这种 <strong>异质性其实正是肿瘤基因组复杂性导致的一种表型性质</strong>。目前的深度测序可以检测样本中含量低至1%的克隆。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_3.png" alt="cancer_review_3"></p>
<blockquote>
<p>肿瘤内的异质性。体细胞突变的逐步累积产生了一个异质的多克隆肿瘤，其中不同克隆可能对治疗的反应不同。</p>
</blockquote>
<p>而且，异质性一般还可以分两种情况讨论，第一种情况指的是：同一个病人的肿瘤细胞具有异质性。处于肿瘤发生的不同时期的肿瘤细胞的基因突变情况不同，造就了每一个肿瘤细胞群体内还有许多亚群（subclones），肿瘤细胞在通过转移时，就会有属于不同亚群的肿瘤细胞去侵入新的地方，形成新的肿瘤；第二种情况指的是：除了同一病人的不同肿瘤细胞会造成肿瘤的异质性外，肿瘤的异质性还体现在不同病人可能得了相同的肿瘤，但是那个『相同』未必真是相同——仅仅是表型相同，不代表着基因型也相同。</p>
<p>在某些基因中，突变频繁发生在同一个位置，这应该有特定的机制在起作用，这些有规律性的情况都会稍微容易对付。然而遗憾的是，对于大多数的基因，突变显然是随机出现在整个基因中的，这其实说明了DNA的复制和修复机制失灵了。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_4.png" alt="cancer_review_4"><br>图中为两个假定的基因，他们有着两种不同的突变模式。深灰色框代表外显子组，而红色柱子代表存在突变的位置。A: 特定位置的频发突变可能表示有产生突变的生物学机制的参与。B: 散发突变存在于整个基因中，如P53，可能是由于复制和修复机制的失效。我们可以通过测序检测这两种情况下的突变。</p>
<p><strong>参考文献</strong></p>
<blockquote>
<p>2012年Ding Li等人发表在Natrue上的一项研究弄清了急性髓细胞白血病（AML）的复发原因。他们发现了两个一般机理：（1）原发肿瘤中的始发克隆（founding clone）获得突变，进化成复发克隆；（2）始发克隆的一个亚克隆挺过了初次治疗，获得了更多突变并在复发时增殖。在一个病例中，原发肿瘤里原本仅占5.1%的亚克隆在复发后却成长为主要的克隆。而且对于所有的病例，化疗并不能够根除这些始发克隆。这项研究直接表明了在诊断后和初次治疗后检测并根除掉那些原本不起眼的小细胞群体有多么的重要！<br><strong>Ding L., Ley T. J., Larson D. E., Miller C. A., Koboldt D. C., et al. (2012) Clonal evolution in relapsed acute myeloid leukaemia revealed by whole-genome sequencing. Nature 481: 506-510</strong> </p>
<p>同样是2012年，同样是Ding Li这波人，同样是研究AML，他们这次发现大约三分之一的骨髓增生异常综合征患者会发展成继发性急性髓细胞白血病(AML)。这个研究的目的是为了确定骨髓增生异常综合征中的突变，它们有可能预测AML的进展。他们对七名继发性AML患者的七组皮肤和骨髓样品以及先前的骨髓增生异常综合征的配对骨髓样品一并做了全基因组测序。结果发现，在所有病例中，占主导地位的继发性AML克隆都是来自一个骨髓增生异常综合征的始发克隆。这其实就是说，骨髓增生异常综合征样品包含了对预后很重要的突变，针对这些突变的治疗方案是可能改善预后的。<br><strong>Walter M. J., Shen D., Ding L., Shao J., Koboldt D. C., et al. (2012) Clonal architecture of secondary acute myeloid leukemia. N Engl J Med 366: 1090-1098</strong></p>
<p>继续肿瘤治疗和预后的话题，Gerlinger M这一波人发起了一项研究，利用全外显子组测序来研究多个样品，这些样品不仅来自两名患者体内的原发肾癌，还包括了患者相关转移部位的空间分隔区域。最后，他们在原发肿瘤中看到了广泛存在的异质性现象！而且还特别指出了在每个肿瘤区域内有63%-69%的体细胞突变是无法检测到的。同时，还检测了同一肿瘤不同区域内预后良好和不良的基因表达特征。这其实还是突出了在突变积累之前早期诊断的重要性，以及对较大肿瘤需要进行多个部位的活检。利用同一名患者的多个样本得到的信息，他们能够重建疾病的发展进程！这是一种极为强大的方法，不仅能检测引发事件，还能检测表现出平行进化的基因。平行进化通常是基因在进化压力下的一种表现，其实也表示那些基因可能成为有效的治疗靶点。<br><strong>Gerlinger M., Rowan A. J., Horswell S., Larkin J., Endesfelder D., et al. (2012) Intratumor heterogeneity and branched evolution revealed by multiregion sequencing. N Engl J Med 366: 883- 892</strong>。</p>
</blockquote>
<h4 id="转移"><a href="#转移" class="headerlink" title="转移"></a>转移</h4><p>肿瘤的转移是一个复杂的过程，其中癌细胞脱离原发肿瘤，通过血液或者淋巴系统循环到身体的其他部位。在新部位，细胞继续繁殖，最终形成更多肿瘤，这些肿瘤包含了反映其组织来源的细胞。肿瘤（胰腺癌和葡萄膜肿瘤）转移的能力大大增加了它们的致死性。关于转移肿瘤的克隆结构、转移酶之间的系统发育关系、转移和原发部位的平行进化规模，肿瘤如何扩散，以及肿瘤微环境在转移部位决定中的作用如何等，许多这些基本问题目前仍然没有很好地解决。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_5.png" alt="cancer_review_5"></p>
<blockquote>
<p>转移瘤可能来源于原发肿瘤中一个主要克隆（如上图：Metastasis1），也可能来源于次要克隆（Metastasis2）。转移瘤也会经历克隆进化（如Metastasis1所示） </p>
</blockquote>
<p>参考文献</p>
<blockquote>
<p>Hsieh A. C., Liu Y., Edlind M. P., Ingolia N. T., Janes M. R., et al. (2012) The translational landscape of mTOR signalling steers cancer initiation and metastasis. Nature 485: 55-61</p>
<p>这篇文章证明了前列腺癌基因组经由致癌mTOR通路的特殊翻译机制，这其中产生了一个特定的基因群，它们参与了细胞增殖、代谢和侵袭。随后作者对一类翻译控制前侵袭信使RNA进行了功能鉴定，并指出了这些mRNA主导了癌症侵袭和转移。</p>
</blockquote>
<h3 id="基因组突变"><a href="#基因组突变" class="headerlink" title="基因组突变"></a>基因组突变</h3><p>所有的肿瘤在其发展的过程中都会不断积累体细胞突变（somatic mutations）。大多数常见的肿瘤与不同的癌基因相关联，这些癌基因以低频率发生突变。从大型癌症数据库中观察到的一个最令人惊讶的现象是癌症间甚至各个癌症类型内的显著遗传异质性。然而，似乎只有有限的细胞通路对肿瘤的细胞生物学很重要。目前很多人正在编辑收录各种癌症类型的体细胞突变综合列表，这对于更好地了解这种疾病背后的机制将有很大的指引作用。</p>
<p><strong>研究参考</strong></p>
<blockquote>
<p>Nik-Zainal S., Alexandrov L. B., Wedge D. C., Van Loo P., Greenman C. D., et al. Mutational processes molding the genomes of 21 breast cancers. Cell 149: 979-993<br>这篇文章中研究了21个乳腺癌基因组，并给出了它的一个体细胞突变列表。发现带BRCA1或者BRCA2突变的癌症会有一种特别的替换突变特征和与众不同的缺失图谱。文章中还描述了一种局部的超突变现象，这称为『kataegis』(kataegis，希腊语中『雷雨』的意思，文中指的是在一个小区域中出现大量突变的机制，如下图)。并且这些区域中的碱基替换几乎都发生在TpC二核苷酸的胞嘧啶上！</p>
</blockquote>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_6.png" alt="cancer_review_6"></p>
<blockquote>
<p>这是Kataegis图像。纵轴是突变间距（对数刻度）。这个图中基因组内大部分的突变都有着$$~10^6bp$$至$$~10^6bp$$的突变距离。其中超突变区即是表现为突变间距较低的簇。</p>
<p><strong>Govindan R., Ding L., Griffith M., Subramanian J., Dees N. D., et al. Genomic landscape of non-small cell lung cancer in smokers and never-smokers. Cell 150: 1121-1134</strong><br>这是另一篇文章，主要是对17个非小细胞肺癌（NSCLC）患者的肺癌及癌旁正常组织样本进行了全基因组和转录组测序。值得注意的是吸烟者中所观察到的突变频率比不吸烟者高10倍！这是通过深度测序揭示出的这些群体间所不同的克隆模式。而且其中所有经过验证的EGFR和KRAS突变都存在于原始克隆中，这其实也就表明了它们在癌症启动中可能发挥非常重要的作用。</p>
</blockquote>
<h3 id="镶嵌性"><a href="#镶嵌性" class="headerlink" title="镶嵌性"></a>镶嵌性</h3><p>对于这个现象我们也同样通过实际的研究来说明。AML（急性髓细胞白血病）基因组中发现的大多数突变实际上是随机事件，在造血干细胞/祖细胞（HSPC）获得原始突变之前就存在了；但是随着克隆的扩增，细胞的突变历史被『捕获』了。如何理解这里的『捕获』？其实说的就是，原本那些突变都没什么鸟用，就摆在那无所事事，但是，在许多情况下，偏偏只需要再来一个或者两个额外的突变来协助就能共同作用，最后产生恶性的原始肿瘤克隆！</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_7.png" alt="cancer_review_7"></p>
<blockquote>
<p>原发肿瘤和转移的镶嵌性。非同义点突变和插入缺失（绿色块）的区域分布的假设热图。行代表来自七个原发肿瘤区域和六个转移区域的样品。</p>
</blockquote>
<p><strong>研究参考</strong></p>
<blockquote>
<p><strong>Abyzov A., Mariani J., Palejev D., Zhang Y., Haney M. S., et al. (2012) Somatic copy number mosaicism in human skin revealed by induced pluripotent stem cells. Nature 492: 438-442</strong><br>这里作者发现了一个现象：平均而言，iPSC细胞系表现出2个拷贝数变异（CNV），而这些CNV在iPSC来源的成纤维细胞中不明显。他们发现，至少50%的CNV以低频体细胞变异存在于亲本的成纤维细胞中。根据这一观察，他们估计大约30%的成纤维细胞的基因组中携带体细胞CNV，这表明体细胞镶嵌性广泛存在于人体中。</p>
</blockquote>
<h3 id="基因融合"><a href="#基因融合" class="headerlink" title="基因融合"></a>基因融合</h3><p>基因融合是非常普遍的，也是癌症的一个重要特征。现在的研究发现，一个强启动子与一个下游功能基因（比如：原癌基因）的融合在某些癌症中很普遍。据估计，半数的前列腺癌含有TMPRSS2和ETS转录因子家族成员之间的融合。基因融合是由两个原本分开的基因或位点融合形成的。他们可能形成一种基因产物，很多时候表现出来的功能都是全新的，与两个融合的基因个体都不同。这种阴差阳错的情况可能引起致癌机制的激活，就像费城染色体阳性急性淋巴细胞白血病一样。这种基因融合导致BCR-ABL酪氨酸激酶表达，从而激活细胞增殖。有几种机制会导致基因融合的发生，这个现象是一些癌症类型的特点。胰腺癌的特点便是染色体重排的频繁断裂-融合-桥循环。目前有几种方法可以通过测序研究融合事件，如对肿瘤的全基因组测序和mRNA-Seq。</p>
<p>mRNA-Seq与全基因组测序组合的方法对于发现基因融合及其机制特别高效。原因就是mRNA-Seq可以提供直接的证据，来支持观察到的融合是否发生，并同时为融合基因是否表达提供了证据。而全基因组测序可以发现那些mRNA-Seq所发现不了的区域的信息，如基因间区和UTR。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_8.png" alt="cancer_review_8"></p>
<blockquote>
<p>由折回倒位所引起的融合事件可捕获基因组中遥远区域的片段，如着丝粒重复或参与体细胞重排的区域。在这个例子中，6号染色体上的片段被插入到19号染色体上的重复区域之间。注意19号染色体的第二个拷贝是倒置的，这是折回倒位的特点。</p>
</blockquote>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_9.png" alt="cancer_review_9"></p>
<blockquote>
<p><em>MED1</em>（红色）与几个伙伴基因（蓝色）：<em>ACSF2，USP32</em> 和 <em>STXBP4</em>形成基因融合。</p>
</blockquote>
<p><strong>实验上的设计参考</strong></p>
<blockquote>
<p>以Pair-end进行全基因组测序是目前检测基因融合最准确、最全面的工具，这些融合包括重复、倒位、通读和单碱基插入缺失。可以说Pair-end是检测融合基因成功与否的一个关键因素。<br>另外就是高深度测序结合更长的读长可以分辨融合连接中微同源的单碱基。而且这种能力是测序独有的。</p>
</blockquote>
<p><strong>研究参考</strong></p>
<blockquote>
<p><strong>Robinson D. R., Wu Y. M., Kalyana-Sundaram S., Cao X., Lonigro R. J., et al. Identification of recurrent NAB2-STAT6 gene fusions in solitary fibrous tumor by integrative sequencing. Nat Genet 45: 180-185</strong><br>文章主要是利用了全外显子组和转录组测序发现了转录抑制因子NAB2与转录激活因子STAT6的基因融合现象。其中27个独立性纤维性肿瘤（SFT）的转录组测序发现所有肿瘤中存在NAB2-STAT6基因融合。NAB2-STAT6基因融合的过表达诱导了培养细胞的增殖，并激活了EGR应答基因的表达，最后导致了肿瘤。</p>
<p><strong>Seshagiri S., Stawiski E. W., Durinck S., Modrusan Z., Storm E. E., et al. Recurrent R- spondin fusions in colon cancer. Nature 488: 660-664</strong><br>这一篇文章则主要分析了70个原发性人结肠癌的外显子组、转录组和拷贝数变异。拷贝数和RNA-Seq的数据分析确定了在一部分结直肠癌中存在IGF2的扩增和相应过表达。他们还利用RNA-Seq，在10%的结直肠癌中发现了与R-脊椎蛋白家族成员（RSPO2和RSPO3）相关的基因融合。这项研究表明了综合多项技术去了解复杂的癌症基因组很重要。</p>
<p><strong>Thompson-Wicking K., Francis R. W., Stirnweiss A., Ferrari E., Welch M. D., et al. (2012) Novel BRD4-NUT fusion isoforms increase the pathogenic complexity in NUT midline carcinoma. Oncogene</strong><br>这篇文章则提到了PER-624中一种新的BRD4-NUT融合竟然编码了一种功能蛋白，并且它对这些细胞的致癌机制很关键。BRD4-NUT融合转录本是通过易位后的RNA剪接而产生的，这似乎是这些癌症的一个共同特征。这种有助于融合基因的替代异构体表达的机制是第一次报道。</p>
<p><strong>Wen H., Li Y., Malek S. N., Kim Y. C., Xu J., et al. (2012) New fusion transcripts identified in normal karyotype acute myeloid leukemia. PLoS ONE 7: e51203</strong><br>在这项研究中，作者运用双端RNA-Seq来发现染色体核型中的融合，它们经传统的细胞遗传学分析未检测到异常。他们发现了临近基因间的融合转录本以及7个只存在于正常核型中的融合本。</p>
</blockquote>
<h3 id="染色体碎裂"><a href="#染色体碎裂" class="headerlink" title="染色体碎裂"></a>染色体碎裂</h3><p>这是一个不希望发生的现象，染色体碎裂是一个一次性的细胞危机，在单次事件中发生数十次至数百次基因组重排。这种灾难性事件的后果是复杂的局部重排和拷贝数变异，其中染色体上2个（偶尔3个）拷贝的有限范围可被检测。这种单次灾难性事件的模式不同于癌症发展的逐步积累突变的典型模式。在突变积累的癌症发展模式中，拷贝数无上限，因此通常有一个较大的范围。据估计，在所有癌症及其不同亚型之间，染色体碎裂的发生概率约2-3%，而在骨癌中发生概率则大约25%。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_10.png" alt="cancer_review_10"></p>
<blockquote>
<p>染色体碎裂的图示。</p>
</blockquote>
<p><strong>研究参考</strong></p>
<blockquote>
<p><strong>Rausch T., Jones D. T., Zapatka M., Stutz A. M., Zichner T., et al. (2012) Genome Sequencing of Pediatric Medulloblastoma Links Catastrophic DNA Rearrangements with TP53 Mutations. Cell 148: 59-71</strong></p>
<p>文章提到一名Sonic-Hedgehong髓母细胞瘤（SHH-MB）患者的大量、复杂的染色体重排，此患者带有生殖细胞系TP53突变（Li-Fraumeni综合征）。同时将规模扩大到11名Li-Fraumeni综合征患者的筛查，发现有36%的肿瘤表现出与染色体碎裂一致的重排。这比一般肿瘤群体所观察到的2%染色体碎裂发生率要高得多。P53的生殖细胞系突变与一些肿瘤中凋亡中止导致染色体碎裂的假说是一致的。</p>
</blockquote>
<h3 id="拷贝数变异（CNV）"><a href="#拷贝数变异（CNV）" class="headerlink" title="拷贝数变异（CNV）"></a>拷贝数变异（CNV）</h3><p>结构性变异影响基因量——可转录基因的功能拷贝数。肿瘤发展、药物反应及耐药性的发生通常是由基本的基因扩增和删除来驱动的。这些基因组上的改变可分成大的畸变和小的畸变。大的畸变包括整个染色体或部分染色体的丢失或重复，这被称为非整倍体。小的畸变可能只包含一个碱基，比如点突变和小片段的插入缺失。与健康的基因组不同，这些基因表达的改变会受到转录因子的严格调控，癌症基因组则通过基因的重复和删除来适应和逃避这种调控。癌症耐药性的发生正是此反应的速度和效率的绝佳证明。</p>
<h3 id="基因表达"><a href="#基因表达" class="headerlink" title="基因表达"></a>基因表达</h3><p>基因表达分析测定基因转录、RNA加工和表观遗传控制的产物。因此，基因表达分析不仅可以看出这些过程的『健康』程度，也可以深入研究细胞里面的分子机制。基于芯片的mRNA分析曾在癌症的基因变得研究中广泛使用，但基于测序的mRNA分析（mRNA-Seq）的出现代表我们测定和解析基因表达产物能力的又一次飞跃。mRNA-Seq可检测修饰过的RNA和表达水平极低的RNA的能力让它特别适合癌症研究。基于mRNA-Seq的方法也可检测非常快的转录变化、剪接异构体、融合基因以及可变聚腺苷酸化位点。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/table1.png" alt="table"></p>
<blockquote>
<p> Feng H., Qin Z. and Zhang X. (2012) Opportunities and methods for studying alternative splicing in cancer with RNA-Seq. Cancer Lett<br> 这篇综述关注了RNA-Seq在研究癌症相关的可变剪切中的应用。文中包含一个生物信息学工具列表，以及有关估计可变剪切异构体的表达水平的详尽讨论。</p>
</blockquote>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_11.png" alt="cancer_review_11"></p>
<blockquote>
<p>利用RNA-Seq研究癌症中基因表达和选择性剪接的典型生物信息学流程。首先，将短read定位到参考基因组或转录组。在定位之后，估算注释基因和转录本的表达与剪接。</p>
<p>van Delft J., Gaj S., Lienhard M., Albrecht M. W., Kirpiy A., et al. (2012) RNA-Seq provides new insights in the transcriptome responses induced by the carcinogen benzo[a]pyrene. Toxicol Sci 130: 427-439</p>
<p>作者发现，RNA-Seq所检测到的基因比芯片技术多约20%，而表达差异明显的基因更是接近三倍之多。因此，他们检测到的受影响的通路和生物学机制达2-5倍。作者还在许多基因中发现了可变异构体的表达，包括细胞死亡和DNA修复的调控因子，如TP53、BCL2和XPA，它们与基因毒性反应相关。他们还发现了功能未知的新亚型，如已知转录本的片段、带有额外外显子的转录本、内含子保留或外显子跳跃事件。</p>
<p>Kaur H., Mao S., Li Q., Sameni M., Krawetz S. A., et al. (2012) RNA-Seq of human breast ductal carcinoma in situ models reveals aldehyde dehydrogenase isoform 5A1 as a novel potential target. PLoS ONE 7: e50249<br>作者将三个DCIS模型（MCF10.DCIS、SUM102和SUM225）的表达与三维（3D）覆盖培养的非致癌乳腺上皮细胞的MCF10A模型进行了比较，确定了DCIS模型共用的表达变化。他们发现，差异表达的基因编码了与多个信号通路相关的蛋白。</p>
<p>Meyer J. A., Wang J., Hogan L. E., Yang J. J., Dandekar S., et al. (2013) Relapse-specific mutations in NT5C2 in childhood acute lymphoblastic leukemia. Nat Genet 45: 290-294</p>
<p>作者利用RNA测序，报道了诊断和复发相配对的骨髓标本的转录本图谱，这些标本来自十名患有小儿B淋巴细胞白血病的个体。转录组测序鉴定出20个新获得的突变，它们不存在于最初的诊断中，而2名个体带有复发特异的突变。带有NT52C2突变的所有个体都在初步诊断后36个月内复发。</p>
</blockquote>
<p><strong>实验设计上的注意事项</strong></p>
<p>RNA-Seq已成为一种研究肿瘤分子变化的常规应用，大部分研究人员采用生产商的试验流程。rRNA的去除可提高信噪比，实现低表达转录本的检测。</p>
<p>癌症中的体细胞突变基本上是 <em>de novo</em>。测序不需要关于突变的先验知识，即可准确定位突变以及得到转录本丰度。</p>
<p>肿瘤通常包含各种细胞。mRNA-Seq的可延伸检测范围和准确性对检测微小的表达变化非常宝贵。只要肿瘤转录本包含了独特的体细胞突变或剪接变异体，那么就可将它与正常的细胞区分开来。</p>
<p>二代双端对读测序检测基因融合的灵敏度取决于许多因素，包括表达水平、转录本长度、所使用的样品制备方法以及cDNA文库的片段长度。</p>
<p>大部分实验方案采用poly（A）富集的RNA制备方法来测定mRNA水平。然而，非编码RNA，如miRNA，也在细胞的生物学中发挥重要作用，并常常介导对肿瘤生长和存活很关键的过程。非编码RNA可通过现有的poly(A)-(rRNA去除)实验方案轻松分析。</p>
<p>RNA表达是组织和细胞类型特异的。在选择肿瘤-正常对照中的对照时，应考虑这一点。</p>
<h3 id="选择性剪接"><a href="#选择性剪接" class="headerlink" title="选择性剪接"></a>选择性剪接</h3><p>癌症的生物起源、发展、转移与转录组中的许多变异相关联。癌症特异的选择性剪接是个普遍存在的现象，也是个主要的转录后调控机制，涉及到许多癌症类型。</p>
<blockquote>
<p>Seo J. S., Ju Y. S., Lee W. C., Shin J. Y., Lee J. K., et al. (2012) The transcriptional landscape and mutational profile of lung adenocarcinoma. Genome Res 22: 2109-2119<br>作者分析了韩国200个肺腺癌。他们在LMTK2、ARID1A、NOTCH2和SMARCA4中发现了新的驱动突变。他们还发现了45个融合基因，其中8个是嵌合的络氨酸激酶。在17个反复发生的选择性剪接事件中，原癌基因MET中的第14号外显子跳过可能是癌症驱动因素。这项研究表明了这种癌症的复杂性以及运用几种技术的价值。</p>
<p>Liu J., Lee W., Jiang Z., Chen Z., Jhunjhunwala S., et al. (2012) Genome and transcriptome sequencing of lung cancers reveal diverse mutational and splicing events. Genome Res 22: 2315- 2327<br>作者对19个肺癌细胞系和3组肺部肿瘤/正常样本配对开展了全基因组测序和转录组测序。他们鉴定出106个与癌症特异性的异常剪接相关的剪接位点突变，包括一些已知的癌症相关基因中的突变。RAC1b是RAC1 GTP酶的一个异构体，含有一个额外的外显子，被认为在肺癌中优先上调，并对MAP2K（MEK）抑制剂PD-0325901敏感。</p>
<p>Thompson-Wicking K., Francis R. W., Stirnweiss A., Ferrari E., Welch M. D., et al. (2012) Novel BRD4-NUT fusion isoforms increase the pathogenic complexity in NUT midline carcinoma. Oncogene<br>这篇文章发现了PER-624中一种新的BRD4-NUT基因融合编码了一种功能蛋白，它对这些细胞的致癌机制很关键。BRD4-NUT融合转录本是通过易位后的RNA剪接而产生的，这似乎是这些癌症的一个共同特征。这种现象以及促进融合基因的可变异构体表达的机制，过去一直未被发现。</p>
</blockquote>
<h3 id="RNA编辑"><a href="#RNA编辑" class="headerlink" title="RNA编辑"></a>RNA编辑</h3><p>在我们人体中，DNA和RNA序列之间的差异也被称之为 <strong>RNA编辑</strong>，这是一种广泛存在的现象。最频繁的RNA编辑类型是通过腺苷脱氨酶作用于RNA(ADAR)从而实现由腺苷到肌苷的转换。然后紧接着，剪接和翻译机制会将肌识别为鸟苷。在一些肿瘤基因组比正常基因组有着更更比例的RNA-DNA差异。</p>
<blockquote>
<p>Jiang Q., Crews L. A., Barrett C. L., Chun H. J., Court A. C., et al. (2013) ADAR1 promotes malignant progenitor reprogramming in chronic myeloid leukemia. Proc Natl Acad Sci U S A 110: 1041-1046<br>作者发现，慢性髓细胞白血病(CML)急变期的祖细胞有着更高的IFN-r通路基因表达以及BCR-ABL扩增。在CML发展期间，他们还发现IFN应答的ADAR1 p150亚型的表达增强，并且腺苷-肌苷的RNA编辑增加。</p>
</blockquote>
<h3 id="MicroRNA和非编码RNA"><a href="#MicroRNA和非编码RNA" class="headerlink" title="MicroRNA和非编码RNA"></a>MicroRNA和非编码RNA</h3><p>MicroRNA(miRNA)的长度很短，大小集中在17bp-25bp之间，属于非编码RNA（ncRNA）家族的成员。它们调控多种不同的生物学功能，包括发育、细胞增殖、细胞分化、信号转导、凋亡、代谢和细胞寿命。</p>
<p>通过RNA-诱导沉默复合物（RISC）与转录本的3-UTR或者与编码区中的识别位点相互作用。miRNA的一个主要作用是抑制基因的转录后表达。在多种癌症中，许多miRNA位于存在序列缺失删除或扩增的基因组区域，这表明它们很可能在癌症的发展过程中扮演很重要的角色。miRNA的编辑位点已经在近年来的研究中被发现了，表明RNA编辑和miRNA介导的调控之间可能存在着关联。同时由于miRNA的测定简单、相对稳定，并且在大量mRNA的控制上起作用，这就让miRNA成为癌症诊断以及治疗期间的检测和分期过程中极具吸引力的标志物。</p>
<blockquote>
<p>Law P. T., Qin H., Ching A. K., Lai K. P., Co N. N., et al. (2013) Deep sequencing of small RNA transcriptome reveals novel non-coding RNAs in hepatocellular carcinoma. J Hepatol<br>这篇文章描述了一种新的PIWI-互作RNA(piRNA)piR-Hep1，它参与了肝脏肿瘤发展。与周围正常肝脏相比，46.6%的肝癌细胞(HCC)存在piR-Hep1表达上调。piR-Hep1的沉默抑制了细胞活力、运动和侵袭。作者还发现miR-1323在HCC中的大量表达，以及它与肝硬化背景下所产生的肿瘤的独特关联。</p>
</blockquote>
<p><strong>实验设计上的注意事项</strong></p>
<p>测序深度与检测灵敏度是直接相关的。在典型的实验中，如果测序流动槽（Flowcell）的一条通道（lane）只上一个样本，那么测序深度会非常高，这能实现极其灵敏的检测。基于此原因，miRNA的测序深度很少成为考虑因素。在筛查应用中，或不需要如此高深度检测的研究中，样品可加上测序Index标签，从而能够在同一个测序lane中上样多个不同的样本。需要注意的是，在设计miRNA的测序深度时，要时刻记住miRNA控制基因表达，因而miRNA水平的小变化可能影响许多编码蛋白。</p>
<p>新发现的miRNA应当通过功能分析(如Ago2结合或敲除实验)来确认。</p>
<p>实验应当包含足够的样品，以便结论具有真正的统计意义和高的可信度。一般来说，建立一个可能的假设相对来说是比较容易的，只要能证明miRNA存在于患者的肿瘤中，即便样本数量不多也是足够的。但是要真正验证假设就没那么容易了，通常需要大量的患者来检验，并建立统计学可信度。目前，关于如何在测序研究中建立统计学可信度和多重检验纠正，还没有特别公认的方法。当前基于测序的miRNA分析并没有实现miRNA表达的绝对定量，而仅是不同miRNA(如肿瘤-正常配对)的相对量。</p>
<p>样品分层是癌症样品的一个问题。一个特定的癌症表型可能代表几种不同的病因和机理。为了要进行严格的分析，应当有足够多的样品量，从而能够充分代表每个肿瘤亚型。而miRNA的表达会随着肿瘤的发展而变化，因此在实验设计时应建立肿瘤分期和分级。</p>
<h3 id="RNA-蛋白结合（CLIP-Seq）"><a href="#RNA-蛋白结合（CLIP-Seq）" class="headerlink" title="RNA-蛋白结合（CLIP-Seq）"></a>RNA-蛋白结合（CLIP-Seq）</h3><p>在人类细胞中，大多数mRNA(或前体mRNA)与核不均一性核糖蛋白(hnRNP)相结合，形成大的hnRNP-RNA复合物。hnRNA蛋白在RNA加工的所有关键环节中都发挥重要作用，包括前体mRNA剪接以及mRNA出核、定位、翻译和稳定性。几十种RNA结合蛋白(RBP)和基因的hnRNP蛋白与癌症相关联。</p>
<p>RNA-蛋白的相互作用可通过交联免疫沉淀测序(CLIP-Seq)来测定。在CLIP-Seq中，细胞经过紫外线处理，让RBP与RNA复合物共价交联。细胞随后被裂解，RBP-RNA复合物被免疫共沉淀，从而测序相应的RNA。</p>
<blockquote>
<p>Wilbert M. L., Huelga S. C., Kapeli K., Stark T. J., Liang T. Y., et al. LIN28 binds messenger RNAs at GGAGA motifs and regulates splicing factor abundance. Mol Cell 48: 195-206<br>LIN28是个保守的RNA结合蛋白，它与多能性、重编程和肿瘤的形成相关。在各种不同的癌细胞和原发肿瘤组织中都发现LIN28的异常上调。在这篇文章中，作者利用CLIP-Seq鉴定了大约四分之一分布于人类转录本中LIN28的结合位点。从这些结合位点中他们发现，LIN28与mRNA中富含环结构的GGAGA序列结合。同时还发现了，LIN28的表达能够导致可变剪切中的下游变化。</p>
</blockquote>
<h3 id="表观遗传和甲基化"><a href="#表观遗传和甲基化" class="headerlink" title="表观遗传和甲基化"></a>表观遗传和甲基化</h3><p>癌症发展过程中的表观遗传改变与异常的基因表达相关联。近期的研究证据表明，表观遗传上的改变可能在癌症 <strong>起始中</strong> 发挥作用。表观遗传的控制是通过多个不同过程进行介导的，包括DNA修饰（甲基化或乙酰化）、组蛋白修饰和核小体重塑。发现控制表观基因组的基因发生突变，在人类癌症中是一个相当普遍的现象。NGS测序技术提供了一整套工具，可定位突变并测定它们对癌症发展的影响。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_1.png" alt="cancer_review_1"></p>
<blockquote>
<p>癌症中表观遗传修饰物的基因突变。在不同类型的癌症中经常观察到三类表观遗传修饰物发生突变，这突出了遗传学和表观遗传学之间的串扰。表观遗传修饰物的突变有可能引起癌症的全基因组表观遗传改变。了解遗传和表观遗传变化的关系将为癌症治疗提供新的见解。</p>
</blockquote>
<h3 id="DNA修饰"><a href="#DNA修饰" class="headerlink" title="DNA修饰"></a>DNA修饰</h3><p>DNA修饰目前可以很容易地通过多种技术来进行测定。不同技术的选择取决于所需的通量和分辨率。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_table2.png" alt="table2"></p>
<blockquote>
<p>Bert S. A., Robinson M. D., Strbenac D., Statham A. L., Song J. Z., et al. Regional activation of the cancer genome by long-range epigenetic remodeling. Cancer Cell 23: 9-22<br>文章作者通过协同的长距离表观遗传激活（LREA）技术，鉴定出一种结构域基因去调控的机制。这些区域通常会跨越1Mb的基因组长度，包括关键癌基因、microRNA和癌症的生物标志物基因。LREA结构域中基因启动子的特点是活性染色体标记的的获得和抑制标记的丢失。</p>
<p>Brastianos P. K., Horowitz P. M., Santagata S., Jones R. T., McKenna A., et al. Genomic sequencing of meningiomas identifies oncogenic SMO and AKT1 mutations. Nat Genet 45: 285-289<br>为了鉴定和验证脑膜瘤中的体细胞遗传改变，作者对17个脑膜瘤进行了全基因组或外显子组测序，并对另外48个肿瘤进行了靶向测序。他们所观察到的突变谱是分布广泛，但他们证实了43%的肿瘤中存在病灶NF2失活，并在另外8%的肿瘤中发现表观遗传修饰物的改变。</p>
<p>Duncan C. G., Barwick B. G., Jin G., Rago C., Kapoor-Vazirani P., et al. A heterozygous IDH1R132H/WT mutation induces genome-wide alterations in DNA methylation. Genome Res 22: 2339-2355<br>脑胶质瘤、急性骨髓性白血病和软骨瘤中经常发生NADP+依赖的异柠檬酸脱氢酶IDH1和IDH2的单等位点基因点突变。作者表明，IDH1R132H等位基因的杂合表达足以诱导这些肿瘤特有的以DNA甲基化为特征的全基因组改变。这说明了IDH1R132H/WT突变体是推动人类癌细胞的表观遗传不稳定性的原因。</p>
<p>Zhang J., Benavente C. A., McEvoy J., Flores-Otero J., Ding L., et al. A novel retinoblastoma therapy from genomic and epigenetic analyses. Nature 481: 329-334<br>视网膜母细胞瘤是一种发生于视网膜的侵袭性儿童癌症，由RB1失活所引发，但潜在机理仍然未知。在此类高度侵袭性的癌症中，许多基因都参与其中，但RB1是唯一的已知发生突变的癌基因。与有限的体细胞突变不同，相对正常的成视网膜细胞，肿瘤的甲基化图谱表现出巨大的改变。最惊人的结果之一是人视网膜母细胞瘤中原癌基因脾络氨酸激酶（SYK）的诱导表达。SYK是肿瘤细胞生存所必需的。研究人员接着表明，小分子抑制剂对SYK的抑制导致培养和体内的视网膜母细胞瘤的细胞死亡。</p>
</blockquote>
<p><strong>实验设计上的注意事项</strong></p>
<p>每种组织和细胞类型都有着独特的甲基化模式；因此必须获得感兴趣的组织以便分析。癌症研究中，肿瘤组织-癌旁正常组织的配对可简化分析。</p>
<p>通过重亚硝酸氢盐测序所产生的超大量CpG标志物很难解释，且可靠的统计学分析目前仍然困难重重。不过，下面一些实际的方法可简化分析：</p>
<ul>
<li>RRBS-Seq通过限制覆盖度而简化分析；</li>
<li>综合分析大大改善了结果的可解释性。例如，将表达分析与甲基化分析相结合，让我们可专注于表达水平改变的基因；</li>
<li>将分析限制在某个感兴趣的基因或区域中。这种方法对GWAS的后续研究很有效，也适合已有实验证据说明目的区域存在基因调控或染色质重塑的研究。与降低代表性的方法不同。这种方法实现了更多区域的分析，因此可获得更多信息。</li>
</ul>
<p>组织培养物应当谨慎使用。随着时间的推移和组织增殖，培养物的甲基化水平可能已经改变，不大能代表原先的组织样本。</p>
<h3 id="组蛋白修饰"><a href="#组蛋白修饰" class="headerlink" title="组蛋白修饰"></a>组蛋白修饰</h3><p>组蛋白修饰通常指的是甲基化和乙酰化。组蛋白H3K9、H3K27和H4K20的甲基化与基因转录的抑制相关，而H3K4和H3K36的三甲基化与活性转录的染色质相关。组蛋白乙酰化几乎总是与染色质可接近性和转录活性水平的增高相关。通过操控染色质状态和DNA可接近性，表观遗传修饰在各个发育阶段、组织类型和疾病中都对基因表达的控制起着关键作用。</p>
<blockquote>
<p>Wilkinson A. C., Ballabio E., Geng H., North P., Tapia M., et al. (2013) RUNX1 is a key target in t(4;11) leukemias that contributes to gene activation through an AF4-MLL complex interaction. Cell Rep 3: 116-127<br>这篇文章报道了一种转化机制，其中两个致癌融合蛋白合作激活目的基因，然后调节其下游产物的功能。</p>
</blockquote>
<p><strong>实验设计上的注意事项</strong></p>
<p>每种组织和细胞类型都有着独特的甲基化模式；因此必须获得目的组织以便分析。</p>
<p>组蛋白修饰可以通过各种ChIP-Seq方法进行检测，原理是通过抗体与目标甲基化组蛋白进行特异结合。</p>
<p>同样，组织培养物应当谨慎使用。随着时间的推移和组织增殖，培养物的甲基化水平可能已经改变，不大能代表原先的组织样本。</p>
<h3 id="染色质结构与重排"><a href="#染色质结构与重排" class="headerlink" title="染色质结构与重排"></a>染色质结构与重排</h3><p>染色体重排需要DNA双链断裂的形成和连接。这些事件的发生，会破坏基因组的完整性，并经常在白血病、淋巴瘤和肉瘤中观察到。并且特定基因间的反复的基因融合在不同的个体中均观察到，这表明这些基因一定在细胞周期中的某个阶段他们之间的物理位置非常接近。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_12.png" alt="cancer_review_12"></p>
<blockquote>
<p>这是一个设想的三维、具有转录活性的复合物，它包含了致密的环化位置。这个示意图是基于检测到的成环事件，并假设所有成环事件都能发生在单个细胞内。在这个模型中，所有小环汇集到一个共同的核心（蓝色球）。环降低了转录活性复合物的物理大小，从而推动转录因子接近待定的基因组位点。</p>
</blockquote>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_13.png" alt="cancer_review_13"></p>
<blockquote>
<p>检测染色质相互作用。在三维空间中，相同或不同染色体上远端的基因组区域相互作用，而这种相互作用是由一个或多个DNA结合蛋白介导的。a) <em>ChIP-Seq</em> 利用染色质免疫共沉淀来鉴定DNA和蛋白的相互作用。人们采用各种DNA-片段化方法和核酸外切酶，来缩小片段的大小分布。b)染色质构像捕获实验利用一个连接步骤将互作的染色质片段相连接。这种方法可鉴定与遥远序列相结合的蛋白。c)配对末端标签测序分析染色质相互作用（ChIA-PET）同样也利用连接步骤来检测染色质相互作用，将不相邻的互作区域配对。然而，ChIA-PET利用染色质免疫共沉淀（ChIP）步骤，只能鉴定特定蛋白的相互作用，如RNA聚合酶II。</p>
</blockquote>
<p><strong>参考文献</strong></p>
<blockquote>
<p>Papantonis A., Kohro T., Baboo S., Larkin J. D., Deng B., et al. TNFalpha signals through specialized factories where responsive coding and miRNA genes are transcribed. EMBO J 31: 4404- 4414<br>作者利用测序以及染色体构像捕获（3C）和ChIA-PET表明，TNF_alpha诱导响应基因聚集在分散的『NF-B工厂』。一些『工厂』还专门转录编码miRNA的响应基因，这些miRNA靶定下调的mRNA。</p>
<p>Rocha P. P., Micsinai M., Kim J. R., Hewitt S. L., Souza P. P., et al. Close proximity to Igh is a contributing factor to AID-mediated translocations. Mol Cell 47: 873-885<br>细胞核组织可决定『脱靶』活性以及融合伴侣的选择。这项研究表明，绝大多数已知的活化诱导胞嘧啶核苷脱氨酶（AID）介导的Igh转位伴侣在类型转换过程中与此位点接触的染色体结构中被发现。此外，这些相互作用结构域可用来鉴定被AID靶定的其他基因。</p>
<p>Theodoratou E., Montazeri Z., Hawken S., Allum G. C., Gong J., et al. Systematic meta- analyses and field synopsis of genetic association studies in colorectal cancer. J Natl Cancer Inst 104: 1433-1457<br>这篇研究文章表明，T细胞特异的转录因子GATA3在介导增强子接近调控区域上扮演了重要角色，这些调控区域参与了雌激素受体（ESR1）介导的转录。GATA3沉默导致在雌激素刺激之前辅助因子和活性组蛋白标记的整体重新分配。</p>
<p>Hakim O., Resch W., Yamane A., Klein I., Kieffer-Kwon K. R., et al. (2012) DNA damage defines sites of recurrent chromosomal translocations in B lymphocytes. Nature 484: 69-74<br>作者发现，在培养的小鼠B淋巴细胞中，缺乏经常性的DNA损伤时，Igh或Myc与其他所有基因之间的易位与它们的接触频率直接相关。反过来，与经常性位点指向的DNA损伤相关的易位与DNA断裂形成的速率成正比。他们认为，非定向重排反映了细胞核结构，而DNA断裂形成决定了包括驱动B细胞恶性肿瘤在内的经常性易位的位置和频率。</p>
</blockquote>
<h3 id="综合分析（多组学分析）"><a href="#综合分析（多组学分析）" class="headerlink" title="综合分析（多组学分析）"></a>综合分析（多组学分析）</h3><p>所有的生物过程都是相互关联的，而在癌细胞发生过程的任何一个变化都会影响其他所有过程。突变可能影响所表达的活性，继而又影响DNA甲基化，再就影响其他许多基因的表达等等一连串的反应。每个个体都有着大量的特有突变，再加上这一连串的事件，能够让人们深入研究各种用于区分癌症的疾病表型。综合分析可以使揭示癌症生物学的真正复杂性向前迈进了一步。研究人员如今能够检测大部分的单个过程，但认识和治疗癌症的真正进步将来自于对所有这些过程的综合分析，也就是常说的多组学分析。</p>
<p><strong>参考文献</strong></p>
<blockquote>
<p>Weischenfeldt J., Simon R., Feuerbach L., Schlangen K., Weichenhan D., et al. (2013) Integrative genomic analyses reveal an androgen-driven somatic alteration landscape in early-onset prostate cancer. Cancer Cell 23: 159-170<br>作者发现，早发性前列腺癌的形成涉及到雄激素驱动的结构重排。相比之下，老年发病的前列腺癌积累了非雄激素相关的结构重排，表明一种不同的肿瘤形成机制。</p>
<p>Cowper-Sal lari R., Zhang X., Wright J. B., Bailey S. D., Cole M. D., et al. (2012) Breast cancer risk- associated SNPs modulate the affinity of chromatin for FOXA1 and alter gene expression. Nat Genet 44: 1191-1198<br>作者表明，与乳腺癌风险相关的SNP集中在FOXA1和ESR1的顺反组（cistrome），以及组蛋白H3赖氨酸4单甲基化（H3K4me1）的表观基因组。大多数的风险相关SNP调控FOXA1在远端调控元件的染色质亲和力，这导致等位基因特异的基因表达。</p>
<p>Peifer M., Fernandez-Cuesta L., Sos M. L., George J., Seidel D., et al. (2012) Integrative genome analyses identify key somatic driver mutations of small-cell lung cancer. Nat Genet 44: 1104-1110<br>作者发现了TP53和RB1失活的证据，并在编码组蛋白修饰物的基因中发现了反复的突变。此外，他们还在PTEN、SLIT2和EPHA7中观察到突变，以及FGFR1络氨酸激酶基因的病灶扩增。这种综合分析表明，组蛋白修饰可能与小细胞肺癌（SCLC）有关。</p>
</blockquote>
<h3 id="技术参考"><a href="#技术参考" class="headerlink" title="技术参考"></a>技术参考</h3><p>一个良好的实验设计可以提高技术性能，从而产生最易解释和可靠的结果。这里重点强调研究人员在设计实验时必须牢记的生物学和技术的特性。</p>
<p>癌症中的实验设计面临一些独特的挑战。典型的肿瘤样本包含两个基因组：遗传自父母的生殖细胞系（germline）和在疾病发展过程中积累的体细胞突变（somatic mutations）。肿瘤细胞在样本中的比例可能在10%-100%之间。肿瘤基因组也是动态的，会快速积累 <em>de novo</em> 突变。因此，每一个肿瘤中又可能同时包含几个克隆亚型。</p>
<p>目前大部分已发表研究的样本量都非常小，可被视为仅是提出了相关的假说。而随着越来越多的测序信息被我们所获得，大部分癌症类型可根据其分子表型，被分成多个亚群。这严重降低了实验的能力，并增加了严格分析所需的样本数量。部分解决这一难题的方案是在探索阶段利用全基因组测序来寻找新的突变。在第二阶段，利用全外显子组或靶向测序来确认新发现的这些突变，并确定他们在大型队列中的丰度。然而，在未来，统计学上严谨的全基因组测序实验有可能是非常大的，需要数千个样本。</p>
<p>使用NGS测序技术进行深度测序是指多次生成定位到同一区域的序列片段，有时达上百次。但由于每条序列片段是从单个DNA分子中产生的，故提高深度测序能够实现原始样品中低至1%的克隆的检测。比较同一个体的肿瘤和癌旁正常组织的序列，我们可以很容易识别浸润组织的序列片段。最佳的读取深度将取决于癌症类型和所需的灵敏度，但一般建议为正常基因组最低为40倍的覆盖深度，而癌症基因组需要80倍以上的覆盖深度。在肿瘤高度异质时，可能需要肿瘤不同部位的多次活检，才能包含所有的细胞类型。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_14.png" alt="cancer_review_14"></p>
<blockquote>
<p>在这个假定的例子中，肿瘤含有两个癌症克隆和邻近组织。肿瘤样本中正常细胞所产生的序列（图中：比对中的前两个序列）可通过与邻近正常组织所产生的序列比较后确定。肿瘤样本中的剩余序列可分为两组，分别代表主要和次要的肿瘤克隆。次要克隆，若不及时治疗，可能在复发时成为肿瘤的主要组分。在实际分析中，肿瘤样本至少要有40倍的覆盖深度，并覆盖靶定的基因集合、全外显子组或全基因组。</p>
</blockquote>
<p>检测癌症基因组中的体细胞突变通常有三种方法：全基因组测序、全外显子组测序和靶向基因测序。下表简要介绍了每种方法的有点和缺点。在比较多发性骨髓瘤的全基因组和外显子组测序时，半数的蛋白编码突变通过染色体畸变（如易位）而存在，其中大部分不能单独被外显子组测序而发现。靶向重测序是一种有用的技术，可收录超大队列中已知癌症相关基因的突变。从长远来看，随着我们对基因组的了解日益加深，并且我们处理和解释大型数据集的能力的提高，全基因组测序无疑是最好的肿瘤分子鉴定方法。而在短期内，靶向基因测序可为患者匹配出市场上已有的药物，让他们立刻受益。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/cancer_review_table3.png" alt="table3"></p>
<p><em>参考来源</em></p>
<blockquote>
<p>Illumina cancer research</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> 癌症基因组 </category>
            
            <category> 基因组学 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 癌症 </tag>
            
            <tag> 综述 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何使用Shapeit2对人类基因组数据进行phasing]]></title>
      <url>/2016/03/15/2016-03-15-how-to-phase-the-human-genome-by-shapeit2.html</url>
      <content type="html"><![CDATA[<p><img src="http://7u2had.com1.z0.glb.clouddn.com/post.Phasing-Data.png" alt="phasing"></p>
<p><a href="https://mathgen.stats.ox.ac.uk/genetics_software/shapeit/shapeit.html" target="_blank" rel="external">SHAPEIT(2.0)</a>是专门用于对推断基因组单体型的软件，有牛津大学的团队所开发，并且一直应用与千人基因组计划中。</p>
<p>以下，我将记录如何通过shapeit2对人群的变异数据集（VCF 格式）进行phasing，并构造出reference panel的过程。</p>
<p>首先，准备文件。整个过程只需要变异数据集（VCF 格式），样本信息文件(sample.fam)，genetic_map文件和参考序列（fasta格式）。对于genetic_map文件需要单独做些说明，这个记录的是基因组中各个位点的重组率和物理距离之间的关系，这是phasing过程非常重要的一个文件。它来自于<a href="http://hapmap.ncbi.nlm.nih.gov/" target="_blank" rel="external">人类单体型计划-Hapmap计划</a>，可以<a href="http://hapmap.ncbi.nlm.nih.gov/downloads/recombination/" target="_blank" rel="external">下载</a>,<br><a href="http://hapmap.ncbi.nlm.nih.gov/downloads/recombination/2011-01_phaseII_B37/genetic_map_HapMapII_GRCh37.tar.gz" target="_blank" rel="external">最新版是b37或者说hg19</a>，如果你的reference版本高于hg19，则需要liftover，liftover之后那些顺序发生交叉的位点，是liftover的错误导致的，要去掉。但是要注意的是，genetic-map中两个位点之间的重组率（recombination rate）是不变的，这其实也很好理解，reference之所以需要升级，是因为它的组装结果并非是百分百符合真实情形的，随着技术的进步，我们会不断去升级逼近这个真实情况，但重组率是根据群体的重组情况来计算的，它是由真实情况反映出来的，因此即便reference版本改变了，它的值也不需要改变。不过对于两个位点之间的物理距离（physical distance）就不同了，leftover之后，这个距离是会发生变化的，通过和原点（一般是重组率为0的点或者就是各个染色体的第一个位点）的距离比例来调节。</p>
<p>至于样本信息文件，格式如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">1009	1009-01	0	0	1	34</div><div class="line">1009	1009-02	0	0	2	33</div><div class="line">1009	1009-06	1009-01	1009-02	2	67</div><div class="line">1030	1030-01	0	0	1	43</div><div class="line">1030	1030-02	0	0	2	44</div><div class="line">1030	1030-06	1030-01	1030-02	1	71</div></pre></td></tr></table></figure>
<p>其他的两份文件就不必多说了。</p>
<p>准备好以上文件之后接下来就是主要的步骤了：</p>
<h3 id="第一步，将vcf转化为bed-bim-fam"><a href="#第一步，将vcf转化为bed-bim-fam" class="headerlink" title="第一步，将vcf转化为bed/bim/fam"></a>第一步，将vcf转化为bed/bim/fam</h3><p>bed/bim/fam这三个文件是phasing的常用谱系文件格式。做这一步转换的工具有很多，我们这里直接借助GATK的<a href="https://www.broadinstitute.org/gatk/guide/tooldocs/org_broadinstitute_gatk_tools_walkers_variantutils_VariantsToBinaryPed.php" target="_blank" rel="external">VariantsToBinaryPed</a>模块进行转换：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">time java -Xmx8g -jar GenomeAnalysisTK.jar -T VariantsToBinaryPed \</div><div class="line">    -R hg20.fa \</div><div class="line">    -V chr22.vcf \</div><div class="line">    --metaData sample.fam \</div><div class="line">    -mgq 0 \</div><div class="line">    -bed chr22.bed \</div><div class="line">    -bim chr22.bim \</div><div class="line">    -fam chr22.fam \</div><div class="line">    -<span class="built_in">log</span> gatk.log &amp;&amp; <span class="built_in">echo</span> <span class="string">"** done **"</span> &amp;&amp; sed <span class="string">'s/^chr//g'</span> chr22.bim &gt; t.bim &amp;&amp; mv -f t.bim chr22.bim</div></pre></td></tr></table></figure>
<p>这个执行命令的最后多加了一小步：将原来输出的.bim文件中第一列的chr22换成了22。之所以要费这个小周折，是因为如果不做这个小操作，接下来的plink步骤中，会直接报<code>ERROR: Problem reading BIM file, line 1</code>退出，原因就是它不允许chr的开头，至于具体的原因我也没去细查。</p>
<h3 id="第二步，过滤genotype高missing-rate和孟德尔错误的位点"><a href="#第二步，过滤genotype高missing-rate和孟德尔错误的位点" class="headerlink" title="第二步，过滤genotype高missing rate和孟德尔错误的位点"></a>第二步，过滤genotype高missing rate和孟德尔错误的位点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">plink=/com/extra/testing/bin/plink</div><div class="line">time <span class="variable">$plink</span> --noweb --bfile chr22 --keep-allele-order --me 1 1 --<span class="built_in">set</span>-me-missing --make-bed --out chr22.nomendel &amp;&amp; <span class="built_in">echo</span> <span class="string">"** nomendel done **"</span> &amp;&amp; time <span class="variable">$plink</span> --noweb --bfile chr22.nomendel --keep-allele-order --geno 0.05 --make-bed --out chr22.nomendel.filter &amp;&amp; <span class="built_in">echo</span> <span class="string">"** fileter done **"</span></div></pre></td></tr></table></figure>
<h3 id="第三步，phasing"><a href="#第三步，phasing" class="headerlink" title="第三步，phasing"></a>第三步，phasing</h3><p>这是phasing的最后一步了，这里分成两小步，phasing和输出格式转换：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># phasing</span></div><div class="line">time shapeit2 \</div><div class="line">    --duohmm \</div><div class="line">    -W 5 \</div><div class="line">    --input-bed chr22.nomendel.filter.bed chr22.nomendel.filter.bim chr22.nomendel.filter.fam \</div><div class="line">    --input-map genetic_map.chr22.txt \</div><div class="line">    -O hapData \</div><div class="line">    --thread 1 &amp;&amp; <span class="built_in">echo</span> <span class="string">"** panel done **"</span></div><div class="line"></div><div class="line"><span class="comment"># 格式转换</span></div><div class="line">time shapeit2 -convert \</div><div class="line">    --input-haps hapData \</div><div class="line">    --output-vcf chr22.haps.vcf \</div><div class="line">    --output-ref chr22.phased.hap chr22.phased.leg chr22.phased.sam &amp;&amp; <span class="built_in">echo</span> <span class="string">"** all done **"</span></div></pre></td></tr></table></figure></p>
<p>以上输出结果中，<code>chr22.haps.vcf</code>便是进行phasing之后的结果，而<code>chr22.phased.hap</code>和<code>chr22.phased.leg</code>这两个文件是从<code>chr22.haps.vcf</code>中得到的，它们便是Imputation分析中的reference panel。</p>
]]></content>
      
        <categories>
            
            <category> phasing </category>
            
        </categories>
        
        
        <tags>
            
            <tag> shapeit2 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何使用Python绘制GWAS分析中的曼哈顿图和QQ图]]></title>
      <url>/2016/02/02/2016-02-02-How-to-create-manhattan-and-qq-plot-for-GWAS-study-in-Python.html</url>
      <content type="html"><![CDATA[<p><img src="http://7lrw1m.com1.z0.glb.clouddn.com/fg.post.2015-02-08.cover.jpg" alt="GWAS-ARRAY"></p>
<p>【前言】这篇文章使用<a href="https://github.com/ShujiaHuang/geneview" target="_blank" rel="external">geneview</a>完成这两类图的作法，它是一个Python高级库，建立在matplotlib的基础之上，专门用于基因组数据的可视化，目的是为了使创建<strong>高大上（精致）</strong>的基因组数据图表变得简单。目前该发布的Python包中已经内置多个优美的调色板和风格（默认情况下就能创建赏心悦目的图形），同时已经集成了曼哈顿图和Q-Q图的绘制函数。作为该Python包的主要开发者，只是如此是远远不够的，在未来的日子里，我希望它能在功能不断完善的同时也变得更加易用。</p>
<p>曼哈顿图和QQ图是两个在全基因组关联（GWAS）分析里面最常出现的图形，基本上已经是GWAS的标配，几乎在每篇GWAS的文章都会见到，它们的作用和所要传达出来的信息我也在<a href="">上一篇关于GWAS的博文</a>中做了些说明，在这里我们就只集中在如何用Python和geneview将其有效地展现出来。</p>
<p>首先，准备一些数据来作为例子。</p>
<p>我这里用来展现的数据是2011年丹麦人所做过的一个关于年轻人过度肥胖的GWAS研究——<a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0024303" target="_blank" rel="external">GOYA</a>，数据也是从他们所发表的结果中获得，总共有5,373个样本，其中超重的个体（case）有2,633个，正常的个体（control）是2,740个，从样本量上看还算可以。为了方便使用，我对其做了相关的处理，包括从PED和MAP文件到GEN文件的生成，并重复了一次case-control的关联性分析，计算出了芯片上所研究的各个SNP位点与肥胖相关的显著性程度（即p-value），最后又将结果数据抽取出来做成数据集——放在<a href="https://github.com/ShujiaHuang/geneview-data/blob/master/GOYA.csv" target="_blank" rel="external">这里供下载（15.6Mb，csv格式）</a>。</p>
<blockquote>
<p>【注】以上内容虽提及到了一些领域内术语和相关文件格式，但若不懂也请不必纠结，因为后续处理都是基于这个最终的数据集来完成的。</p>
</blockquote>
<p>接着，需要将geneview软件包加入到你的Python中，有多种不同的方式，但推荐直接使用pip，以下是安装比较稳定的发布版，直接在终端命令行下(Linux or Mac)输入：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install geneview</div></pre></td></tr></table></figure>
<p>或者，也可以直接从github上安装正在开发的版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install git+git://github.com/ShujiaHuang/geneview.git<span class="comment">#egg=geneview</span></div></pre></td></tr></table></figure>
<p>第三种办法就是直接下载源码，然后自行编译，虽然不推荐这种做法（因为还有依赖包必须自行下载安装，过程会比较麻烦低效），但对于某些不能连接外网的集群也只能如此，这三种方式都是可行的。</p>
<h2 id="曼哈顿图"><a href="#曼哈顿图" class="headerlink" title="曼哈顿图"></a>曼哈顿图</h2><p>将示例数据下载下来：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">wget https://raw.githubusercontent.com/ShujiaHuang/geneview-data/master/GOYA.csv ./</div></pre></td></tr></table></figure>
<p>先简单地查看一下数据的格式:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">chrID,rsID,position,pvalue</div><div class="line">1,rs3094315,742429,0.144586</div><div class="line">1,rs3115860,743268,0.230022</div><div class="line">1,rs12562034,758311,0.644366</div><div class="line">1,rs12124819,766409,0.146269</div><div class="line">1,rs4475691,836671,0.458197</div><div class="line">1,rs28705211,890368,0.362731</div><div class="line">1,rs13303118,908247,0.22912</div><div class="line">1,rs9777703,918699,0.37948</div><div class="line">1,rs3121567,933331,0.440824</div></pre></td></tr></table></figure>
<p>一共是4列（逗号分隔），分别为：[1]染色体编号，[2]SNP rs 编号，[3] 位点在染色体上的位置，[4]显著性差异程度（pvalue）。在本例曼哈顿图中我们只需要使用第1,3和4列；而QQ图则只需要第4列——pvalue。</p>
<p>下面我们先从绘制曼哈顿图开始。我们先将需要的数据读取到一个列表中，可以这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> csv</div><div class="line"></div><div class="line">data = []</div><div class="line"><span class="keyword">with</span> open(<span class="string">"GOYA.csv"</span>) <span class="keyword">as</span> f:</div><div class="line">	f_csv = csv.reader(f)</div><div class="line">	headers = next(f_csv)</div><div class="line">	data = [[row[<span class="number">0</span>], int(row[<span class="number">2</span>]), float(row[<span class="number">3</span>])] <span class="keyword">for</span> row <span class="keyword">in</span> f_csv]</div></pre></td></tr></table></figure>
<p>现在GOYA.csv中的数据就都存放在data列表中了，由于Python在读取文件中数据时，都是以string类型存放，因此对于第3和第4列的数据有必要事先把做点类型转换。</p>
<p>接下来，调用geneview中的曼哈顿图函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="keyword">from</span> geneview.gwas <span class="keyword">import</span> manhattanplot</div><div class="line"></div><div class="line">ax = manhattanplot(data, xlabel=<span class="string">"Chromosome"</span>, ylabel=<span class="string">"-Log10(P-value)"</span>)  <span class="comment"># 这就是Manhattan plot的函数</span></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="http://7u2had.com1.z0.glb.clouddn.com/post.man-0.png" alt="manhantan1"></p>
<p>只需这样的一句代码就能创建一个漂亮的曼哈顿图，有必要再次指出的是，geneview是以matplotlib为基础开发出来的，所创建的图形对象实际上仍属于matplotlib，geneview内部自定义了很多图形风格，同时封装了大量只属于基因组数据的图表类型，但图形的输出格式以及界面显示都仍和matplotlib一样，因此在这里我们使用matplotlib.pyplot的show()函数(上例中：plt.show())将所绘制出来的曼哈顿图显示出来。如果要将图形保存下来，则只需执行<code>plt.savefig(&quot;man.png&quot;)</code>，这样就会在该目录下生成一个名为『man.png』png格式的曼哈顿图，若是要存为pdf格式，则只需将所要保存的文件名后缀改成『.pdf』（plt.savefig(“man.pdf”)）就可以了。下面这些格式：emf, eps, pdf, png, jpg, ps, raw, rgba, svg, svgz等都是支持的，至于最新的还有多少种，还请参照matplotlib文档中说明。</p>
<p>此外，geneview中的每个画图函数都有着足够的灵活性，我们也可以根据自己的需要做一些调整，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">xtick = [<span class="string">'1'</span>, <span class="string">'2'</span>,<span class="string">'3'</span>,<span class="string">'4'</span>,<span class="string">'5'</span>,<span class="string">'6'</span>,<span class="string">'7'</span>,<span class="string">'8'</span>,<span class="string">'9'</span>,<span class="string">'10'</span>,<span class="string">'11'</span>,<span class="string">'12'</span>,<span class="string">'13'</span>,<span class="string">'14'</span>,<span class="string">'16'</span>,<span class="string">'18'</span>, <span class="string">'20'</span>,<span class="string">'22'</span>]</div><div class="line">manhattanplot(data,</div><div class="line">              xlabel=<span class="string">"Chromosome"</span>, <span class="comment"># 设置x轴名字</span></div><div class="line">              ylabel=<span class="string">"-Log10(P-value)"</span>, <span class="comment"># 设置y轴名字</span></div><div class="line">              xtick_label_set = set(xtick), <span class="comment"># 限定横坐标轴上的刻度显示</span></div><div class="line">              s=<span class="number">40</span>, <span class="comment"># 设置图中散点的大小</span></div><div class="line">              alpha=<span class="number">0.5</span>, <span class="comment"># 调整散点透明度</span></div><div class="line">              color=<span class="string">"#f28b1e,#9a0dea,#ea0dcc,#63b8ff"</span>, <span class="comment"># 设置新的颜色组合</span></div><div class="line">              )</div></pre></td></tr></table></figure>
<p><img src="http://7u2had.com1.z0.glb.clouddn.com/post.man-1.png" alt="manhantan2"></p>
<p>实现新的颜色组合、限定x轴上的刻度显示和散点大小的调节。甚至还可以将散点改为线：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">manhattanplot(data，</div><div class="line">              xlabel=<span class="string">"Chromosome"</span>, <span class="comment"># 设置x轴名字</span></div><div class="line">              ylabel=<span class="string">"-Log10(P-value)"</span>, <span class="comment"># 设置y轴名字</span></div><div class="line">              xtick_label_set = set(xtick), <span class="comment"># 限定横坐标轴上的刻度显示</span></div><div class="line">              alpha=<span class="number">0.5</span>, <span class="comment"># 调整散点透明度</span></div><div class="line">              color=<span class="string">"#f28b1e,#9a0dea,#ea0dcc,#63b8ff"</span>, <span class="comment"># 设置新的颜色组合</span></div><div class="line">              kind=<span class="string">"line"</span></div><div class="line">              )</div></pre></td></tr></table></figure>
<p><img src="http://7u2had.com1.z0.glb.clouddn.com/post.man-2.png" alt="manhantan3"></p>
<p>其它方面的调整请查看geneview文档中的相关说明。</p>
<h2 id="Q-Q图"><a href="#Q-Q图" class="headerlink" title="Q-Q图"></a>Q-Q图</h2><p>qq图只需用到上例中的pvalue那一列：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> csv</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> geneview.gwas <span class="keyword">import</span> qqplot</div><div class="line"></div><div class="line">pvalue=[]</div><div class="line"><span class="keyword">with</span> open(<span class="string">"GOYA.csv"</span>) <span class="keyword">as</span> f:</div><div class="line">    f_csv = csv.reader(f)</div><div class="line">    headers = next(f_csv)</div><div class="line">    pvalue = [float(row[<span class="number">3</span>]) <span class="keyword">for</span> row <span class="keyword">in</span> f_csv]</div><div class="line"></div><div class="line">ax = qqplot(pvalue, color=<span class="string">"#00bb33"</span>, xlabel=<span class="string">"Expected p-value(-log10)"</span>, ylabel=<span class="string">"Observed p-value(-log10)"</span>) <span class="comment"># Q-Q 图</span></div><div class="line">plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="http://7u2had.com1.z0.glb.clouddn.com/post.qq-1.png" alt="QQ图"><br>同样，也可以根据自己的需要对改图进行相关的调整。</p>
<p>以上，便是如何使用Python来制作Manhattan图和QQ图的方法，geneview的集成函数简化了这样的一个过程。</p>
<p>另外，如果你也看过丹麦人的这个GOYA研究，就会发现实际以上的两个图和其文章中的基本是一致的，当然我自己做了些数据清洗的操作，结果上仍然会有些许的不同。虽然此刻下结论还有点为时尚早，但总的来讲，我应该也可以通过这个数据集比较顺利的将其结果重复出来了。</p>
<p>最后，附上利用geneview画曼哈顿图和QQ图的代码：</p>
<p>（1）曼哈顿图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> sys</div><div class="line"></div><div class="line"><span class="keyword">import</span> csv</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="keyword">from</span> geneview.gwas <span class="keyword">import</span> manhattanplot</div><div class="line"></div><div class="line"><span class="keyword">with</span> open(<span class="string">"data/GOYA.csv"</span>) <span class="keyword">as</span> f:</div><div class="line">    f_csv = csv.reader(f)</div><div class="line">    headers = next(f_csv)</div><div class="line">    data = [[row[<span class="number">0</span>], int(row[<span class="number">2</span>]), float(row[<span class="number">3</span>])] <span class="keyword">for</span> row <span class="keyword">in</span> f_csv]</div><div class="line"></div><div class="line">ax = manhattanplot(data, xlabel=<span class="string">"Chromosome"</span>, ylabel=<span class="string">"-Log10(P-value)"</span>)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>（2）QQ图：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> csv</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="keyword">from</span> geneview.gwas <span class="keyword">import</span> qqplot</div><div class="line"></div><div class="line">pvalue=[]</div><div class="line"><span class="keyword">with</span> open(<span class="string">"data/GOYA.csv"</span>) <span class="keyword">as</span> f:</div><div class="line">    f_csv = csv.reader(f)</div><div class="line">    headers = next(f_csv)</div><div class="line">    pvalue = [float(row[<span class="number">3</span>]) <span class="keyword">for</span> row <span class="keyword">in</span> f_csv]</div><div class="line"></div><div class="line">ax = qqplot(pvalue, color=<span class="string">"#00bb33"</span>, xlabel=<span class="string">"Expected p-value(-log10)"</span>, ylabel=<span class="string">"Observed p-value(-log10)"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure></p>
]]></content>
      
        <categories>
            
            <category> 数据可视化 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
            <tag> GWAS </tag>
            
            <tag> manhattan plot </tag>
            
            <tag> Q-Q plot </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基于Flask开发基因组数据web服务的RESTful API（1）]]></title>
      <url>/2015/12/20/2015-12-20-Develop-a-genome-data-RESTful-API-server-by-flask-1.html</url>
      <content type="html"><![CDATA[<p>这真的是一个系统工程！</p>
<p>首先，我要开发一个在线的基因组数据库，目的是能够以符合<a href="http://www.restapitutorial.com/" target="_blank" rel="external">RESTful API</a>设计准则来进行访问的数据Web服务。虽然，此前我没接触过任何Web开发，想想也是困难重重，但这并不能阻止我——兴趣所在，而且我自己也更清楚需求是什么。</p>
<p>那么说干就要干。经过一番有限的比较之后（主要是看各种技术论坛和博客），我发现<a href="http://rubyonrails.org/" target="_blank" rel="external">Rails</a>、<a href="https://www.djangoproject.com/" target="_blank" rel="external">Django</a>和<a href="http://flask.pocoo.org/" target="_blank" rel="external">Flask</a>都适合用来干这个事情，他们的文档都很好，是目前Web开发的主流，社区活跃，用的人多了碰到问题也容易找到解决方案，但是觉得Rails和Django太大，太系统，以至于冗余，我希望的是微框架，因为初步搭成只是第一步，后面一定会有很多自定的优化和开发，而不是一次就完成，太冗余不灵活的话反而会极大影响自己的后续步骤，灵活的可拓展性对我而言反而更重要。这么考虑之后，我就选了Flask，其他的组件需要的时候加入即可，还可以随时换掉，或者自己重写，而且项目结构是完全自定的，这就很适合我的口味了，框架功能不需多，只要解决了Web开发中最重要的问题，作为一个最小可行集就OK——就像人体肠道最小宏基因组一样，浓缩即是精华，其他的枝叶要能够被灵活地删增。</p>
<p>那么选好之后，接下来我是怎么做的呢？第一，学会了Flask，读了很多技术博客，操练了一些如何例子，比如<a href="http://www.pythondoc.com/flask-restful/" target="_blank" rel="external">使用 Python 和 Flask 设计 RESTful API</a>，<a href="http://www.pythondoc.com/flask-mega-tutorial" target="_blank" rel="external">The Flask Mega-Tutorial</a>等，同时读了<a href="https://book.douban.com/subject/26274202/" target="_blank" rel="external">《Flask Web开发：基于Python的Web应用开发实战》</a>，这对于我这种从未接触过Web应用开发的小白来说，真是一本好书，读了之后真是有一种相见恨晚的感觉，内容很好读[但我并不是说它读起来容易。虽然它确实是从基础的讲起，然而对我这类没有任何基础的人来说并不十分容易，好在]从开发到测试到部署每一步都十分清晰，很多内容讲得比博客清楚得多多了，而且整本书本身就是项目驱动的，就像我也是项目驱动要去用Flask一样。<br>第二，要学会数据库。其实这一步和前面是分不开了，基本都是同时进行。但在选择使用那种数据库这一步中我也花了不少心思，最后按照我的数据情况，我选择了非关系型数据库MongoDB。</p>
<p>在这个一周多的学习时间中（工作之外的时候），我学到了很多，不但实现了一个完整的Web应用，还进一步加深加强了对如何更有效应用python的理解和认识。对我提升最大的还不是会了如何用flask去做一个简单的web，而是它们的设计理念，程序/项目如何布局，如何分离，如何做到低耦合，测试和性能分析应该怎么做才合理有效，这些理念是我们平时做基因组数据分析所缺少（马虎）的，因为本来许多生物信息工程师并不懂web开发，甚至从未有过任何IT软件设计的训练，平时也难有时间专门去学习，很多时候都是，任务来了要赶紧写个程序解决一下，然后再来一个任务，又再写个程序处理一下，如此反复，时间久了这些零散的程序根本难以复用，最后慢慢地也就成了垃圾程序。就算是比较大型的生信软件的开发，过程和布局也欠缺规范，关于这一点我深有体会，也想抽时间去系统学习，但实际操作起来并没严格注意，更多的是直接参看github上一些项目的布局和设计来依样画葫芦而已。</p>
<p>接下来就难了，要做符合RESTful API准则的数据web服务，那么就得开始设计API了。怎么做？不同版本如何管理，如何<a href="http://semver.org/lang/zh-CN/" target="_blank" rel="external">语义化</a>，如何条理化管理新旧版本的内容和功能？如何把基因组数据资源转换为JSON格式的序列化字典，大数据资源如何分页等等诸多的细节都需要一一考虑，我参考了<a href="https://docs.solvebio.com/docs/api-overview" target="_blank" rel="external">Solvebio</a>，<a href="api.23andme.com">23andme</a>，<a href="https://developers.douban.com/wiki/?title=api_v2" target="_blank" rel="external">豆瓣</a>等，最后得到了我自认为比较合理的设计方式。</p>
<p>（未完待续）</p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
            <category> 基因组学 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> RESTful API </tag>
            
            <tag> web </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[为什么说FPKM/RPKM是错的]]></title>
      <url>/2015/08/25/why-fpkm-and-rpkm-are-wrong.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/rna.png" alt="cover"></p>
<p>两周前，我接触了一个RNA-seq的项目，做完之后，我重新思考了FPKM和RPKM的计算，觉得它们很可能是不对的，后来查阅了一些文献终于验证了我的想法。现在我重新将这个过程记录了下来：</p>
<h3 id="FPKM和RPKM分别是什么"><a href="#FPKM和RPKM分别是什么" class="headerlink" title="FPKM和RPKM分别是什么"></a>FPKM和RPKM分别是什么</h3><p>RPKM是 <strong>R</strong>eads <strong>P</strong>er <strong>K</strong>ilobase <strong>P</strong>er <strong>M</strong>illion的缩写，它的计算方程非常简单：</p>
<p>$$RPKM = \frac{10 ^ 6 \times n_r}{L × N}$$</p>
<p>其中，$n_r$ 是比对至某一个基因的read数量；$L$是该基因的外显子长度之和除以1000，因此，要注意这里的$L$单位是kb，不是bp；$N$ 是 <strong>有效</strong>比对至基因组的总read数量。<br>FPKM是 <strong>F</strong>regments <strong>P</strong>er <strong>K</strong>ilobase <strong>P</strong>er Million的缩写，它的计算与RPKM极为类似，如下:</p>
<p>$$FPKM =  \frac{10 ^ 6 \times n_f}{L \times N}$$</p>
<p>其中，$n_f$是比对至目标基因的fregment数量。FPKM与RPKM唯一的区别是：F代表fragments，R代表reads。如果是<a href="http://seqanswers.com/forums/showthread.php?t=503" target="_blank" rel="external">Pair-end测序</a>，每个fragments会由这两个成对的reads构成，因此FPKM只计算两个reads能比对到同一个转录本的fragments数量；而RPKM计算的是可以比对到转录本的reads数量（不管Pair-end的两个reads是否能比对到同一个转录本上）。如果是single-end测序，那么FPKM和RPKM计算的结果将是一致的。</p>
<p>以上是这两个量的计算方式。这样计算的目是为了解决在计算RNA-seq转录本丰度时的两个bias：</p>
<p>（1）相同表达丰度的转录本，往往会由于其基因长度上的差异，导致测序获得的Read（Fregment）数不同。总的来说，越长的转录本，测得的Read（Fregment）数越多，但这并不代表表达量就真的多。</p>
<p>（2）由测序文库的不同大小而引来的差异。即同一个转录本，其测序深度越深，通过测序获得的Read（Fregment）数就越多。</p>
<p>FPKM和RPKM通过同时除以L（转录本长度）和除以N（有效比对的Read（Fregment）总数）的办法，最终将不同样本（或者同个样本在不同条件下）的转录本丰度归一化到一个能够进行量化比较的标准上。</p>
<p>上面的式子看起来似乎合情合理，<strong>但是它们却都做错了</strong>。</p>
<h3 id="为什么FPKM-RPKM是错的"><a href="#为什么FPKM-RPKM是错的" class="headerlink" title="为什么FPKM/RPKM是错的"></a>为什么FPKM/RPKM是错的</h3><p>要回答这个问题，我们需要先撇开所有形式上的计算，重新思考这个问题——到底什么是RNA转录本的表达丰度？事实上，对于任何一个取得的样本，它上面任何一个基因的表达量（或者说丰度），都将已是一个客观存在的值，这个值是不管你改变了多少测序环境都不会变的。而且细胞中此刻总共有多少个基因在表达，实际上也已经是客观定好了的。一旦我们开始以这样一种“先知”的形式来理解的时候，有趣的事情就开始出现了。</p>
<p>此时，我们可以假定，对于样本X，其中有一个基因G被转录了g次，同时样本X中所有基因的转录总次数假定是total，那么正确描述基因G转录丰度的值应该是：</p>
<p>$$r_{g}=\frac{mRNA<em>g}{mRNA</em>{total}}$$</p>
<p>没毛病！而且与此同时，样本X中其他基因转录丰度的计算也和以上式子类似，除了要把分子换为其他基因对应的转录次数之外，分母都一样。于是这个有趣的事情就是，所有基因转录本丰度的均值$r_{mean}$将是一个恒定不变的数，由以上定义这个数就是：</p>
<p>$$r<em>{mean} = \frac{1}{g</em>{total}}\sum_{g}^{G}{r<em>g} = \frac{1}{g</em>{total}}\frac{\sum_{g}^{G}{mRNA<em>g}}{mRNA</em>{total}}$$</p>
<p>而</p>
<p>$$\sum_{g}^{G}{mRNA<em>g} = mRNA</em>{total}$$</p>
<p>所以</p>
<p>$$r<em>{mean} = \frac{1}{g</em>{total}}$$</p>
<p>这个值是由基因的总数决定的，也就是说，对于同一个物种，不管它的样本是哪种组织（正常的或病变的等），也不管有多少个不同的样本，只要它们都拥有相同数量的基因，那么它们的$r_{mean}$都将是一致的。这是一个在进行比较分析的时候，非常有意义的恒等关系。</p>
<p>但在实际的操作中，我们是难以直接计算这些r值的。好在只要能够保证模型的自洽性，我们是能通过自建一些统计量来对r值进行间接描述的，比如FPKM和RPKM。本质上它们的目的就是为了描述r。虽然如此，但我们也要注意，所有这些要用来描述转录本丰度的统计量，都应该能等价描述这一恒等关系。也就是说，不管我们使用了什么统计量，它所描述出来的转录本丰度应该且必须是真实丰度$r<em>g$的m倍（m必须是一个根据模型定出的不变值），它的均值也将是$r</em>{mean}$的m倍，至少这样才是得到有意义结果的前提！</p>
<p>（那么）现在，我们回过头来看看FPKM和RPKM的计算式，就会发现它们根本做不到。</p>
<p>举个例子来说明（以FPKM的计算为例），我们假定有两个来自同一个个体不同组织的样本X和Y，这个个体只有5个基因，分别为A、B、C、D和E，它们的长度分别如下：</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/rpkm1.png" alt="gene_length"></p>
<p>由此，我们可以得到，样本X和Y的转录本的不变量，$r<em>{mean}$值都是$r</em>{mean} = \frac{1}{5} = 0.2$。如果FPKM或RPKM是一个合适的统计量的话，那么至少，样本X和Y的平均FPKM（或RPKM）值应该相等。</p>
<p>我们以FPKM的计算的为例子，以下这个表格列出的分别是样本X和Y在这5个基因中比对上的fregment数和各自总的fregment数量：</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/rpkm2.png" alt="fregments"></p>
<p>于是，按照以上公式我们可以得到样本X和Y在这5个基因上的FPKM值分别为：</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/rpkm3.png" alt="FPKM"></p>
<p>接下来就可以计算FPKM的均值了。我们得到，样本X在这5个基因上的FPKM均值$FPKM<em>{mean} = 5,680$；而样本Y的FPKM均值却是$FPKM</em>{mean} = 161,840$!! 它们根本不同，而且差距相当大，<strong>那么究竟为什么会有如此之大的差异？</strong>难道这是我故意构造出来的例子所造成的吗？当然不是，<strong>这是由其数学计算上的缺陷所导致的</strong>。</p>
<p>首先，我们可以把FPKM的计算式拆分成两个部分：（1）等价（其实严格来讲也没那么等价）描述某个基因转录本数量的统计量（$\frac{n_f}{L}$） 和（2）测序获得的总有效Fregment数量的百万分之一（$\frac{N}{10 ^ 6}$）；<strong>看</strong>，FPKM便是这两部分的商！分开来看它们貌似都有点道理，但是合起来的时候其实很没逻辑，尤其是第二部分$\frac{N}{10 ^ 6}$，本来式子的第一部分 （$\frac{n_f}{L}$）就已经是描述某个基因的转录本数量，那么正常来讲，第二部分就应该是描述样本总体的转录本数量（或至少是其等价描述）才能说得通，而且可以看得出FPKM(RPMK)是有此意的，因为这本身就是这一统计量的目的。然并卵，它失败了！$\frac{N}{10 ^ 6}$的大小其实是由RNA-seq的测序深度所决定的，并且是一个和总转录本数量无直接线性关系的统计量——N与总转录本数量之间的关系还受转录本的长度分布所决定，而这个分布往往在不同样本中是有差异的！比如，有些基因，虽然有效比对到它们身上的Fregment数目是相等的，但很明显，长度越长的基因，其被转录的次数就越少。也就是说，N必须将各个被转录的基因的长度考虑进去才能正确描述总体的转录本数！而FPKM（RPKM）显然没有做到这一点，这便是FPKM（RPKM）出错的内在原因。</p>
<h3 id="那么应该是用什么样统计量才合适"><a href="#那么应该是用什么样统计量才合适" class="headerlink" title="那么应该是用什么样统计量才合适"></a>那么应该是用什么样统计量才合适</h3><p>其实，通过以上分析，我们已经可以确定一个更加合理的统计量来描述RNA转录本的丰度了。我意外地发现，这个统计量其实在2012年所发表的一篇关于讨论RPKM的文章（RPKM measure is inconsistent among samples. Wagner GP, Kin K, Lynch VJ. Theory Biosci. 2012.）中就已被提到过了，它被称之为TPM —— <strong>T</strong>ranscripts <strong>P</strong>er <strong>M</strong>illion，它的计算是：</p>
<p>$$TPM = \frac{\frac{n_r \times read_l}{ g_l} \times {10}^{6}} {T} = \frac{n_r \times read_l \times {10} ^ {6} } {g_l \times T}$$</p>
<p>$$T =\sum_{g=i}^{G}{ (\frac{n_r \times read_l}{g_l})_i }$$</p>
<p>其中，$read_l$是比对至基因G的平均read长度，$g_l$ 是基因G的外显子长度之和（这里无需将其除以1000了）。在不考虑比对剪切的情况下，$read_l$这个值往往都是一个固定值（如100bp或者150bp等），因此我们也可以将$read_l$统一约掉，那么分子就会蜕变成RPKM计算式的第一部分，但把$read_l$留着会更合理。这样，整个统计量就很好理解了，分子是基因G的转录本数（等价描述），分母则为样本中总转录本的数量，两者的比值TPM——便是正确描述基因G的转录本丰度！并且，简单计算我们就可以知道TPM的均值是一个独立于样本之外的恒定值：</p>
<p>$$TPM_{mean} = \frac{10^6}{N}$$</p>
<p>这个值也刚好是$r_{mean}$的$10^6$倍，满足上述等价描述的关系。我们仍然通过上面的例子来进作说明，为简单起见我们只把fregment换为read，其他数字都一样，并且统一假设$read_l$都是一样的：</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/rpkm4.png" alt="TPM"></p>
<p>接着，我们可以分别计算样本X和Y的TPM_mean,并且很明显它们都是$200000 = 10^6 / 5$. 而且，经过这样的标准化之后，X和Y就处于同样的一个标准上了，此刻，彼此之间的比较分析才是真正有意义的。</p>
<h3 id="既然FPKM-RPKM是错的，那为什么大家直到现在都还在用，而且还真找到了（能被实验所验证）有价值的结果呢？"><a href="#既然FPKM-RPKM是错的，那为什么大家直到现在都还在用，而且还真找到了（能被实验所验证）有价值的结果呢？" class="headerlink" title="既然FPKM/RPKM是错的，那为什么大家直到现在都还在用，而且还真找到了（能被实验所验证）有价值的结果呢？"></a>既然FPKM/RPKM是错的，那为什么大家直到现在都还在用，而且还真找到了（能被实验所验证）有价值的结果呢？</h3><p>关于对于这个问题，我也思考过。而且我们都知道2008那篇关于RPKM的文章更是用实验结果证明了，RPKM是一个合适的统计量，符合qPCR的验证结果。但归根到底，我觉得眼见未必为实，很多实验其实是表象的，我们更应该从其本质意义和原理上去考虑。FPKM/RPKM之所以看起来会是一个合适的值，我想主要原因有二：</p>
<p><strong>其一，它们和TPM之间存在一定的正比关系</strong>。这可通过它们各自的数学计算方程式看出来（以RPKM的计算为例）：</p>
<p>$$RPKM = \frac{T \times 10^3}{ N \times read_l } \times TPM$$</p>
<p>而且在同一个样本内部由于T，N和$read_l$实际上都是定值，因此同个样本内的RPKM和TPM是可以恒等转换的。然而在样本与样本之间就不行，因为不同样本T和N是不同的（假定测序长度$read_l$都一样），这就导致它们之间的转换因子大小不一样！</p>
<p>如以上例子，对于样本X，TPM转换到RPKM的转换因子为：0.0284，但在样本Y中，它的转换因子却是：0.8092。而由于这个基础标准的改变，导致其原本所要描述的“转录本丰度”变得不可比较。然，这其实不是最根本的原因，更本质的原因是，这个转换会对本来已经正确标准化了的结果——TPM，再次做了一次无意义的不等变换，最终导致了结果不可解释。如何理解呢，后文会有补充，这里先简单说一下：这个数学转换式子仅是告诉了我们这样子来计算是可行的，但是在RNA-seq的实际应用场景中，它其实是无生物（或物理）意义的；</p>
<p><strong>其二，实验验证的精度是有限的，常用的qPCR也只能给出定性的比较结果，而且实验验证也未必总能成功</strong>。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>现在回过头来总结一下。事实上，FPKM/RPKM最大的问题就在于其无意义性。我们所要表达出来的任何统计量，它的变化都应该要能对应到物理或生物过程中的变化，如果做不到这一点，那么这个统计量往往都是无意义的，用它得到的结果就算看起来符合预期也只不过是数值上的巧合，本质上是不可解释的。<strong>FPKM/RPKM的分母($N/10^6$)并不具有任何形式的生物意义，它所能表达出来的这个量，只能代表测序深度的变化，而无法作为表达生物过程的量，比如无法代表（等价代表）样本中转录本的总量</strong>。</p>
<p>一个统计量该如何计算，说到底都只是一个“术”的问题，而我们应该尽可能在接近其本质意义的地方去确定。</p>
<p>FPKM/RPKM和TPM存在一定的正比关系，因此我们在使用FPKM/RPK时，有些时候确实也能获得可以被实验所验证的“好”结果，但其实它是一个橡皮筋，它的单位刻度是会随着样本的不同而改变的。到头来，样本之间的差异比较实际上也只是在不同的标准下进行的，这样的比较就算得到了所谓的“好”结果，那又有什么意义，根本就是个错误的东东。想想就是由于这种统计量，我们一定已经获得了许多的假阳性结果，同时也肯定错过了许多本来真正有意义的差异，真是弯路走尽也不知，而且还浪费了大堆的心情和时间。</p>
<p>这篇文章：A comprehensive evaluation of normalization methods for Illumina high-throughput RNA sequencing data analysis. Briefings in Bioinformatics.10.1093/bib/bbs046. 对7种主要的RNA-seq标准化方法（但不包含本文提到的TPM）做了一个详细的比较，它用实际结果进行比较（不同于本文所用的数学方式）也得出了RPKM/FPKM这些统计量应该被摒弃的结论，因为它所描述出来的结果是最不合理的，其实所有类似于RPKM/FPKM的统计量在描述转录本丰度的时候都应该被摒弃。</p>
<p>【注】本文已同时发布于<a href="http://blog.fungenomics.com/2016/07/why-fpkm-and-rpkm-are-wrong.html" target="_blank" rel="external">泛基因fungenomics</a>以及我的个人微信公众号。</p>
<blockquote>
<p><a href="http://bib.oxfordjournals.org/content/early/2012/09/15/bib.bbs046.full" target="_blank" rel="external">http://bib.oxfordjournals.org/content/early/2012/09/15/bib.bbs046.full</a><br><a href="http://www.ncbi.nlm.nih.gov/pubmed/22872506" target="_blank" rel="external">http://www.ncbi.nlm.nih.gov/pubmed/22872506</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> RNA </tag>
            
            <tag> RPKM </tag>
            
            <tag> FPKM </tag>
            
            <tag> TPM </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[目前最好最完整的SOAPdenovo使用说明]]></title>
      <url>/2015/07/09/The-best-manual-for-soapdenovo2.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/tree.jpeg" alt="cover"></p>
<p>由于丹麦人国家基因组项目的原因，近期我整理了一份关于SOAPdenovo2的使用说明，内容包括了程序使用、参数的详细说明、参数如何调整、各个主要输出文件的格式说明等，而且我敢说这是目前最好最全的！</p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>SOAPdenovo（目前最新版是SOAPdenovo2）是一种应用de Bruijn graph组装短read的方法，它以kerm为节点单位，利用de Bruijn图的方法实现全基因组的组装，与其他短序列组装软件相比，它可以进行大型基因组，比如人类基因组的组装，组装结果更加准确可靠，可以通过组装的结果非常准确地鉴别出基因组上的序列结构性变异，为构建全基因组参考序列和以低测序成本对未知基因组实施精确分析创造了可能。</p>
<p>下载地址：<a href="http://soap.genomics.org.cn/soapdenovo.html" target="_blank" rel="external">http://soap.genomics.org.cn/soapdenovo.html</a></p>
<p>安装：</p>
<ul>
<li>下载SOAPdenovo的压缩包          </li>
<li>解压缩     </li>
<li>将得到可执行文件SOAPdenovo和一个配置文件的模板example.contig</li>
</ul>
<h3 id="使用程序及参数"><a href="#使用程序及参数" class="headerlink" title="使用程序及参数"></a>使用程序及参数</h3><p>SOAPdenovo可以一步跑完，也可以分成四步单独跑，一步跑完的脚本:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./SOAPdenovo all -s lib.cfg -K 29 -D 1 -o ant &gt;&gt;ass.log</div></pre></td></tr></table></figure>
<p>四步单独跑的脚本:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">./SOAPdenovo pregraph -s lib.cfg -d 1  -K 29 -o ant &gt;pregraph.log</div><div class="line">./SOAPdenovo contig -g ant -D 1 -M 3 &gt;contig.log</div><div class="line">./SOAPdenovo map -s lib23.cfg -g ant &gt;map.log</div><div class="line">./SOAPdenovo scaff -g ant -F &gt;scaff.log</div></pre></td></tr></table></figure></p>
<h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">用法：/PathToProgram/SOAPdenovo all -s configFile [-K kmer -d KmerFreqCutOff -D EdgeCovCutoff -M mergeLevel -R -u -G gapLenDiff -L minContigLen -p n_cpu] -o Output</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">-s    STR     配置文件</div><div class="line">-o    STR     输出文件的文件名前缀</div><div class="line">-g    STR     输入文件的文件名前缀</div><div class="line">-K    INT     输入的K-mer值大小，默认值23，取值范围 13-63</div><div class="line">-p    INT     程序运行时设定的线程数，默认值8</div><div class="line">-R            利用read鉴别短的重复序列，默认值不进行此操作</div><div class="line">-d    INT     去除频数不大于该值的k-mer，默认值为0</div><div class="line">-D    INT     去除频数不大于该值的由k-mer连接的边，默认值为1，即该边上每个点的频数都小于等于1时才去除</div><div class="line">-M    INT     连接contig时合并相似序列的等级，默认值为1，最大值3。</div><div class="line">-F            利用read对scaffold中的gap进行填补，默认不执行</div><div class="line">-u            构建scaffold前不屏蔽高覆盖度的contig，这里高频率覆盖度指平均contig覆盖深度的2倍。默认屏蔽</div><div class="line">-G    INT     估计gap的大小和实际补gap的大小的差异，默认值为50bp。</div><div class="line">-L            用于构建scaffold的contig的最短长度，默认为：Kmer参数值 ×2</div></pre></td></tr></table></figure>
<h3 id="使用方法及示例"><a href="#使用方法及示例" class="headerlink" title="使用方法及示例"></a>使用方法及示例</h3><p>（1）示例<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">SOAPdenovo all -s HCB.lib -K 25 -d -o <span class="built_in">test</span></div></pre></td></tr></table></figure></p>
<p>（2） 输入文件<br>configFile，配置文件内容如下，非程序生成，需要软件使用者自己配置。各个说明参考如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">#maximal read length （read的最大长度）</div><div class="line">以“#”开头的行是注释内容</div><div class="line">max_rd_len=50    </div><div class="line">#该值一般设置的比实际read读长稍微短一些，截去测序最后的部分，具体长度看测序质量</div><div class="line">[LIB]</div><div class="line">#文库信息以此开头</div><div class="line">avg_ins=200</div><div class="line">#文库平均插入长度，一般取插入片段分布图中给出的文库大小</div><div class="line">reverse_seq=0</div><div class="line">#序列是否需要被反转，目前的测序技术，插入片段大于等于2k的采用了环化，所以对于插入长度大于等于2k文库，序列需要反转，reverse_seq＝1，小片段设为0</div><div class="line">asm_flags=3</div><div class="line">#该文库中的read序列在组装的哪些过程（contig/scaff/fill）中用到</div><div class="line">设为1：只用于构建contig；</div><div class="line">设为2：只用于构建scaffold；</div><div class="line">设为3：同时用于构建contig和scaffold；</div><div class="line">设为4：只用于补洞</div><div class="line"></div><div class="line">【注意】短插入片段(&lt;2K)的设为3，同时用于构建contig和scaffold，长插入片段(&gt;=2k)设为2，不用于构建contig，只用于构建scaffold，454single 长reads只用于补洞。</div><div class="line">rank=1</div><div class="line">#rank该值取整数，决定了reads用于构建scaffold的次序，值越低，数据越优先用于构建scaffold。设置了同样rank的文库数据会同时用于组装scaffold。一般将短插入片段设为1；2k设为2；5k设为3；10k设为4；当某个档的数据量较大时，也可以将其分为多个档，同样，当某档数据量不足够时，可以将多个档的数据合在一起构建scaffold。这里说的数据量够与不够是从该档的测序覆盖度和物理覆盖度两个方面来考虑的。</div><div class="line">pair_num_cutoff=3</div><div class="line">#可选参数，pair_num_cutoff该参数规定了连接两个contig 或者是pre-scaffold 的可信连接的阈值，即，当连接数大于该值，连接才算有效。短插入片段(&lt;2k)默认值为3，长插入长度序列默认值为5</div><div class="line">map_len=32</div><div class="line">#map_len该参数规定了在map过程中 reads和contig的比对长度必须达到该值（比对不容mismacth和gap），该比对才能作为一个可信的比对。可选参数，短插入片段(&lt;2k)一般设置为32，长插入片段设置为35，默认值是K＋2。</div><div class="line">q1=/path/**LIBNAMEA**/fastq_read_1.fq</div><div class="line">#read 1的fastq格式的序列文件，“/path/**LIBNAMEA**/fastq_read_1.fq”为read的存储路径</div><div class="line">q2=/path/**LIBNAMEA**/fastq_read_2.fq</div><div class="line">#read 2的fastq格式的序列文件，与read1对应的read2文件紧接在read1之后）</div><div class="line">f1=/path/**LIBNAMEA**/fasta_read_1.fa</div><div class="line">#read 1的fasta格式的序列文件</div><div class="line">f2=/path/**LIBNAMEA**/fasta_read_2.fa</div><div class="line">#read 2的fasta格式的序列文件</div><div class="line">q=/path/**LIBNAMEA**/fastq_read_single.fq</div><div class="line">#单向测序得到的fastq格式的序列文件</div><div class="line">f=/path/**LIBNAMEA**/fasta_read_single.fa</div><div class="line">#单向测序得到的fasta格式的序列文件</div><div class="line">p=/path/**LIBNAMEA**/pairs_in_one_file.fa</div><div class="line">#双向测序得到的一个fasta格式的序列文件</div></pre></td></tr></table></figure></p>
<h3 id="输出文件及说明"><a href="#输出文件及说明" class="headerlink" title="输出文件及说明"></a>输出文件及说明</h3><p>SOAPdenovo 分四部分别对应的输出文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">1. pregraph  生成7个文件 *.kmerFreq  *.edge  *.preArc  *.markOnEdge  *.path *.vertex  *.preGraphBasic</div><div class="line">2. contig       生成4个文件 *.contig  *.ContigIndex  *.updated.edge  *.Arc</div><div class="line">3. map          生成3个文件 *.readOnContig  *.peGrads  *.readInGap</div><div class="line">4. scaff        生成6个文件 *.newContigIndex  *.links  *.scaf  *.scaf_gap  *.scafSeq  *.gapSeq</div></pre></td></tr></table></figure></p>
<p>*.contig：contig序列文件，fasta格式；</p>
<p>*.scafSeq：fasta格式的scaffold序列文件，contig之间的gap用N填充；</p>
<p>对于得到的*.scafSeq文件还需要用GapCloser去合并其中的gap，最后的contig文件则是对补洞之后的scaffold文件通过打断N区的方法得到。</p>
<p>以上两个文件是组装结果中最主要的输出。</p>
<p>*.scaf：包括scaffold中contig的详细信息；在scaffold行中包括scaffold名字、contig长度和该scaffold长度。在contig行包括contig名字、contig在scaffold上的起始位置、正反链、长度和contig间的链接信息;</p>
<p>*.links：contig间的pair-end连接信息;</p>
<p>*.readOnContig：reads在contig上的位置;</p>
<p>*.peGrads： 主要可以通过调整本文件中的参数来显示构建scaffold所用到的插入片段库的个数，总共要到的read数，最长的read的长度，每个库对应的哪些reads，rank设置，pair_num_cutoff设置。例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">grads&amp;num: 10   522083934       70</div><div class="line">323     104577616       1       3</div><div class="line">334     180770522       1       3</div><div class="line">345     226070520       1       3</div><div class="line">486     361955834       2       3</div><div class="line">2200    392088076       3       5</div><div class="line">2290    422272580       3       5</div><div class="line">2400    445522690       3       5</div><div class="line">4870    475666064       4       5</div><div class="line">9000    511030930       5       8</div><div class="line">9110    522083934       5       5</div></pre></td></tr></table></figure>
<p>该文件中共分成4列。组装的配置文件中有n个文库，该文件则有n+1行，且按照文库大小顺序排列。<br>第1行中，第二三四列分别是 所用文库，reads总数和组装中用到的最长的reads长度。<br>第2行中，四列分别是文库大小，文库中的reads数目，该文库reads用到的rank等级和该文库中reads用到的pair_num_cutoff。<br>第3～n+1行，四列分别是文库大小，文库中的reads数目加上前面的文库中的reads总数，该文库reads用到的rank等级和该文库中reads用到的pair_num_cutoff。<br>如果配置文件中没有设置pair_num_cutoff，即使用默认参数，则最后一列显示为0。</p>
<p>对于SOAPdenovo的每个步骤都有日志文件输出，要保存好日志文件，日志文件中包含有很多有用的信息。</p>
<h3 id="SOAPdenovo日志输出说明"><a href="#SOAPdenovo日志输出说明" class="headerlink" title="SOAPdenovo日志输出说明"></a>SOAPdenovo日志输出说明</h3><p>1）pregraph.log:  其中有很多的统计信息，包括构建debruijn-graph时用到多少reads数，构图中生成了多少uniq的kmer以及设置-d参数后去除了多少kmer。<br>在pregraph中，可选参数有 –R –K –d  结果如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">5467781332 nodes allocated, 70662750348 kmer <span class="keyword">in</span> reads, 70662750348 kmer processed</div><div class="line">3283081670 kmer removed</div></pre></td></tr></table></figure>
<p>其中Kmer 数是取决于所设k值大小以及数据量，nodes数即特异性的kmer数目，当nodes数目过高（一般和基因组大小差不多大小），可能是数据的错误率比较高，也可能是存在杂合。若nodes数目偏小，并且kmer数目很多，则基因组本身可能存在一定的重复度。对于k值的选取，当数据量充足时（&gt;=40X），植物基因组一般采用大kmer会有比较好的效果，而对于动物基因组，k值一般多取27和29则足够。kmer removed表示的 –d 参数所去除的低频的kmer。</p>
<p>2）contig.log: contig 中，可选参数 –R –D –M，注，-R 参数的选定，必须pregraph和contig中同时选择才有效。结果例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">16430183 pairs found, 2334584 pairs of paths compared, 1674493 pairs merged</div></pre></td></tr></table></figure>
<p>从merged的数量可以作为估计杂合以及测序错误的程度。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sum up 1932549703bp, with average length 1170</div><div class="line">the longest is 36165bp, contig N50 is 2871 bp,contig N90 is 553 bp</div></pre></td></tr></table></figure>
<p>3）map.log:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Output 415219610 out of 1956217742 (21.2)% reads in gaps</div><div class="line">1661094582 out of 1956217742 (84.9)% reads mapped to contigs</div></pre></td></tr></table></figure></p>
<p>一般情况下，reads in gap的比例和map to contig 的比例总和大于1。可能是因为reads map到多个地方都被算在其中的原因。当map to contig的比例很高（80%左右时），但是组装效果并不很好，可能是重复序列比较多。reads in gap比例较高（大于40%），是因为基因组组装的较碎，gap区域较多。<br>map_len 默认值=K+5，当默认值大于设置的map_len时，以默认值为准，当默认值小于map_len值时，设置的map_len为准。</p>
<p>4）scaff.log:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">average contig coverage is 23, 5832270 contig masked</div></pre></td></tr></table></figure></p>
<p>构建scaffold是对高频覆盖的contig进行屏蔽（即频率高于average contig coverage的两倍的contig不用于构建scaffold），从这里可以看出组装的基因组一定的重复情况。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">estimated PE size 162, by 40034765 pairs</div><div class="line">on contigs longer than 173, 38257479 pairs found,SD=8, insert_size estimated: 163</div></pre></td></tr></table></figure>
<p>173 是配置文件中该文库的insertsize，163 是根据reads  map到contig上的距离的估计值，8是这个分布的标准偏差。一般考虑 比对上去的pair数目和SD值。若pair对数很多且SD值很小（小片段文库数据不超过三位数，大片段文库数据部超过500），那我们一般可以将配置文件中的文库插入片段的值改对短插入片段文库（<1k）的大小估计值，一般是比较准确的，下次组装以及补洞时应根据这个值对原来配置文件中的insertsize信息做修正。对于大片段文库（>=2K），因为是把reads map到contig上，若最长contig较短时，可能找不到成pair比对上去的reads，这时，无法估计文库大小，需要自己将大片段一级一级的map到前一级的组装结果上，然后再分析大片段文库的插入片段大小。注，需要调整insertsize信息时，只需要修改<em> .peGrads文件中的第一列，然后删除</em>.links文件，重新跑scaff这一步即可。即构建scaffold时，主要是根据*.links文件的信息进行连接。</1k）的大小估计值，一般是比较准确的，下次组装以及补洞时应根据这个值对原来配置文件中的insertsize信息做修正。对于大片段文库（></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">Cutoff for number of pairs to make a reliable connection: 3</div><div class="line">1124104 weak connects removed (there were 4773564 active cnnects))</div></pre></td></tr></table></figure>
<p>Cutoff for number是在配置文件中设的pair_num_cutoff值，weak connects是低于这个值被认定为无效的连接数，active connects是满足cutoff的连接数，根据这个数值可对pair_num_cutoff做调整</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Picked  25241 subgraphs,4 have conflicting connections</div></pre></td></tr></table></figure>
<p>conflicting connections 是表示构建scaffold时的矛盾数，矛盾数比较高（&gt;100）时，可根据前面的有效连接数，适当提高pair_num_cutoff值，即提高scaffold连接要求的最少关系数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">182483 scaffolds&amp;singleton sum up 1990259817bp, with average length 10906</div><div class="line">the longest is 6561520bp,scaffold N50 is 836795 bp, scaffold N90 is 157667 bp</div></pre></td></tr></table></figure>
<p>scaffold 统计信息，将是根据rank分梯度的统计:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Done with 13301 scaffolds, 2161915 gaps finished, 2527441 gaps overall</div></pre></td></tr></table></figure></p>
<p>-F 参数补洞的统计信息。</p>
<h3 id="参数调整"><a href="#参数调整" class="headerlink" title="参数调整"></a>参数调整</h3><p>一般组装时需要调整的参数，主要分两种：</p>
<p>一种是针对脚本中的参数改动：如调整  -K  -R  -d  -D  -M<br>-K 值一般与基因组的特性和数据量相关，目前用到的SOAPdenovo软件主要有两个版本，grape1123和grape63mer，其中grape1123是最新版的组装软件，K值范围13-31，grape63mer是可以使用大kmer的组装版本，K值范围13-63。 </p>
<p>【经验】：植物基因组的组装采用大kmer效果会比较好（要求短片段reads长度75bp），动物基因组很少有用到大kmer后有明显改进效果的，且动物基因组的组装K值一般设置为27和29较多。</p>
<p>-R参数，对于动物基因组，R参数一般不设置，植物基因组由于较多的repeat区，则设置R参数后，效果更好。注意，设置-R时，一般使用-M 的默认值。（熊猫基因组组装时得出的结论）</p>
<p>-M 参数，0-3,默认值1。一般杂合率为千分之几就设为几。熊猫基因组组装时-M 2 。</p>
<p>-d 参数，对于没有纠错，没有处理的质量又较差的原始数据，kmer的频数为1的很多的数据的组装，一般设置为-d 1 则足够。对于处理过，或者是测序质量较好的数据，可以不用设置。数据量很多时，也可以以-d 参数去除部分质量稍差的数据。</p>
<p>-D 参数，默认为1，一般不用另行设置。</p>
<p>第二种，从map这一过程去调节参数。可以调整配置文件的map_len的值和调整文件*.peGrads。</p>
<p>当文库插入片段分布图中文库大小与实验给出的文库大小差异很大时，调整*.peGrads文件中的插入片段大小。</p>
<p>根据每一档数据的数据量去调整文库的rank等级。当该文库的数据量很多或者是在构建scaffold的过程中的冲突数很多时，可是适当的调大第四列 的pair_num_cutoff，把条件设置的更严一些。</p>
<h3 id="内存估计"><a href="#内存估计" class="headerlink" title="内存估计"></a>内存估计</h3><p>SOAPdenovo的四个步骤消耗的内存是不一样的，其中第一步消耗的内存最多，使用没有纠错的的reads，(K&lt;=31)第一步消耗的内存在基因组大小的80－100倍左右，纠过错则在40－50倍左右，第二步相对消耗的内存会少很多，第三步消耗的内存是仅次于第一步的，在第一步的一半左右，第四步消耗的内存也会比较少。对于CPU的使用，默认是8个，如果申请内存时申请一个计算节点的所有内存测将CPU就设置为该计算节点的CPU个数充分利用计算资源，如果仅申请一个节点的部分内存则根据实际情况考虑。对于大kemr(K&gt;31)其内存使用是(k&lt;=31)的1.5倍左右，有时甚至更多，要充分估计内存的使用，在第一次运行的时候考虑不能太保守。</p>
<h3 id="常见错误"><a href="#常见错误" class="headerlink" title="常见错误"></a>常见错误</h3><p>1）配置文件中read存储路径错误</p>
<p>只输出日志文件。<br>pregraph.log中的错误信息：“Cannot open /path/<strong>LIBNAMEA</strong>/fastq_read_1.fq. Now exit to system…”</p>
<p>2）-g 后所跟参数与pregraph（第一步） -o  后所跟参数名不一致</p>
<p>contig  map  scaff 这三个步骤都只是输出日志文件。</p>
<p>contig.log中的错误信息：“Cannot open *.preGraphBasic. Now exit to system…”</p>
<p>map.log中的错误信息：“Cannot open *.contig. Now exit to system…”</p>
<p>scaff.log中的错误信息：“Cannot open *.preGraphBasic. Now exit to system…”</p>
<p>3）从map开始重新跑时，需要删除*.links文件，否则会生成core文件，程序退出。</p>
<p>【注】仅同时发布于<a href="http://www.fungenomics.com/article/26" target="_blank" rel="external">泛基因fungenomics</a></p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 组装 </tag>
            
            <tag> SOAPdenovo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[de Bruijn graph组装基因组的时候，Kmer数必须是奇数的原因]]></title>
      <url>/2015/05/22/why-Kmer-count-should-be-odd-number-in-assembly-algorithm.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/37v38closeuptata.png" alt="cover"></p>
<p>根本原因就是为了避免导致 <strong>正反链混淆</strong>。</p>
<p>一开始，我并没弄明白，后来仔细想想也终于懂了。</p>
<p>如果kmer是偶数，我们会发现基因组上有些序列（如，CGCGCGCG，kmer=4）的Kmer在反向互补后得到的序列仍然是它自身！而这是不能允许的，因为这将导致你无法区分某段序列的kmer到底是属于它自身还是说只是来自于它的互补链！！这会给解de Bruijn graph带来极大的困难！</p>
<p>或许你会觉得“为什么我需要纠结于序列是不是来自互补链呢？毕竟双链DNA的正反链是严格互补的啊，目前的基因组组装技术也就是把它们合并装在一起的呀！”，若你能这样来理解其实是没问题的，但前提却是基因组必须能够被一次性完整地（至少是非常接近完整）测出来，这时的测序深度只需是1就可以了——如果都已经把基因组完整测序出来了，那还要组装个屁呀 ！！！！！ (╯‵□′)╯︵┻━┻。</p>
<p>并且目前的NGS测序技术也做不到通测，一般来说都是测出成千上百万千万亿万个小小的片段（也叫read，长度一般是100bp-250bp）。同时基因组会被反复测很多层，所以这个时候构建kmer的单位实际上就是对这些read进行的，具体的操作就是按照kmer的长度把这些read切割成更小的片段。这个时候在构建de Bruijn graph时，能够保证正确地把同属于一条read上的kmer连接起来，就显得极其重要了呀！我们总不能一会儿把A kmer正确地连到它自己所在的read，一会儿又连到它互补链的read上去呀！这就是为何kmer不能是偶数的原因了，因为只有奇数，才能保证每个kmer序列的反向互补Kmer与自身是不同的，<strong>而这个不同的真正意义就是为了避免正反链混淆</strong>，如 ：5-mer的 CGCGC，反向互补后是 GCGCG， 它们是不同的；这就不会像 4-mer，CGCG发现它反向互补后仍然是CGCG！！</p>
<p>最后上一个来自GenomeResearch的图，给大家欣赏下一个Repeat序列的组装过程。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/assembly.jpg" alt="assembly-repeat"></p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 组装 </tag>
            
            <tag> de Bruijn graph </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[在Python中调用C++模块]]></title>
      <url>/2015/04/03/2015-04-03-How-Does-Python-call-Cpp-module.html</url>
      <content type="html"><![CDATA[<p>在Python中成功实现了对原来C++代码模块的复用！这个好处多多，Python写得快，C++跑得快，那就是既快又快了！方法很简单，以至于我能够用一张截图记录下整个过程（点击图片看大图）！</p>
<p><a href="http://blog-fungenomics-com.qiniudn.com/st.post.2015-04-03-Python-call-Cpp-Module.png" target="_blank" rel="external"><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2015-04-03-Python-call-Cpp-Module.png" alt="Python-call-C++-Module"></a></p>
<p>其实，注意到，必须在原来的C++代码后面添加extern “C”来辅助（C则不需要，这也是与复用C代码时最大的不同点），不然Python在调用这个构建后的动态链接库时是找不到原来的方法或者函数的，说到底还都是因为当前Python的设定中只能调用C函数，而不能直接调用C++的方法，因此extern “C”封装的函数必须是C风格的，也即就是说，函数中的所有参数（输入参数和返回参数）都不能包含任何非C定义的类型（包括不能包含只属于C++的类型）。</p>
]]></content>
      
        <categories>
            
            <category> 信息图 </category>
            
            <category> 编程技术 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> C++ </tag>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[何不争当数据科学家？]]></title>
      <url>/2015/03/22/2015-03-22-how-to-become-a-data-scientist.html</url>
      <content type="html"><![CDATA[<p>当今世上大数据横行，于是<del>duang</del>许多人都想成为数据科学家。</p>
<p>分享两张路线图：</p>
<p>（1）成为数据科学家的8个简单步骤！<br><a href="http://blog-fungenomics-com.qiniudn.com/st.post.2015-03-22-Fig1.jpg" target="_blank" rel="external"><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2015-03-22-Fig1.jpg" alt="How-to-become-a-data-scientist"></a></p>
<p>（2）数据科学家之路<br><a href="http://blog-fungenomics-com.qiniudn.com/st.post.2015-03-22-Fig2.png" target="_blank" rel="external"><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2015-03-22-Fig2.png" alt="Load-to-become-a-data-scientist"></a></p>
<p>OK，步骤就是这样，图样说简单，过程其实不简单，但却也都是可操作的！这种东西看多了，只觉得什么都是虚的，这些也都是术，关键还是在人，既然有抱负，地图就在那了，接下来，就看你是否有耐心，是否愿意花时间走下去。奋斗吧，Fighting！</p>
<blockquote>
<p><a href="http://blog.datacamp.com/how-to-become-a-data-scientist" target="_blank" rel="external">http://blog.datacamp.com/how-to-become-a-data-scientist</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> 信息图 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 数据科学家 </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[二代测序：碱基平衡性与barcode选择]]></title>
      <url>/2015/02/01/2015-02-01-base-balance-and-barcode-selection.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.Figure0.png" alt="Sequencing"></p>
<p>这是转载过来的一篇文章，虽然基础，但却觉得是很重要的知识,所以便记录了下来，原始出处来自微信公众号“基因测序资讯AGCT”。</p>
<h2 id="碱基平衡性"><a href="#碱基平衡性" class="headerlink" title="碱基平衡性"></a>碱基平衡性</h2><hr>
<p>碱基复杂度与碱基多样性是一个意思；复杂度高，碱基即平衡。低多样性(low diversity)即碱基不平衡，指碱基的组成太单纯了，种类少。碱基复杂度本来无关紧要，从前除了设计PCR的时候考虑高GC(GC-rich)以外，基本没人思考这个问题，没人觉得这是一个问题。随着Illumina的二代测序技术风靡全球，独占鳌头，这个不起眼的概念意外地变得重要起来。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.Figure1.jpg" alt="Figure1"></p>
<p><strong>一、概念</strong></p>
<p>对于一个基因来说，它所包含的碱基种类越多，则碱基复杂度越高；如果各种碱基的百分含量越接近一致，则碱基组成越平衡。</p>
<p>假设一个DNA片段，它的全部碱基都是A，AAAAAAAAAAAAAAAAAAAAAAAA，显然其碱基组成是极度不平衡的。</p>
<p>DNA碱基有4种：AGCT。所以碱基最平衡的情况就是：%A=%G=%C=%T=25%，比如这样的DNA片段：AGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCT。</p>
<p>以上是从纵的方面讲的。对于二代测序，更重要的是横的方面。假设12个基因整整齐齐站成一排，第一个位置的12个碱基如果都是A，复杂度太低，严重不平衡；如果A和G各有6个，虽然平衡了，但是复杂度还是不够；如果AGCT各有3个，最复杂，也最平衡；如果A3个G4个C4个，T1个，虽然复杂，但是严重不平衡。</p>
<p><strong>二、影响</strong></p>
<p>4张滤色片，在4个波长处收集信号，然后合成，进行cluster定位及其他运算。如果缺少一种碱基，该波长的照片就是全黑的，没有信号，无法完成图片合并以及cluster定位，导致数据浪费。</p>
<p><strong>需要特别注意碱基复杂度的二代测序应用:</strong>PCR产物测序，特别是用于鉴定细菌、真菌以及其他物种的16S rRNAPCR产物测序；小RNA测序；甲基化测序。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.Figure2.jpg" alt="Figure2"></p>
<p><strong>三、增加碱基复杂度的方法</strong></p>
<p>文库：</p>
<ol>
<li>把不同的文库混合在一起。        </li>
<li>如果没有其他文库，那么掺入人基因组DNA文库、人外显子组文库或者PhiX标准品。这些都是已知碱基平衡的。     </li>
</ol>
<p>引物：     </p>
<ol>
<li>对于PCR产物来说，只要引物长度不同，就能自然错开，增加碱基复杂度。       </li>
<li>采用多对序列不同的引物来完成扩增，然后将产物混合在一起。     </li>
</ol>
<p>Barcodes：          </p>
<ol>
<li>仔细挑选barcode组合，确保每个位置都有3-4种碱基且碱基分布均匀。</li>
</ol>
<h2 id="barcode-选择"><a href="#barcode-选择" class="headerlink" title="##barcode 选择"></a>##barcode 选择</h2><p>很多情况下，我们需要把多个样本混合在一起，在同一个通道(lane)里完成测序。像转录组测序、miRNA测序、lncRNA测序、ChIP测序等等，通常每个样本所需要的数据量都比较少，远少于HiSeq一个通道的产出能力，混合样本是非常常见。以转录组测序为例，一个样本测序60 M reads (8G PF data) ，就能够满足绝大部分研究所需。而HiSeq2500-PE125的一条通道，使用V4试剂，数据产出&gt;480 M reads。为了充分利用测序仪产能，节约成本，需要把7~8个RNA样本混合起来。</p>
<p>为了能够把测序数据按样本分离（de-multiplexing），在构建文库(library)的时候，需要用不同的标签序列(index, 也叫barcode)对文库进行标记。只有文库作了记号，数据才能区分。</p>
<p>Barcode的选择是一门技术活。如果barcode组合不佳，标签序列测序质量下降，部分或者全部标签碱基识别不正确，将导致部分数据无法归属到任何一个样本，成为undetermined数据，造成浪费。</p>
<p><strong>一、如何判断barcode组合好坏</strong></p>
<ol>
<li><p>碱基平衡。好的barcode组合必须是“4种碱基达到平衡”的，或者说碱基复杂度高。具体就是：<br>a. 在一组barcode的每一个位置，同时存在A、G、C、T四种碱基，不缺少任何一种碱基；<br>b. 这4种碱基的比例接近，最好各1/4，分别为25%左右，没有任何一种碱基特别多或者特别少。  </p>
</li>
<li><p>激光平衡。 受客观条件限制</p>
</li>
</ol>
<p>a.试剂盒提供的barcode种类有限<br>b.有些barcode已经被其他样本占用，导致可选的余地受限制，这就导致barcode组合经常无法达到理想的碱基平衡要求。退而求其次，要力保“红绿激光达到平衡”。</p>
<blockquote>
<p>在所有型号的Illumina测序仪中，<strong>A和C两种碱基共用一种激光，由波长660 nm的红激光激发</strong>；<strong>G和T共用一种激光，由波长532 nm的绿激光激发</strong>。对于一组barcode的每一个位置，如果A＋C的总数与G＋T的总数相接近，可以在一定程度上弥补碱基不平衡的负面作用。</p>
</blockquote>
<p>3、激光平衡是次优选择，不得已而为之。它虽然可以在一定程度上提高barcode测序质量，减少de-multiplexing出问题的可能性，但是并不是说，只要激光平衡了，测序数据的分离就一定不受影响。<br>4、如果barcode组合碱基也不平衡，激光也不平衡，则de-multiplexing风险非常高。</p>
<p><strong>二、Barcode组合举例</strong></p>
<ol>
<li>好的组合。</li>
</ol>
<p>Illumina推荐的12个样本barcode组合如下。 </p>
<table>
<thead>
<tr>
<th style="text-align:center">编号</th>
<th style="text-align:center">序列</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">01</td>
<td style="text-align:center">ATC ACG</td>
</tr>
<tr>
<td style="text-align:center">02</td>
<td style="text-align:center">CGA TGT</td>
</tr>
<tr>
<td style="text-align:center">03</td>
<td style="text-align:center">TTA GGC</td>
</tr>
<tr>
<td style="text-align:center">04</td>
<td style="text-align:center">TGA CCA</td>
</tr>
<tr>
<td style="text-align:center">05</td>
<td style="text-align:center">ACA GTG</td>
</tr>
<tr>
<td style="text-align:center">06</td>
<td style="text-align:center">GCC AAT</td>
</tr>
<tr>
<td style="text-align:center">07</td>
<td style="text-align:center">CAG ATC</td>
</tr>
<tr>
<td style="text-align:center">08</td>
<td style="text-align:center">ACT TGA</td>
</tr>
<tr>
<td style="text-align:center">09</td>
<td style="text-align:center">GAT CAG</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">TAG CTT</td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:center">GGC TAC</td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">CTT GTA</td>
</tr>
</tbody>
</table>
<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.Figure3.png" alt="Figure 3"></p>
<p>以第一个位置（纵列）为例，<strong>A:G:C:T=3:3:3:3=1:1:1:1</strong>。实际上，该barcode组合每个位置的碱基比例都接近<strong>1:1</strong>，碱基平衡度近乎完美。</p>
<ol>
<li>不好的组合</li>
</ol>
<p>下面的组合有缺陷。比如说，第1个位置只有A和C两种碱基，A、C都属于红激光，导致绿激光没有信号，碱基和激光都不平衡。</p>
<p>AGT TCC<br>ACT GAT<br>ACG AGC<br>ACT CCT<br>CAA AAG<br>CAA CCA<br>CAC CAG        </p>
<p><strong>三、Barcode碱基不平衡的后果</strong></p>
<ol>
<li>如果barcode组合的碱基组成不平衡，会导致测序进行到这些碱基时，软件对测序信号的处理出现障碍，不能准确地识别这些碱基(base-calling)，表现为QV值降低，%Q30曲线波动。   </li>
<li>在这种情况下，运用生物信息软件对测序数据进行数据分离（de-multiplexing）出现困难，部分数据不能准确分离，成为undetermined 数据的一部分，造成undetermined数据增多，可分离的数据减少。     </li>
<li>如果测序数据的总量很多，远大于全部样本数据量期望值的总和，则问题有可能不那么严重，全部或者大部分样本仍然可能分离到足够的数据量。    </li>
<li>万一样本性质特殊，反应效率低；或者混合样本之间竞争和抑制严重，导致测序数据总量在期望值附近，余量很少；或者其中个别样本数据量特别少，这时如果undetermined数据比例过高，就会导致部分或者全部样本的数据量不够用。   </li>
<li>混合样本补数据是一个非常麻烦的问题，成本极高。如果一组样本中只有个别样本需要补数据，由于文库是混合在一起的，其他样本也不得不跟着重测一次。这是困难之一。困难之二，如果数据缺口比较小，本来可以与其他样本混合，搭个便车，可是，进行第二次混合的时候，经常会遇到barcode冲突或者碱基不平衡，拼lane非常困难，往往要等很长时间，才有合适的机会。</li>
</ol>
<p><strong>四、实验证明de-multiplexing成功，该barcode组合今后是否一定好用</strong></p>
<ol>
<li>如果barcode组合碱基平衡，则无论样本怎么变，该组合一定好用。    </li>
<li>如果barcode组合的碱基组成不理想，即使以前的实验证明好用，不等于今后一定好用。下一次测序效果可能好，也可能不好。   </li>
<li>这是由于不同的项目样本不同，有可能导致两种后果：  </li>
</ol>
<blockquote>
<p>a.数据总量在期望值附近，余地不够多，de-multiplexing后部分样本数据量不够；<br>b.如果新的样本本身也碱基不平衡，read 1测序质量很差，会影响到barcode和read2的测序质量。当然，情况b责任不在barcode，即使barcode很好，数据还是不够。</p>
</blockquote>
<p><strong>五、补救措施</strong></p>
<p>如果满足以下两个条件：</p>
<p>a. 混合样本的数据总量足够，只是由于barcode质量不好，导致de-multiplexing后部分或全部样本数据量不够；<br>b. 排除QV值低的barcode碱基后，其余质量好的barcode碱基仍然足够用来区分全部样本；</p>
<p>那么，可以通过改变de-multiplexing算法来为每个样本获得尽量多的数据。比如去掉信号识别模糊的碱基，或者增加mismatch碱基的数目，重新运行de-multiplexing程序。</p>
<p><strong>六、样本少于4种，不可能碱基平衡，怎么办</strong></p>
<p>如果样本数少于4种，每一个位置的碱基最多只有3种，不可能碱基平衡，怎么办呢？这时一定要保证激光平衡。Illumina推荐了3种low-level pooling的barcode组合：</p>
<p>2个样本：</p>
<p>:—:|:—-:</p>
<p>#6|GCC AAT</p>
<p>#12|CTT GTA</p>
<p>3个样本：</p>
<p>:—-:|:—-:</p>
<p>#4|TGACCA</p>
<p>#6|GCCAAT</p>
<p>#12|CTTGTA</p>
<p>6个样本：</p>
<p>:—:|:—:</p>
<p>#2|CGATGT</p>
<p>#4|TGACCA</p>
<p>#5|ACAGTG</p>
<p>#6|GCCAAT</p>
<p>#7|CAGATC</p>
<p>#12|CTTGTA</p>
<p>这3种组合包含一个共同内核：6号和12号。6号和12号组合是百分百激光平衡的，每一个位置的碱基（纵列，即GC、CT、CT、AG、AT和TA）都分别属于不同的激光。只要barcode组合中包含6号和12号，就能满足最基本的要求，不至于颗粒无收。6号和12号是barcode组合的核心，不可或缺。</p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NGS </tag>
            
            <tag> 测序技术 </tag>
            
            <tag> 碱基平衡性 </tag>
            
            <tag> barcode </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何选择入门编程语言]]></title>
      <url>/2015/01/22/2015-01-22-which-programming-language-should-I-learn-first.html</url>
      <content type="html"><![CDATA[<p>这是一个很有意思的编程语言入门学习图谱，以不同人的目的为导向推荐不同程度的入门语言，还有一个最有意思的地方就是为每一个主流编程语言配上一个魔戒人物用以形象描述它的独特之处,非常有特点！点击图片看<a href="http://blog-fungenomics-com.qiniudn.com/st.post.2015-01-22.ProgramToStart.png" target="_blank" rel="external">大图</a>。<br><a href="http://blog-fungenomics-com.qiniudn.com/st.post.2015-01-22.ProgramToStart.png" target="_blank" rel="external"><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2015-01-22.ProgramToStart.png" alt="Learning language"></a></p>
]]></content>
      
        <categories>
            
            <category> 编程技术 </category>
            
            <category> 信息图 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 入门学习 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何布局并管理好项目目录（2/2）]]></title>
      <url>/2015/01/14/2015-01-14-how-to-manage-the-project-data-2.html</url>
      <content type="html"><![CDATA[<p>…<a href="">书接上回</a>…</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2015-01-13-Fig1-1.png" alt="Figure"></p>
<p>不同类型的项目会有适合其特点的目录布局，很难说会有一个真正的标准，我这里特指的是基因组（或相关的）项目目录布局问题。</p>
<p>基因组项目目录结构的科学管理是一个非常重要的问题。如果不科学，东西一多更是成了灾难，很多时候你想找的东西并不是丢了，而是根本找不到了！有时更糟的是就算找到了，也会忘了是怎么来的，最后连自己都会怀疑它是不是真的就是正确的那一份！只得一轮又一轮地冥思苦想寻找着若隐若现的线索，甚至有时即使打算重做都不知道该怎么下手，效率真是极低，简直就是在浪费生命！这样下去根本没得玩，直到昨天我才意识到实际上要用程序设计的思想来管理每个项目的目录！还是<a href="http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html" target="_blank" rel="external">A workflow for R</a>这篇博文给我的灵感，它提出了一些规则：</p>
<ul>
<li><strong>透明</strong>（Transparency）：目录布局清晰明朗，逻辑清楚，整个结构一看就能明白。    </li>
<li><strong>易维护</strong>（Maintainability）：容易实行项目的修改和相关（如文件名、目录变换）调整。           </li>
<li><strong>模块化</strong>（Modularity）：任务之间都应该尽可能保持其独立性，每一个就只干好自己的事，不干涉其他任务，避免牵一发而动全身的情况发生，要有一一对应的关系，这样即便是需要修改也将会非常方便。     </li>
<li><strong>可移植</strong>（Portability）：项目可以很容易地移植到其他的平台或者系统中。     </li>
<li><strong>易复现</strong>（Reproducibility）：不论经过多久，都要能轻易重现原来的结果。     </li>
<li><strong>秒懂</strong> (Efficiency)：无需多想就能明白项目执行过程中的相关细节，比如所要解决的每个问题是如何处理的和所用的工具是什么等等。</li>
</ul>
<p>这几点完全说出了项目布局和目录管理所应达到的目的和状态，值得铭记于心多拿来参考参考。只是仅有理论，还是太过不着边际了点，有必要弄个例子来看看具体应该怎么做。</p>
<p>有那么一段时间以来，我的数据目录布局是这样的：<br><img src="http://blog-fungenomics-com.qiniudn.com/32015-01-13-Fig2.png" alt="Figure1"><br>虽然可以保持每次被调用的文件名字都是“sample.mat”弊病其实也是明显的，时间一久，真的很难想得起到底是哪个对哪个，每次都需要重新回忆。<br>后来我看到了<a href="http://software-carpentry.org/v4/data/mgmt.html" target="_blank" rel="external">这篇文章</a>，顿时觉得，</p>
<p>所以分享出来，不过我不会严格地按原文去翻译。</p>
]]></content>
      
        <categories>
            
            <category> 规范和标准 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 项目管理 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[如何布局并管理好项目目录（1/2）]]></title>
      <url>/2015/01/13/2015-01-13-how-to-manage-the-project-data-1.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2015-01-13-Fig1-1.png" alt="Figure"></p>
<p>不同类型的项目会有适合其特点的目录布局，很难说会有一个真正的标准，我这里特指的是基因组（或相关的）项目目录布局问题。</p>
<p>基因组项目目录结构的科学管理是一个非常重要的问题。如果不科学，东西一多更是成了灾难，很多时候你想找的东西并不是丢了，而是根本找不到了！有时更糟的是就算找到了，也会忘了是怎么来的，最后连自己都会怀疑它是不是真的就是正确的那一份！只得一轮又一轮地冥思苦想寻找着若隐若现的线索，甚至有时即使打算重做都不知道该怎么下手，效率真是极低，简直就是在浪费生命！这样下去根本没得玩，直到昨天我才意识到实际上要用程序设计的思想来管理每个项目的目录！还是<a href="http://blog.revolutionanalytics.com/2010/10/a-workflow-for-r.html" target="_blank" rel="external">A workflow for R</a>这篇博文给我的灵感，它提出了一些规则：</p>
<ul>
<li><strong>透明</strong>（Transparency）：目录布局清晰明朗，逻辑清楚，整个结构一看就能明白。    </li>
<li><strong>易维护</strong>（Maintainability）：容易实行项目的修改和相关（如文件名、目录变换）调整。           </li>
<li><strong>模块化</strong>（Modularity）：任务之间都应该尽可能保持其独立性，每一个就只干好自己的事，不干涉其他任务，避免牵一发而动全身的情况发生，要有一一对应的关系，这样即便是需要修改也将会非常方便。     </li>
<li><strong>可移植</strong>（Portability）：项目可以很容易地移植到其他的平台或者系统中。     </li>
<li><strong>易复现</strong>（Reproducibility）：不论经过多久，都要能轻易重现原来的结果。     </li>
<li><strong>秒懂</strong> (Efficiency)：无需多想就能明白项目执行过程中的相关细节，比如所要解决的每个问题是如何处理的和所用的工具是什么等等。</li>
</ul>
<p>这几点完全说出了项目布局和目录管理所应达到的目的和状态，值得铭记于心多拿来参考参考。只是仅有理论，还是太过不着边际了点，有必要弄个例子来看看具体应该怎么做。</p>
<p>…<a href="">未完待续</a></p>
]]></content>
      
        <categories>
            
            <category> 规范和标准 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 翻译 </tag>
            
            <tag> 项目管理 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[替换google字体，加快网站访问速度]]></title>
      <url>/2015/01/07/2015-01-07-Replace-google-fonts.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2015-01-07-font-face.png" alt="font log"></p>
<p>一段时间以来，这个博客的打开速度慢得出奇！本来只有很少的东西，ping的速度也在120ms左右，算是可以的了，怎么会这样！一开始我没有搞明白问题的根源。今天才突然醒悟到一定是google字体加载的问题！当时这个博客的主题是从<a href="http://yihui.name/" target="_blank" rel="external">yihui</a>和<a href="http://webfrogs.me/" target="_blank" rel="external">Carl Chen</a>那里抄来的（不用试了，这俩的页面是极难打开的了），因为懒，因为不认识网页语言，尤其是<code>css</code>，所以所有<code>.css</code>后缀什么的，我一概不去看！所以那几个css就成了我的一个暗区，现在问题很明显了，要修改的地方一定在那！目标锁定之后，那么着手处理吧！</p>
<p>我页面的主要配置都在<code>style.css</code>和<code>home.css</code>，打开这两文件，搜索<code>google</code>，果然发现“fonts.googleapis.com”！那么剩下的就是要先找字体替换方案了，搜索一番后，发现原来360已经把整个的google字库下载下来放在了自己的服务器上了，这样就太好了！改起来也极其简单，只需要直接把所有的<code>fonts.googleapis.com</code>替换为<code>fonts.useso.com</code>,而且还不用做任何其他的修改，字体也还是原来的字体！</p>
<p>完成后，发现这下页面打开的速度和以前相比根本就是两个网站！不禁感叹：“靠！怎么这么简单！”</p>
<p><em>参考 :</em>   </p>
<blockquote>
<p><a href="http://www.cgtt.net/1323.html" target="_blank" rel="external">http://www.cgtt.net/1323.html</a></p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> google </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[修改二维码生成工具]]></title>
      <url>/2014/09/19/2014-09-19-Change-QRCode-Generator.html</url>
      <content type="html"><![CDATA[<h2 id=""><a href="#" class="headerlink" title=""></a><img src="http://blog-fungenomics-com.qiniudn.com/fg.post.qrcode.png" alt="QR"></h2><p>本来博客在改版后，短时间内，我是不想做修改了的，回国后，没办法，原来的用于生成二维码的代码必须得换！根本就是失效了。原因也是很明朗的，二维码生成代码用的API是google的，稳定，强大……但在墙面前显得跟土一样。有时候我总觉得，是不是因为有了墙，大家的技术反而都要强那么一点了——魔高一尺道高一丈！只是有些时候也不得不拜倒了。总之，为了速度，为了大家的良好体验，换了！google的跟踪代码，我也取消了，换成了百度，不然网页死活打不开的，这是题外话了。</p>
<p>总之货比三家，最后我做了个比较大众化的选择——联图网的服务接口！说是接口，听起来似乎很高大上的样子，其实也就是一小段代码罢了，无需紧张。下面就是主题了。</p>
<p>API接口调用代码如下：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- 网页自动生成二维码的代码 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span><span class="undefined">  </span></div><div class="line"><span class="undefined">  thisURL  = document.URL;  </span></div><div class="line"><span class="xml">  strwrite = "<span class="tag">&lt;<span class="name">p</span> <span class="attr">align</span>=<span class="string">'center'</span>&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'http://qr.liantu.com/api.php?w=120&amp;m=2&amp;text=" + thisURL + "'</span> <span class="attr">alt</span>=<span class="string">'QR Code'</span>/&gt;</span>（传送门）<span class="tag">&lt;/<span class="name">p</span>&gt;</span>"</span></div><div class="line"><span class="undefined">   document.write( strwrite ); </span></div><div class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div></pre></td></tr></table></figure>
<p>还和上次<a href="/2014/07/18/Change-blog-template.html">博客改版</a>中提到的一样，把上面这段代码完全替换掉原来<code>qrCodeGenerate.md</code>中的内容就ok了。</p>
<p>为了方便大家看的明白，这里需要对上面这段代码中的一些重要参数做些说明：</p>
<ol>
<li><code>http://qr.liantu.com/api.php?</code> 是联图网的QR code API接口地址；   </li>
<li>w：二维码图片宽度，根据需要自行修改，如<code>w=120</code>，表示生成一个120×120像素的正方形二维码图片；     </li>
<li>m：二维码静区（外边距），一般设为比较小的数，我这里设置为2；    </li>
<li>text：二维码数据信息， 我这里设置为对应页面的URL地址；     </li>
<li>alt：二维码图片描述， 这个就没什么讲究了。 </li>
</ol>
<p>总体的效果请看文章末尾的二维码图片。</p>
<p><em>参考资料：</em></p>
<blockquote>
<p><a href="http://uuxn.com/wordpress-articles-qr" target="_blank" rel="external">免插件自动生成wordpress文章二维码图片</a>  很感激这篇文章给我的启发。</p>
</blockquote>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 网站插件 </tag>
            
            <tag> google </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[浅述CNV——我怎么会是我]]></title>
      <url>/2014/08/16/2014-08-16-Slightly-Introduce-CNV.html</url>
      <content type="html"><![CDATA[<h3 id="什么是CNV"><a href="#什么是CNV" class="headerlink" title="什么是CNV"></a>什么是CNV</h3><hr>
<p>CNV的全称是Copy Number Variantion ，但这里特指“人类基因组拷贝数变异”。我们知道人类基因组是由60亿个化学碱基（核苷酸）所构成的。这60亿个化学碱基一共组合成了23对（46条）染色体，在每一对染色体中都有两条姐妹染色单体，它们分别遗传自父亲和母亲。在这些染色体中包含着大约30000个编码基因。鉴于人类基因组是一个二倍体，那一般来说，我们会自然而然地认为每一个基因都应该恰好有2份拷贝，即姐妹染色单体上各有一份。但，实际的情况却并非如此。近期的研究也表明，基因组上有些长度较大的DNA序列片段——长度通常在1000bp至1Mbp，存在着反常的拷贝次数。这样的一种现象就称之为拷贝数变异，也就是英文的CNV。这种拷贝数变异会导致受其影响的基因表达失调。举个例子来说，比如我们一直以为在每个人中这些基因都只有2份拷贝，但是现在却会在有些人中看到它们只有1份，或是3份，甚至更多。更为罕见的情况是在有些人中甚至存在着两条染色单体中同时丢失某个基因的情况（见下图）。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/fg.2014-08-16-Figure.png" alt="Human CNV"><br>大家从图中可以看到，大多数的基因确实只有2分拷贝，但也存在着反常的情形（红色高亮的部分）（译者：这个图只是示意而已，实际的情形可不一定是这样分布的）。</p>
<p><br></p>
<h3 id="CNV为什么那么重要"><a href="#CNV为什么那么重要" class="headerlink" title="CNV为什么那么重要"></a>CNV为什么那么重要</h3><hr>
<p>明确了CNV是什么之后，接下来的话题便是要明白CNV为什么重要了。应该说正是由于DNA序列上的差异使得我们每个人在这个世界上都是独一无二的。当然了，这些独一无二的特性还包括了对不同疾病易感程度的差异和不平等性。以前的看法认为单核苷酸多态性（SNP）是DNA遗传差异的最主要和最重重要的来源。然而，近期的研究成果中我们却看到，人类基因组CNV变异的序列，在长度上至少已经是单核苷酸变异（SNP）总长的3倍（译者：原文虽然这么说，但其实并不能因此就简单的以为CNV在遗传差异上的贡献会是SNP的3倍，这还和它所能表现出来的效应相联系）。并且考虑到发生CNV的序列常常围绕在基因的周围，这很可能暗示了它们在人类疾病和药物反应中扮演着一个相当重要的角色。另外，能弄明白人类基因组CNV的发生机制也将有助于我们更好的了解自身基因组的进化历程，以便搞清楚<strong>“我为何不同，我之所以是我”</strong>的深层原因。</p>
<p><a href="http://www.gene-quantification.de/cnv-faq.pdf" target="_blank" rel="external">这里是原文。</a></p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 变异检测 </tag>
            
            <tag> 翻译 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[多Github账号的使用问题]]></title>
      <url>/2014/08/10/2014-08-10-Multiple-Github-In-One-Machine.html</url>
      <content type="html"><![CDATA[<p>同一台电脑，或者同一个（大型机）服务器账号下要使用多个Github账号，该咋办？</p>
<p>如果是单用户(single-user)，很方便，默认拿id_rsa与你的github服务器的公钥对比；如果是多用户（multi-user）如user1,user2,那么就不能用在user2的身上了，这个时候就要配置一下了：</p>
<p><strong>1、新建user2的SSH Key</strong>  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#新建SSH key(若有需求，注意不要误删其他的id_rsa key)：</span></div><div class="line">$ <span class="built_in">cd</span> ~/.ssh <span class="comment"># 切换到~/.ssh</span></div><div class="line">ssh-keygen -t rsa -C mywork@email.com  <span class="comment"># 新建SSH key</span></div><div class="line"><span class="comment"># 设置名称为id_rsa_hshujia (设置别的名字，防止因名字冲突而误删其他的id_rsa)</span></div><div class="line">Enter filein <span class="built_in">which</span> to save the key (~/.ssh/id_rsa): id_rsa_hshujia</div></pre></td></tr></table></figure>
<p><strong>2、新密钥添加到SSH agent中</strong>  </p>
<p>因为默认只读取id_rsa，为了让SSH识别新的私钥，需将其添加到SSH agent中：</p>
<p>添加至SSH agent<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssh-add  ~/.ssh/id_rsa_hshujia</div></pre></td></tr></table></figure></p>
<p>如果出现Could not open a connection to your authentication agent的错误，就试着用以下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssh-agent bash &amp;&amp; ssh-add ~/.ssh/id_rsa_hshujia</div></pre></td></tr></table></figure></p>
<p><strong>3、修改config文件</strong></p>
<p>在~/.ssh目录下找到config文件，vi打开进行编辑，如果没有就创建：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 创建config</span></div><div class="line">touch config</div></pre></td></tr></table></figure></p>
<p>然后按如下形式修改：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 该文件用于配置私钥对应的服务器</span></div><div class="line"><span class="comment"># Default github user(first@mail.com)**</span></div><div class="line">Host github.com</div><div class="line">    HostName github.com</div><div class="line">    User <span class="string">"user1"</span>    </div><div class="line">    IdentityFile ~/.ssh/id_rsa </div><div class="line"></div><div class="line">    <span class="comment"># second user(second@mail.com)</span></div><div class="line">    <span class="comment"># 建一个github别名，新建的帐号使用这个别名做克隆和更新</span></div><div class="line">Host hshujia.github.com     </div><div class="line">    HostName github.com</div><div class="line">    User <span class="string">"user2"</span>    </div><div class="line">    IdentityFile ~/.ssh/id_rsa_hshujia</div></pre></td></tr></table></figure>
<p>其规则就是：从上至下读取config的内容，在每个Host下寻找对应的私钥。这里将GitHub SSH仓库地址中的<strong><em>git@github.com</em></strong>替换成新建的Host别名如：<strong><em>hshujia.github.com</em></strong>，那么原地址是：<strong>git@github.com</strong>:user/Mywork.git，替换后应该是：<strong>git@hshujia.github.com</strong>:user/Mywork.git.</p>
<p>以下是我在（大型机）服务器上config文件的具体内容：<br><img src="http://blog-fungenomics-com.qiniudn.com/fg.post.2014-08-30-Fig1.jpg-blog.fungenomics.com" alt="My config"></p>
<p><strong>4、打开新生成的~/.ssh/id_rsa_hshujia.pub文件，将里面的内容添加到GitHub后台，此处默认大家懂得如何添加，就不详述了。</strong></p>
<p>完成之后，在终端命令行中测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ssh -T git@hshujia.github.com  # 检验key是否已被成功添加到Github后台</div><div class="line">Hi hshujia! You&apos;ve successfully authenticated, but GitHub does not provide shell access. # 若能看到类似于这样的一句话就说明已经成功添加。</div></pre></td></tr></table></figure>
<p><strong>5、若以前是Global配置，则取消Global配置。</strong></p>
<p>因为<code>git pull</code> or <code>git push</code> 的时候识别的是邮箱，多个github账号，就有多个邮箱，我们自然不能使用global的user.email了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1.取消global </span></div><div class="line">git config --global --<span class="built_in">unset</span> user.name</div><div class="line">git config --global --<span class="built_in">unset</span> user.email</div><div class="line"></div><div class="line"><span class="comment"># 2.在各个对应repo的目录下，设置项目repo自己的user.email和user name</span></div><div class="line">git config  user.email <span class="string">"xxxx@xx.com"</span></div><div class="line">git config  user.name <span class="string">"xxx"</span></div></pre></td></tr></table></figure>
<p><strong>6、应用（例子）</strong></p>
<p>写个Hello world 测试一下。打开<github.com>登陆自己的账号，并新建一个repo，我这里命名为hello-world，创建完成之后，我在命令行中的具体操作如下:</github.com></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">git init</div><div class="line">git add README.md</div><div class="line">git config user.name hshujia</div><div class="line">git config user.email hshujia@qq.com</div><div class="line">git commit -m <span class="string">"first commit"</span></div><div class="line">git remote add origin git@hshujia.github.com:hshujia/hello-world.git <span class="comment"># 注意修改git@后面的名字 </span></div><div class="line">git push -u origin master</div></pre></td></tr></table></figure>
<p>这里唯一需要注意的就是多了两个<code>git config</code>用于告知该repo是属于哪一个user的；还有就是<code>git remote add origin</code>后面的host名字需要做相应的修改，具体的情况也已经在上面的代码中指明了。 </p>
<p><em>参考资料</em>  </p>
<blockquote>
<ol>
<li><a href="http://www.cnblogs.com/BeginMan/p/3548139.html" target="_blank" rel="external">git初体验（七）多账户的使用</a></li>
<li><a href="https://gist.github.com/suziewong/4378434" target="_blank" rel="external">Git的多账号如何处理</a>  </li>
</ol>
</blockquote>
]]></content>
      
        <categories>
            
            <category> 编程技术 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> github </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[人类不是从猴子进化而来]]></title>
      <url>/2014/07/19/2014-07-19-Human-did-not-evolved-from-monkey.html</url>
      <content type="html"><![CDATA[<p>问：如果人类是从猴子进化而来，那为什么猴子依然存在？</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2014-07-19-evolution_1.jpg" alt="Figure1"></p>
<p>答：我们不是从猴子进化而来的。我们（人和猴子）是从一个共同的祖先进化而来。</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2014-07-19-evolution_2.jpg" alt="正文"></p>
<p>原文出处：<a href="http://sufuns.com/wp-content/uploads/2012/06/infographic.png" target="_blank" rel="external">sufuns</a></p>
<p>译文出处：<a href="http://www.scipark.net/archives/18372" target="_blank" rel="external">科学公园–RhettZhang</a></p>
]]></content>
      
        <categories>
            
            <category> 物种进化与迁徙 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 自然选择 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[博客大改版：添加评论，二维码生成，数学公式的显示，添加分析代码等]]></title>
      <url>/2014/07/18/2014-07-18-Change-blog-template.html</url>
      <content type="html"><![CDATA[<h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><hr>
<p>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#introduce"><strong>概述</strong></a><br>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#1"><strong>评论系统由Disqus改成了多说</strong></a><br>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#2"><strong>添加google analytics</strong></a><br>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#3"><strong>如何搞定数学公式显示问题</strong></a><br>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#4"><strong>如何让每个页面自动生成二维码</strong></a><br>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#5"><strong>如何添加“返回顶部”按钮“</strong></a><br>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#6"><strong>其他</strong></a><br>&nbsp;&nbsp;&nbsp;&nbsp;<a href="#7"><strong>最后</strong></a>   </p>
<p><a id="introduce" name="introduce"> </a></p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><hr>
<p>前几天把博客改版了，在<a href="https://github.com/jekyll/jekyll/wiki/Sites" target="_blank" rel="external">Jekyll wiki</a>上爬主题来回爬了好几遍，先是爬了<a href="http://lhzhang.com/" target="_blank" rel="external">Linghua Zhang</a>,而后又直接爬了<a href="http://yihui.name/" target="_blank" rel="external">yihui</a>和<a href="http://webfrogs.me/" target="_blank" rel="external">Carl Chen</a>，不过做我这个版面主题的原始作者是<a href="http://yihui.name/" target="_blank" rel="external">yihui</a>，随便一提这位是牛人，<a href="http://cos.name/" target="_blank" rel="external">统计之都</a>和<a href="http://cos.name/cn/" target="_blank" rel="external">COS</a>论坛，以及中国R会议（第一届开始）都是他弄的，年纪轻轻且最近已经出了两本和R相关的书在亚马逊上卖了，恐怕国内（+很多国外）用过R的基本都知道！感兴趣的可以去他的<a href="http://yihui.name/" target="_blank" rel="external">主页</a>扒扒。。。扯远了，说回我自己，虽然我也想改得更加不同一些，但一方面暂时还没啥时间，其次，此前未碰过任何与网页制作相关之事，HTML勉强能看，css和js就停留在听说过这两词的地步；然，最重要的是，我也看上了这个版面！所以门面修改一事先缓一缓吧，先在这里谢过各位作者大人！但是话也说回来，以上只要有任意一位作者介意，我也只能作罢，重新扒过。</p>
<p>OK，既然现在外在的部分还无力去动，那接下来，我说一下自己所做的一些内在改变。</p>
<p><a id="1" name="1"> </a></p>
<h3 id="1-评论系统由Disqus改成了多说"><a href="#1-评论系统由Disqus改成了多说" class="headerlink" title="1. 评论系统由Disqus改成了多说"></a>1. 评论系统由Disqus改成了多说</h3><hr>
<p><img src="http://www.lagou.com/upload/webproduct/ff80808142c5ed7f0142c6bb08cc12fe.png" alt="多说"></p>
<p>我倒不是排斥Disqus，一开始我用的就是它，Disqus，国际化，版面简洁，管理容易，主页也生动，cool，我很喜欢！一个账号说遍天下，当然这一点上多说也一样。换掉它根本的原因还是在于伟大的‘墙’，Disqus只具有分享到Facebook和Twitter的功能，而这两货正常途径咱是上不了的。也罢，在国内的话多说用起来确实要更友好些，所以这次改版就重新选择了和国内社交网络联系在一起的多说。</p>
<p>这里我说一下自己是如何添加的。</p>
<p>（1）如果还没有绑定多说，那第一步需要做的就是到<a href="http://duoshuo.com/" target="_blank" rel="external">多说</a>上绑定。多说是不需要做任何注册的，只要有QQ，微博，百度等账号就行，这一点相当方便，不然我又得增加一个账号，我都已经记不得自己在网络上到底注册过多少个账号了，恐怕各型各色几百个都有了！ </p>
<p>（2）尔后，登陆多说主页点击“我要安装”，按提示走就行了。在<code>站点地址</code>这一栏填入自己的域名，再填上其他信息，按”创建”</p>
<p>（3）创建了之后，在侧边框找到选择<code>工具</code>，选择<code>通用代码</code>，会看到多说提供了一段相关的代码，我们要做的事情很简单，直接把它复制并贴到你自己网页的代码中，然后，就没有然后了。。。诶诶，慢着呀！那应该贴进哪一个的网页里头啊？我这简单说明一下，比如，<code>_layout</code>（生成网站页面的相关网页绝大数都放这，jekyll会自己来加载）目录下我们一般都会创建<code>defualt.html</code>, <code>page.html</code>, <code>post.html</code>等这么几个文件。我假设<code>post.html</code>就是你用于博客的，而你也只想博客的页面才加多说评论框，那么代码就应该贴进<code>post.html</code>。虽然你可以放在<body>与</body>之间的任何位置，但考虑到页面主要内容的加载速度，最好还是把它靠后放，评论模块要那么急着加载干嘛，最好就只是在之前，下面我给了个示意。</p>
<p>添加多说的代码结构。</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">title</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line">...</div><div class="line"><span class="tag">&lt;<span class="name">!—多说代码--</span>&gt;</span></div><div class="line">&#123; % include duoshuo.md % &#125; <span class="comment">&lt;!-- 实际使用中 “&#123;” 和 “%”，以及“%”和”&#125;“ 之间的空格需要去掉--&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></div></pre></td></tr></table></figure>
<p>如果除了post.html，你还打算将它添加到其他的页面中，也没问题，操作和上面类似。但整块整块的代码复制来复制去的，总觉得很麻烦，一旦想修改还得一个个去改，太吃力了！！我自己更喜欢的一种方式就是直接把多说的代码写到一个单独的文件中，比如就叫<code>duoshuo.md</code>，并放在<code>_include</code>目录下（这个目录按照jekyll的建议，就是让我们用来存放模块插件的），然后在需要多说的页面中直接调用<code>{ % include duoshuo.md % }</code>就行(本文为了显示的原因在<code>{</code>和<code>%</code>以及<code>%</code>和<code>}</code>之间加了一个空格，正式使用的时候注意去掉，下同)，这就犹如一个函数一般，修改也只需要改duoshuo.md这个文件，相当方便。</p>
<p>在设置<code>&lt;div class=&quot;ds-thread&quot; data-thread-key=&quot;请将此处替换成文章在你的站点中的ID&quot; data-title=&quot;请替换成文章的标题&quot; data-url=&quot;请替换成文章的网址&quot;&gt;&lt;/div&gt;</code>，这几个值的时候，我参考了这篇<a href="http://www.leejianyang.com/2014/05/25/duoshuo_tutorial/" target="_blank" rel="external">博文</a>，它讲的很清楚，不过里面写成data-thread-key=”&lt;%= page.path %&gt;”，data-title=”&lt;%= page.title %&gt;”和data-url=”&lt;%= page.permalink %&gt;”的形式在我这并不能成功，不知道是否与我用markdown格式编辑有关。所以后来我改成<code>&lt;div class=&quot;ds-thread&quot; data-thread-key=&quot;{ { page.path } }&quot; data-title=&quot;{ { page.title } }&quot; data-url=&quot;{ { page.permalink } }&quot;&gt;&lt;/div&gt;</code>就成功了（本文为了显示的原因在<code>{</code>和<code>{</code>以及<code>}</code>和<code>}</code>之间加了一个空格，正式使用的时候注意去掉，下同）。</p>
<p><a id="2" name="2"> </a>  </p>
<h3 id="2-添加google-analytics"><a href="#2-添加google-analytics" class="headerlink" title="2. 添加google analytics"></a>2. 添加google analytics</h3><hr>
<p><img src="http://www.proyectosbds.com/blog/wp-content/uploads/2012/11/google-analytics-impementation.jpg" alt="GA"><br>对于自己的网站，一个关心的事情就是网站的浏览量如何？是通过什么途径以什么方式流入流出的？别的先不多说，光是能时刻知道自己的网站在哪里、被用什么操作系统、多少人浏览、网页之间的流入流出是怎么样的等状态信息的本身就是一个很cool的事情，这跟玩游戏一样，相信没多少人会愿意把自己角色的血条和蓝条隐藏掉，然后在那瞎玩。所以我就开始琢磨着应该怎么搞定这样一个事情。后来知道了可以用<a href="http://www.google.com/analytics/" target="_blank" rel="external">google-analytics</a>帮助记录和分析，同样需要登陆google账号，填入网站域名，获得对应该域名的google analytics网络跟踪代码，然后又是复制粘贴。改过一次评论系统之后，我就发现，这东西都是大同小异的，就那么几下板斧。当然了我还是把它单独写成一个模块放在<code>_include</code>下供调用。代码如下：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- Google Analysis For website tracking --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="undefined"></span></div><div class="line"><span class="undefined">  (function(i,s,o,g,r,a,m)&#123;i['GoogleAnalyticsObject']=r;i[r]=i[r]||function()&#123;</span></div><div class="line"><span class="undefined">  (i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o),</span></div><div class="line"><span class="undefined">  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)</span></div><div class="line"><span class="undefined">  &#125;)(window,document,'script','//www.google-analytics.com/analytics.js','ga');</span></div><div class="line"><span class="undefined"></span></div><div class="line"><span class="undefined">  ga('create', 'UA-52659904-2', 'auto');</span></div><div class="line"><span class="undefined">  ga('require', 'displayfeatures'); </span></div><div class="line"><span class="undefined">  ga('send', 'pageview');</span></div><div class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span> </div><div class="line"><span class="comment">&lt;!-- Google Analysis end --&gt;</span></div></pre></td></tr></table></figure>
<p>这一次我希望全站跟踪，<code>default.html</code>是我所有页面都会添加的网页，所以这个代码就放在<code>default.html</code>中了。关于GA代码应该放在文件中的哪个位置比较适合，我还是做了一下考虑的，参考了<a href="http://bluewhale.cc/2010-07-19/google-analytics-add-tracking-code.html" target="_blank" rel="external">这篇文章</a>,按照异步跟踪的方式添加：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">title</span>&gt;</span></div><div class="line">...</div><div class="line"><span class="tag">&lt;<span class="name">!—GA异步追踪代码--</span>&gt;</span></div><div class="line">&#123; % include googleAnalysis.md % &#125; <span class="comment">&lt;!-- 实际使用中 “&#123;” 和 “%”，以及“%”和”&#125;“ 之间的空格需要去掉--&gt;</span></div><div class="line">...</div><div class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></div></pre></td></tr></table></figure>
<p><strong>注意:</strong> <em>在国内使用google analytics 并不是一个明智的做法，它已被封，这会大大降低你的网页访问速度，体验极差，建议还是使用百度分析.</em></p>
<p><a id="3" name="3"> </a></p>
<h3 id="3-如何搞定数学公式显示问题"><a href="#3-如何搞定数学公式显示问题" class="headerlink" title="3. 如何搞定数学公式显示问题"></a>3. 如何搞定数学公式显示问题</h3><hr>
<p><img src="http://media.journals.elsevier.com/content/images/main/elsevier-continues-as-mathjax-12092220.jpg" alt="mathJax"></p>
<p>makrdown是个好东西，jekyll+markdown做网站很容易！但麻烦的是，markdown不能支持LaTeX，我写个数学公式在那上面，它解析不了，只是原封不动的显示在那。。这一点真让人捉急！咋办？又是一番的google，后来还是找到了个好办法——<a href="http://www.mathjax.org/" target="_blank" rel="external">MathJax</a>，它是一个数学公式显示引擎，能够把LaTeX编辑的公式在网页上显示出来，不过它是在线解析的，公式如果比较多的话，显示速度会稍慢。要用MathJax，需要先把<code>_config.yml</code>中的makrdown解析器换成<code>kramdown</code>，不然用不了。我同样也是写成模块<code>mathJax.md</code>，然后由post.html调用(估计我也不会在除了博客之外的地方用到它)，为了加载速度，也放的靠后一些，刚好在多说模块上面，下面是我用的mathJax.md模块的代码，当然了，若有需要你也可以使用：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/x-mathjax-config"</span>&gt;</span><span class="undefined"></span></div><div class="line"><span class="undefined">  MathJax.Hub.Config(&#123; tex2jax: &#123;inlineMath: [['$','$'], ['\\(','\\)']]&#125; &#125;);</span></div><div class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span></span></div><div class="line"><span class="tag">  <span class="attr">src</span>=<span class="string">"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"</span>&gt;</span><span class="undefined"></span></div><div class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div></pre></td></tr></table></figure>
<p>成功添加了之后只需在公式前后用\$\$围起来，就可方便的编写公式了。如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">\rho_i=\sum_j\chi(d_&#123;ij&#125;-d_c)</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>显示的结果就是：<br>$$<br>\rho_i=\sum<em>j\chi(d</em>{ij}-d_c)<br>$$</p>
<p><a id="4" name="4"> </a></p>
<h3 id="4-如何让每个页面自动生成二维码"><a href="#4-如何让每个页面自动生成二维码" class="headerlink" title="4. 如何让每个页面自动生成二维码"></a>4. 如何让每个页面自动生成二维码</h3><hr>
<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2014-07-18-Figure4-QRcode.jpg" alt="QR"></p>
<p>二维码的作用其实也不必做多解释，最重要的就是方便。我之前并不知道可以用Google API为网站自动生成二维码一事，直到我成功了之后才恍然一悟，真是隔行如隔山。一开始我都是暴力解决：找个能生成二维码的网站，把自己每一篇博客的网址贴进去，点击生成二维码，然后再把这个生成的二维码图片下载下来，接着将它上传至图床，最后再将图片的链接添加到每篇博客的最后！这实在是。。。这个过程就算只是这样说起来都觉得十分费事，可以想象操作起来该有多费劲，而且一旦网站发生调整，就得重来一遍，实在低效！所以我一直寻思着该如何做才能让网页自己去产生二维码，一定要将它自动化！后来注意到了这个<a href="https://www.the-qrcode-generator.com/" target="_blank" rel="external">QR生成器</a>，并且受到它<code>share</code>按钮中内容的启发，我当时就在想为什么它这样的一段代码（如下）在插入到网页中之后就能出来一个二维码呢？（当然这个二维码是指向它自己网站的）</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!--QR生成器的share代码--&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">"https://www.the-qrcode-generator.com/"</span>&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">"http://chart.apis.google.com/chart?chs=200x200&amp;amp;cht=qr&amp;amp;chld=|1&amp;amp;chl="</span> <span class="attr">alt</span>=<span class="string">"QR Code"</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></div></pre></td></tr></table></figure>
<p>在仔细观察后发现代码中url地址中后面的<code>chs=200x200&amp;amp;cht=qr&amp;amp;chld=|1&amp;amp;chl=</code>看起来很像某些参数，貌似是可以自己设置的，若真如此的话，又该怎么做呢？这次得感谢<a href="http://www.divcss5.com/template/m504.shtml" target="_blank" rel="external">DIVCSS5</a>这篇博文中的那一段代码结构，它完全让我明白了自己该如何改装上面的QR生成器代码让它能为我所用，给每一个网页都自动根据自己的网址产生正确的二维码，改了的代码如下：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">&lt;!-- 网页自动生成二维码的代码 --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span>&gt;</span><span class="undefined">  </span></div><div class="line"><span class="undefined">  thisURL  = document.URL;  </span></div><div class="line"><span class="xml">  strwrite = "<span class="tag">&lt;<span class="name">p</span> <span class="attr">align</span>=<span class="string">'center'</span>&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">'http://chart.apis.google.com/chart?chs=120x120&amp;amp;cht=qr&amp;amp;chld=|1&amp;amp;chl=" + thisURL + "'</span> <span class="attr">width</span>=<span class="string">'120'</span> <span class="attr">height</span>=<span class="string">'120'</span> <span class="attr">alt</span>=<span class="string">'QR Code'</span>/&gt;</span>（传送门）<span class="tag">&lt;/<span class="name">p</span>&gt;</span>";</span></div><div class="line"><span class="undefined">  document.write( strwrite ); </span></div><div class="line"><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></div></pre></td></tr></table></figure>
<p>这一段代码本身是独立的，不依赖于任何特定的网站拥有者，而且光看代码我们也能想象得到，这里头调用的就是Google API，所以如果有需要，也欢迎大家直接复制粘贴到自己的页面代码中。当然了，我还是把它独立写在了一个文件——<code>qrCodeGenerate.md</code>中并放在了<code>_include</code>目录下，然后再调用，就跟上面添加多说和google跟踪代码一个思路，不过这个倒没什么位置限制，就看各自喜欢放哪就放哪，只要是在<body>与</body>之间就行，但我还是建议放在博文内容加载模块之后，这样可以确保博文内容先加载。</p>
<p><strong>注意:</strong> <em>后来在国内发现上面这个基于google的二维码，不但没用还大大影响了页面的加载速度！原因大家也都知道，具体的修改请参看这篇<a href="/2014/09/19/Change-QRCode-Generator.html">修改二维码生成工具</a>博文。</em></p>
<p><a id="5" name="5"> </a></p>
<h3 id="5-如何添加“返回顶部”按钮"><a href="#5-如何添加“返回顶部”按钮" class="headerlink" title="5. 如何添加“返回顶部”按钮"></a>5. 如何添加“返回顶部”按钮</h3><hr>
<p><img src="http://85ryan.com/wp-content/uploads/2013/09/simple-jquery-back-to-top-button.jpg" alt="Backtop"></p>
<p>“返回顶部”按钮（本博客右下角）的添加完全参照<a href="http://liberize.me/tech/jekyll-add-back-to-top-button.html" target="_blank" rel="external">这个教程</a>，作者写的相当详细！按照文章的信息我把’二’中的代码写入backtop.js，’三’中的信息写入backtop.css。然后将它们放在defualt.html中的<head>与</head>之间调用，示意如下：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></div><div class="line">...</div><div class="line"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">href</span>=<span class="string">"/media/css/backtop.css"</span>&gt;</span>  <span class="comment">&lt;!-- Back Top --&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">type</span>=<span class="string">"text/javascript"</span> <span class="attr">src</span>=<span class="string">"/media/js/backtop.js"</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span>  <span class="comment">&lt;!-- Back Top --&gt;</span></div><div class="line">...</div><div class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></div><div class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></div></pre></td></tr></table></figure>
<p><a id="6" name="6"> </a></p>
<h3 id="6-其他"><a href="#6-其他" class="headerlink" title="6. 其他"></a>6. 其他</h3><hr>
<p>（1）关于主页上的“<a href="/forum">FORUM</a>”，本来有打算做成类似于论坛形式可以让大家就某个话题来进行讨论，但是发现要实现这个功能难度很大工作量不小！限于我自己能力有限，这个也得慢慢来，暂时将就变成了贴“鸡汤文”的板子了，o(╯□╰)o！</p>
<p>（2）google网站站长工具，上传了一份google提供的验证网页（它仅是验证之用），成功之后虽然可以将它删掉，但google官网的意思是建议留下。</p>
<p>（3）图床的选择。图床对于独立博客来说是一个重要又头疼的事情，目前我直接用<a href="http://www.diandian.com/" target="_blank" rel="external">点点博客</a>的图片功能，使用外链。它的好处是免费，稳定，访问速度也快，要是真有一天点点不允许这样用的话，那我也不怕，实在不行就用七牛(<strong>注: 已改用七牛</strong> )，或者想办法用Github作为图床。</p>
<p><a id="7" name="7"> </a></p>
<h3 id="7-最后"><a href="#7-最后" class="headerlink" title="7. 最后"></a>7. 最后</h3><hr>
<p>OK！唠叨了不少总算把这篇文章写完了。接下来与博客搭建相关的内容先暂放一边了，我要回归主业的更新了。</p>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 网站插件 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[博客搬家]]></title>
      <url>/2014/07/09/2014-07-09-Blog-Migration.html</url>
      <content type="html"><![CDATA[<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2014-07-09-github_logo.png" alt="github logo"></p>
<p>以前在<a href="http://www.cnblogs.com/huangshujia/" target="_blank" rel="external">cnblog</a>上还遗留了几篇文章。当时还准备就在那上面安家的呢。不过，发了几篇博文之后，大伙多评论说，“哥们是不是发错地方了”之类的云云，还有一次惊讶了我一下的是，博文直接被管理员丢出了cnblog的首页，o(╯□╰)o。也罢了，毕竟博客园面向的是IT开发人员，发BT(Biology Technical)相关的东西也确实不好意思，既然门不当户不对，那就另觅出路呗。后来搞了搞，要整个独立博客还真有点费事，主机要花钱，IP要花钱。。。那会我还没听说过Jekyll，再加上自己平时比较忙也没什么时间，所以就一直搁着了，直到最近用了github，才在无意间发现了Jekyll+github的原来可以方便的搭建起独立博客来！这个中的具体情况我在<a href="http://stbioinf.com/2014/07/05/Build-MyOwn-blog-with-jekyll-and-githubpage.html" target="_blank" rel="external">上次的博文</a>中提到了。</p>
<p>虽然碰到了不少困难，但也还是七手八手把个人网站搭起来了。现在算是有了块自己的地，接着自然也就是要配置一些“行当”了，一些旧的“家当”也是不能扔的。所以就趁热打铁，把以前发在cnblog上的<a href="http://www.cnblogs.com/huangshujia/" target="_blank" rel="external">几篇文章</a>也一起给搬过来了。这搬的过程倒也不难（但比较遗憾的事评论貌似不好搬过来），这要感谢这位<a href="https://github.com/RichardUSTC/cnblogs-extractor" target="_blank" rel="external">RichardUSTC</a>写的一个格式转换程序，虽然如<a href="http://richardustc.github.io/blog/2013/05/migration/" target="_blank" rel="external">ta自己所言</a>，格式转换有些瑕疵，但我觉得已经很好了！也就只是瑕疵，自己再稍微调整一下也就OK了！关键的目的还是在于能节省下不少时间，就这样顺利搬家了！多谢多谢！不过话也说回来，转换过程中有些图表和段落的问题还没解决好，在<a href="http://stbioinf.com/2013/08/02/An-Introduction-of-NGS-Sequence.html" target="_blank" rel="external">三代基因组测序技术原理简介</a>这篇博文中尤为明显！算了等后面有时间再一一来解决吧！</p>
<p>补充一下，以前我在cnblog上的网名是T&amp;S，后来想想还是换成YellowTree好了。</p>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
    </entry>
    
    <entry>
      <title><![CDATA[Science上发表的一个超赞聚类算法]]></title>
      <url>/2014/07/06/2014-07-06-Science-Public-Cluster.html</url>
      <content type="html"><![CDATA[<p>作者(Alex Rodriguez, Alessandro Laio)提出了一种很简洁优美的聚类算法, 可以识别各种形状的类簇, 并且其超参数很容易确定.</p>
<p>##算法思想</p>
<p>该算法的假设是, 类簇的中心由一些局部密度比较低的点围绕, 并且这些点距离其他高局部密度的点的距离都比较大. 首先定义两个值: 局部密度$$\rho_i$$以及到高局部密度点的距离$$\delta_i$$:</p>
<p>$$<br>\rho_i=\sum<em>j\chi(d</em>{ij}-d_c)<br>$$</p>
<p>其中</p>
<p>$$<br>\chi(x)= \begin{cases}1 &amp; if x&lt;0\ 0&amp; otherwise \end{cases}<br>$$</p>
<p>$$d_c$$是一个截断距离，是一个超参数。所以$$\rho_i$$相当于距离点$$i$$的距离小于$$d_c$$的点的个数。由于该算法只对$$\rho_i$$的相对值敏感，所以对$$d_c$$的选择比较鲁棒，一种推荐的做法是选择$$d_c$$使得平均每个点的邻居数为所有点的1%-2%。</p>
<p>$$<br>\delta<em>i=\min</em>{j:\rho_j&gt;\rho<em>i}(d</em>{ij})<br>$$</p>
<p>对于密度最大的点，设置$$\delta_i=max<em>j(d</em>{ij})$$。注意只有那些密度是局部或者全局最大的点才会有远大于正常的相邻点间距。</p>
<p>##聚类过程</p>
<p>那些有着比较大的局部密度$$\rho_i$$和很大的$$\delta_i$$的点被认为是类簇的中心. 局部密度较小但是$$\delta_i$$较大的点是异常点.在确定了类簇中心之后, 所有其他点属于距离其最近的类簇中心所代表的类簇. 图例如下:<br><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2014-07-06-figure1.png" alt="Figure1"></p>
<p>左图是所有点在二维空间的分布, 右图是以$$\rho$$为横坐标, 以$$\delta$$为纵坐标, 这种图称作决策图(decision tree). 可以看到, 1和10两个点的$$\rho_i$$和$$\delta_i$$都比较大, 作为类簇的中心点。 26, 27, 28三个点的$$\delta_i$$也比较大, 但是$$\rho_i$$较小, 所以是异常点。</p>
<p>##聚类分析</p>
<p>在聚类分析中, 通常需要确定每个点划分给某个类簇的可靠性. 在该算法中, 可以首先为每个类簇定义一个边界区域(border region), 亦即划分给该类簇但是距离其他类簇的点的距离小于$$d_c$$的点。 然后为每个类簇找到其边界区域的局部密度最大的点, 令其局部密度为$$\rho_h$$该类簇中所有局部密度大于$$\rho_h$$的点被认为是类簇核心的一部分(亦即将该点划分给该类簇的可靠性很大), 其余的点被认为是该类簇的光晕(halo), 亦即可以认为是噪音. 图例如下:</p>
<p><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2014-07-06-figure2.png" alt="Figure2"></p>
<p>A图为生成数据的概率分布, B, C二图为分别从该分布中生成了4000, 1000个点. D, E分别是B, C两组数据的决策图(decision tree), 可以看到两组数据都只有五个点有比较大的$$\rho_i$$和很大的$$\delta_i$$。 这些点作为类簇的中心, 在确定了类簇的中心之后, 每个点被划分到各个类簇(彩色点), 或者是划分到类簇光晕(黑色点)。F图展示的是随着抽样点数量的增多, 聚类的错误率在逐渐下降, 说明该算法是鲁棒的。</p>
<p>最后展示一下该算法在各种数据分布上的聚类效果，非常漂亮。<br><img src="http://blog-fungenomics-com.qiniudn.com/st.post.2014-07-06-figure3.png" alt="Figure3"></p>
<p>参考文献:</p>
<blockquote>
<p>[1]. <a href="http://www.sciencemag.org/content/344/6191/1492.abstract" target="_blank" rel="external">Clustering by fast search and find of density peak. Alex Rodriguez, Alessandro Laio</a></p>
</blockquote>
<p>本文转载自<a href="http://www.kemaswill.com/machine-learning/science%E5%8F%91%E8%A1%A8%E7%9A%84%E8%B6%85%E8%B5%9E%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/" target="_blank" rel="external">Kemaswill’s Blog</a></p>
]]></content>
      
        <categories>
            
            <category> Machine Learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> cluster </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Github+Jekyll建站]]></title>
      <url>/2014/07/05/2014-07-05-Build-MyOwn-blog-with-jekyll-and-githubpage.html</url>
      <content type="html"><![CDATA[<p><img src="http://m3.img.srcdd.com/farm5/d/2014/0712/06/7EF2461D96BD30ABB890C3846C08DCAE_LARGE_1900_800.png" alt="Figure1"></p>
<p>这该如何说起。。。 </p>
<p>关于搭建自己的网站，其实早有此心，只是。。。。。。<br>靠，本想长篇大论一番，讲讲自己是如何搭建起这个网站的，讲讲自己是如何在对Jekyll，markdown，GithubPage，DNS解析等一无所知的情形下，怎样现学现卖，等等之类的，最后想想还是算了，网上的这些教程太多了，况且我也是参照着别人的经验做出来的，还是不装了。呵呵。</p>
<p>这里就只列一下之前看到的教程，权当作记录吧。</p>
<p>我自己其实是纯属意外才会真的动起来去用Jekyll+Github来建博客的，Jekyll和Github以前是有所听闻的，但一直不知道它们竟还能有这层关系！嗨，其实说来也是因为不知道，Github也还用了不到一个月的时间，要不是因为有一次很偶然的看到了代码仓库的<code>setting</code>之下有个<code>Automatic page generator</code>的按钮，一时好奇心起，想知道这玩意是个啥，于是一查，哟！原来如此！就这样我知道了<a href="https://pages.github.com/" target="_blank" rel="external">GitHub Pages</a>的神奇作用，发现了它完全符合我自己想要建个人网站的需求！除了能映射自己的域名之外，更重要的还是免费+无限空间，对于是静态还是动态网站于我来讲并不是很关心，我关心的是，我要能完全的把控它！接着也就自然而然的认识了<a href="http://jekyllrb.com/" target="_blank" rel="external">Jekyll</a>,markdown我在一开始使用Github的时候就知道了，所以这个没啥槛。</p>
<p>那么说干就干了，方向有了，怎么开始好呢！习惯性Google了一下，发现了这个<a href="http://beiyuu.com/github-pages/" target="_blank" rel="external">使用Github Pages建独立博客</a>,作者写的很好，也是这篇博文领我入门的。包括了如何设置DNSpod解析域名，添加CNAME，怎么添加A记录等等这些。再次千恩万谢！不过我重设DNSpod进行域名解析的时候，基本是瞬间生效，不需要如作者所言要等一天那么久，但不知这情况是否普遍。接下来又陆陆续续看了很多有关Jekyll+github的文章，基本上大同小异，也算不断填补自己的空白吧。</p>
<p>要特别提到的是今天看到的一篇迟来的博客<a href="http://www.pchou.info/web-build/2014/07/04/build-github-blog-page-08.html" target="_blank" rel="external">一步步在GitHub上创建博客主页</a>,讲的也是相当的清楚，而且作者写的一整个系列的<a href="http://www.pchou.info/web-build/2013/01/03/build-github-blog-page-01.html" target="_blank" rel="external">一步步教程</a>，对于新手而言非常具有参考价值，还对域名以及与其相关的一些概念，如A记录，CNAME，TTL之类的做了详细的解析，真心推荐新手读之，此外也看到作者的博客做的很漂亮。</p>
<p>其他的一些则如<a href="http://yanping.me/cn/blog/2011/12/15/building-static-sites-with-jekyll/" target="_blank" rel="external">一步步构建Jekyll网站</a>,<a href="http://calefy.org/2012/03/03/my-process-of-building-jekyll-blog.html" target="_blank" rel="external">Jekyll建站之旅</a>等等之类的，也都不错。就不一一列出了！</p>
<p>OK，暂时先这样了！</p>
]]></content>
      
        <categories>
            
            <category> Blog </category>
            
        </categories>
        
        
        <tags>
            
            <tag> github </tag>
            
            <tag> jekyll </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[基因组变异检测概述]]></title>
      <url>/2013/09/28/2013-09-28-An-Introduction-of-genome-variant-detect.html</url>
      <content type="html"><![CDATA[<p>首先，在开始之前我觉得有必要稍微科普缓冲一下，以便不使得不熟悉生物信息或基因组的客官们疑惑。</p><br><p>1.基因组：每个人都有一个基因组，这里的&ldquo;<a href="http://zh.wikipedia.org/wiki/%E5%9F%BA%E5%9B%A0%E7%B5%84" target="_blank">基因组</a>&rdquo;并不只是&ldquo;<a href="http://zh.wikipedia.org/wiki/%E5%9F%BA%E5%9B%A0" target="_blank">基因</a>&rdquo;的集合，基因是控制性状的遗传单元（什么是性状呢？性状也可以狭义的理解为个体的各种外在和内在特征，比如头发和眼睛颜色，高矮胖瘦，抵抗力强等），但是基因组所指的其实是我们的所有遗传信息，而不单单只是一些外在和内在特征，也包含很多目前而言不明其功能性（或者被认为无功能）的DNA序列。 其实说白了就是整一个的DNA序列！因而，基因也只是基因组的一个子集。此外，需要特别指出的是，我们虽都为&ldquo;人&rdquo;，但人与人之间的基因组是不一样的（即是多态的），彼此之间都存在着一些差异，即使是和父母或是兄弟姐妹之间去比较。这些差异也是造成我们彼此之间为何如此这般不同的一个重要原因。而这些差异也是基因组多态性的来源。</p><br><p>2.Reads：这里的reads是一个在基因组测序中使用的名词（对测序原理感兴趣的客官请猛戳：<a href="http://stbioinf.com/2013/08/02/An-Introduction-of-NGS-Sequence.html" target="_blank">三代基因组测序技术原理简介</a>），指的就是一段特定长度的DNA片段，这个长度取决于测序仪的读长。</p><br><p>3. 变异是一个相对的概念，只有在彼此的比较中才有存在的意义。目前关于人类基因组变异的讨论，都是以&ldquo;人类基因组计划&rdquo;中所组装出来的人类基因组作为参照物。以下谈到的涉及比对过程所用的基因组指的就是这个人类参考基因组。</p><br><p>4. 以下常出现&ldquo;序列&rdquo;，指的都是DNA序列片段。</p><br><p>OK！简单的科普就此完毕，剩余的在后面碰到了再说明，以下进入正文。</p><br><p>&nbsp;</p><br><p><strong>&nbsp; &nbsp; &nbsp; 摘要：</strong>人类基因组上的结构性变异研究对于基因组进化，群体多态性分析以及疾病易感性等方面的研究有着重要的意义。第二代短reads高通量测序技术的发展在带来了测序成本降低的同时，这种短读长的测序方式也给人类的变异检测带来了很大的挑战。这里我主要对当前常用的变异检测方法、软件以及他们各自的有确定做一个简要的小结。</p><br><p>&nbsp; &nbsp; &nbsp;人类基因组上的变异主要分为三大类：1. 单核苷酸变异，（通常称为单核苷酸多态性，通俗的说法就是单个DNA<a href="http://zh.wikipedia.org/wiki/%E6%A0%B8%E9%B9%BC%E5%9F%BA" target="_blank">碱基</a>的不同，简称SNP）；2. 小的Indel（Insertion 和 Deletion的简），指的是在基因组的某个位置上所发生的小片段序列的插入或者删除，其长度通常在50bp以下（这个长度范围的变异可以利用Smith-Waterman 的比对算法来获得<sup>1,2</sup>）；3. 大的结构性变异，这种类型比较多，包括长度在50bp以上的长片段序列的插入或者删除、染色体倒位，染色体内部或染色体之间的序列易位，拷贝数变异，以及一些形式更为复杂的变异。为了和SNP变异作区分，第2和第3类变异通常也被称为基因组结构性变异（Structural variation，简称SV）。这里值得一提的是，研究人员对基因组的结构性变异发生兴趣，主要是由于这几年的研究发现：（1）虽然还未被广泛公认，但研究人员发现SV对基因组的影响比起SNP来说还要大<sup>3</sup>；（2）基因组上的SV比起SNP而言，似乎更能用于解释人类群体多样性的特征；（3）稀有且相同的一些结构性变异往往和疾病（包括一些癌症）的发生相关联甚至还是其致病的诱因<sup>4&ndash;6</sup>。不过应该注意的地方是，大多数的结构性变异并不真正与疾病的发生相关联，但是却确实与周围环境的响应或者其他的一些表型多态性相联系。</p><br><p>&nbsp; &nbsp; &nbsp; 近年来，随着芯片技术（这里的芯片技术和IT领域所说的芯不是同一个概念，这里指的是一种用于抓获基因组特定序列片段的技术）和第二代高通量测序技术的发展，人类基因组上的结构性变异图谱才被真正全面而又集中地进行了研究。生物信息研究人员已针对这两种不同的技术开发了许多相对应的软件用于检测基因组的结构性变异。相比较而言，虽然成本较高，但是基于测序的方法要明显优于芯片的检测，其中最重要的一个方面是，高通量测序技术能够在单碱基精度之下对全基因组范围内所有类型的变异进行检测，而芯片技术实际上只对大片段的序列删除比较敏感。</p><br><p>&nbsp; &nbsp; &nbsp; 接下来我将会对目前基于第二代测序技术的变异检测方法进行介绍。</p><br><p>&nbsp; &nbsp; &nbsp; 在各大生物信息学期刊（包括Nature，Science，Cell等这些顶级期刊）上都有许多关于介绍变异检测方面的文章。这里我大致说一下四篇自己觉得在这方面比较重要的文章：综述&ldquo;Genome structural variation discovery and genotyping<sup>7</sup>&rdquo;和综述&ldquo;computational methods for discovering structural variation with next-generation sequencing&rdquo;，这两篇文章所探讨的主要是，如何根据实验上和计算上的途径来检测和发现基因组上的各种变异，特别是对检测SVs而已。另外两篇文章则是基于千人基因组计划的，他们描述的是如何利用trio家系全基因组测序的数据和群体低覆盖度的数据来做变异检测的生物信息学方法<sup>8,9</sup>。然而需要指出的是，对于千人基因组计划，他们基本上只关注于一些大片段的序列删除和一些特定的序列插入方面的检测，而忽视了很多基因组上其他形式的变异。关于这方面的局限性，一方面可能是由于生物信息检测方法上的不完善，另一方面可能也和千人基因组本身的数据特点有关，使得他们难以准确地获得更多的信息。</p><br><p>&nbsp; &nbsp; &nbsp; 目前主要有4种检测基因组上结构性变异的策略，分别为：（1）Read pair（也称为Pair-end Mapping，简称PEM）；（2）Split read（简称SR）；（3）Read Depth（简称RD）和（4）基于de novo组装的方法（图1）。同时生物信息研究人员也已开发了众多根据以上4中策略中一种或者多种的软件用于结构性变异的检测。接下来我将对这四种策略以及他们各自的特点逐一进行介绍。</p><br><p style="text-align: center;"><img src="http://images.cnitblog.com/blog/346148/201309/28234328-aff4feab4deb44c2bda4eb5daf5fbf1a.png" alt=""></p><br><p style="text-align: center;">图1</p><br><p>&nbsp; &nbsp; &nbsp; 1.&nbsp;基于Pair-end Mapping（PEM）</p><br><p>&nbsp; &nbsp; &nbsp;&nbsp;图2是PEM方法的一个主要分析框架，理论上来讲，PEM方法能够检测到的变异类型包括：序列删除（deletion），序列插入（insertion），序列转置（inversion），染色体内部和染色体外部的易位（intra- and inter-chromosome translocation），序列串联倍增（tandem duplications）和序列在基因组上的散在倍增（interspersed duplications）。这里有两个地方需要指出，第一，对于序列删除的检测，其所能检测到的片段长度受插入片段长度的标准差（SD）所影响（这里的插入片段长度指的是测序之前在构建DNA测序文库阶段，所选取的经由超声波打断的DNA片段长度，这些片段也称之为测序片段，这是实验过程中的操作，并不是指基因组的变异），并且越大的序列删除约容易被检测到，并且准确性也越高；第二，其所能检测的序列插入，长度只能在插入片段长度的范围内，并且最大长度也受限于测序的插入片段长度的标准差。目前，Breakdancer是应用PEM方法的软件，也是在使用变异检测方面用得最广泛的软件之一。其他类似的软件还包括：VariationHunter<sup>10</sup>, Spanner, PEMer<sup>11</sup>等等。但是，事实上整个过程并不像流程图中看起来的那么简单，而且绝大多数的软件都在检测复杂的序列结构方面（如序列易位和序列倍增）存在很大的困难。</p><br><p style="text-align: center;"><img src="http://images.cnitblog.com/blog/346148/201309/28234556-eedf4446718246be8030c341c37dfaba.png" alt=""></p><br><p style="text-align: center;">图2</p><br><p>&nbsp; &nbsp; &nbsp;&nbsp;</p><br><p>&nbsp; &nbsp; &nbsp; 2. Split Read（分裂read，简称SR）</p><br><p>&nbsp; &nbsp; &nbsp; 对于这个方法，首先要求比对软件具备soft-clip reads的能力，如BWA 比对软件。我们知道目前illumina测序平台Pair-End测序的方法是对测序片段的两端来进行的，所以每次获得的都是来自同一个测序序列片段两端的一对read。当BWA成功地将这一对reads中的一条比对到参考序列上，而另一条却无法正常比上的时候，BWA会对这条read没能正常比上的read尝试在比对上的那条read附近使用更为宽松的Smith-Waterman局部比对策略搜索可能的比对位置。如果这条read只有一部分能够比上，那么BWA会对其进行soft-clip，而这里也往往是包含结构性变异的断点之处。Pindel<sup>12</sup>，这是目前唯一一个使用SR方法进行变异检测的软件。它在千人基因组计划和生物信息分析人员中被广泛使用。图1中也清楚地展示了Split reads的信号如何被用来进行结构性变异的检测。首先，在获得了单端唯一比对到基因组上的PE read之后，Pindel会将不能比上的那条read切开成2或者3小段，然后再分别重新按照用户所设置的最大序列删除长度去比对，并获得最终的比对位置和比对方向，而断点位置的确定则是根据soft-clipped的结果来获得。</p><br><p>&nbsp; &nbsp; &nbsp; Pindel 理论上能够检测所有长度范围内的deletion，和小片段的insertion（长度在50bp以下），inversion，tandem duplication和一些large insertion。不过目前，作者并未公开发布关于检测lager insertion的原理。Split-reads的一个优势就在于，它们精确到单碱基。但是也和大多数的PEM方法一样，Pindel同样无法解决复杂结构性变异的情形。</p><br><p>&nbsp; &nbsp; &nbsp; 3. Read Depth （read 覆盖深度，简称RD）</p><br><p>&nbsp; &nbsp; &nbsp; 目前存在两种利用Read depth的信息检测大拷贝数变异（Copy number variation，包括丢失序列和序列重复倍增，简称CNV）的策略。一种是，通过检测样本在一个参考基因组上read的深度分布情况来检测CNV，适用于单样本；另一种则是通过和识别出比较两个样本中所存在的丢失和重复倍增区，以此来获得相对的CNV，适用于case-control模型的样本。这有点像CGH芯片。CNVnator使用的是第一种策略，同时也广泛地被用于检测大的CNV。当然还有一些比较冷门的软件，但是由于他们没有发表相应的文章，这里就不再列举了。CNV-seq使用的是第二个策略。基于其原理，RD的方法能够很好地用于检测一些大的deletion或者duplication事件，但是对于小的变异事件就无能为力了。</p><br><p>&nbsp; &nbsp; &nbsp; 4. 基于De novo assembly&nbsp;</p><br><p>&nbsp; &nbsp; &nbsp; 理论上来讲，de novo assembly 的方法应该要算是基因组变异检测上最有效的方法了。就目前来说，它能够提供（特别是）对于long insertion和复杂结构性变异的最好检测方法。现在虽然研究人员开发了很多基于第二代测序技术数据来进行组装的软件，但是组装却仍然是一件棘手的事情，特别是脊椎动物的组装则更是如此。其中最主要的原因在于，脊椎动物基因组上所存在的重复性序列和序列的杂合会严重影响组装的质量，除去资金成本，这也在很大程度上阻碍了利用组装的方法在基因组变异检测方面的应用。</p><br><p><strong>&nbsp; &nbsp; &nbsp; 小结：</strong></p><br><p>&nbsp; &nbsp; &nbsp; 通过对上面四种不同的变异检测策略的比较可以发现，小长度范围内的变异以及较长的deletion，目前都能够较好地检测出来，但对于大多数的long insertion和更复杂的结构性变异情况，当前的检测软件基本都没法还解决。Assembly应是当前全面获得基因组上各种变异的最好方法，但是目前的局限却也发生在Assembly本身，若是基因组没能装得好，后面的变异检测就更是无从说起。从目前的情况看，de novo assembly的方法并不能很快进入实际的应用。因此，暂且不提assembly，其余的三种策略都各有各的优势，从目前的结果看，并没有哪一款软件能够一次性地将基因组上的各种不同情况变异类型都获得。因此就目前短reads高通量测序技术来说，最合适的方案应是结合多个不同的策略，将结果合并在一起，这样可以最大限度地将FP降低。HugeSeq pipeline<sup>13</sup>在这方面做了一个比较好的总结，这个软件整合了BreakDancer, CNVnator, Pindel，BreakSeq以及GATK的结果。能够给出一个相对比较准确的变异检测结果。最后这句怎么看起来像是在帮别人卖广告o(╯□╰)o。<br></p><br><blockquote><br><p>1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DePristo, M. a <em>et al.</em> A framework for variation discovery and genotyping using next-generation DNA sequencing data. <em>Nature genetics</em><strong>43</strong>, 491&ndash;8 (2011).</p><br><p>2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Albers, C. a <em>et al.</em> Dindel: accurate indel calls from short-read data. <em>Genome research</em><strong>21</strong>, 961&ndash;73 (2011).</p><br><p>3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conrad, D. F. <em>et al.</em> Europe PMC Funders Group Origins and functional impact of copy number variation in the human genome. <strong>464</strong>, 704&ndash;712 (2012).</p><br><p>4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Campbell, P. J. <em>et al.</em> Identification of somatically acquired rearrangements in cancer using genome-wide massively parallel paired-end sequencing. <em>Nature genetics</em><strong>40</strong>, 722&ndash;9 (2008).</p><br><p>5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Berger, M. F. <em>et al.</em> The genomic complexity of primary human prostate cancer. <em>Nature</em><strong>470</strong>, 214&ndash;20 (2011).</p><br><p>6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Stephens, P. J. <em>et al.</em> Massive genomic rearrangement acquired in a single catastrophic event during cancer development. <em>Cell</em><strong>144</strong>, 27&ndash;40 (2011).</p><br><p>7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Alkan, C., Coe, B. P. &amp; Eichler, E. E. Genome structural variation discovery and genotyping. <em>Nature reviews. Genetics</em><strong>12</strong>, 363&ndash;76 (2011).</p><br><p>8.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mills, R. E. <em>et al.</em> Mapping copy number variation by population-scale genome sequencing. <em>Nature</em><strong>470</strong>, 59&ndash;65 (2011).</p><br><p>9.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Africa, W. A map of human genome variation from population-scale sequencing. <em>Nature</em><strong>467</strong>, 1061&ndash;73 (2010).</p><br><p>10.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hormozdiari, F., Alkan, C., Eichler, E. E. &amp; Sahinalp, S. C. Combinatorial algorithms for structural variation detection in high-throughput sequenced genomes. <em>Genome research</em><strong>19</strong>, 1270&ndash;8 (2009).</p><br><p>11.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Korbel, J. O. <em>et al.</em> PEMer: a computational framework with simulation-based error models for inferring genomic structural variants from massive paired-end sequencing data. <em>Genome biology</em><strong>10</strong>, R23 (2009).</p><br><p>12.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ye, K., Schulz, M. H., Long, Q., Apweiler, R. &amp; Ning, Z. Pindel: a pattern growth approach to detect break points of large deletions and medium sized insertions from paired-end short reads. <em>Bioinformatics (Oxford, England)</em><strong>25</strong>, 2865&ndash;71 (2009).</p><br><p>13.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Lam, H. Y. K. <em>et al.</em> Detecting and annotating genetic variations using the HugeSeq pipeline. <em>Nature biotechnology</em><strong>30</strong>, 226&ndash;9 (2012).</p><br></blockquote><br><p>&nbsp;</p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
            <category> 基因组学 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NGS </tag>
            
            <tag> 变异检测 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[三代基因组测序技术原理简介]]></title>
      <url>/2013/08/02/2013-08-02-An-Introduction-of-NGS-Sequence.html</url>
      <content type="html"><![CDATA[<p><strong>摘要：</strong>从1977年第一代DNA测序技术（Sanger法）<sup>1</sup>，发展至今三十多年时间，测序技术已取得了相当大的发展，从第一代到第三代乃至第四代，测序读长从长到短，再从短到长。虽然就当前形势看来第二代短读长测序技术在全球测序市场上仍然占有着绝对的优势位置，但第三和第四代测序技术也已在这一两年的时间中快速发展着。测序技术的每一次变革，也都对基因组研究，疾病医疗研究，药物研发，育种等领域产生巨大的推动作用。在这里我主要对当前的测序技术以及它们的测序原理做一个简单的小结。</p>

<p><img src="http://images.cnitblog.com/blog/346148/201308/02214339-89514ce8dd50409392c389f35bb9be01.png" alt="Figure1"></p>
<p align="center"><a href="http://images.cnitblog.com/blog/346148/201308/02214339-89514ce8dd50409392c389f35bb9be01.png" target="_blank" rel="external">图1：测序技术的发展历程</a></p>

<p>生命体遗传信息的快速获得对于生命科学的研究有着十分重要的意义。以上图1（右键打开图片可查看大图，下同）所描述的是自沃森和克里克在1953年建立DNA双螺旋结构以来，整个测序技术的发展历程。</p><br><p><strong><span style="color: #ff0000;">第一代测序技术</span></strong></p><br>第一代DNA测序技术用的是1975年由桑格（Sanger）和考尔森（Coulson）开创的链终止法或者是1976-1977年由马克西姆（Maxam）和吉尔伯特（Gilbert）发明的化学法（链降解）. 并在1977年，桑格测定了第一个基因组序列，是噬菌体X174的，全长5375个碱基<sup>1</sup>。自此，人类获得了窥探生命遗传差异本质的能力，并以此为开端步入基因组学时代。研究人员在Sanger法的多年实践之中不断对其进行改进。在2001年，完成的首个人类基因组图谱就是以改进了的Sanger法为其测序基础，Sanger法核心原理是：由于ddNTP的2&rsquo;和3&rsquo;都不含羟基，其在DNA的合成过程中不能形成磷酸二酯键，因此可以用来中断DNA合成反应，在4个DNA合成反应体系中分别加入一定比例带有放射性同位素标记的ddNTP（分为：ddATP,ddCTP,ddGTP和ddTTP），通过凝胶电泳和放射自显影后可以根据电泳带的位置确定待测分子的DNA序列（图2）。这个<a href="http://smcg.cifn.unam.mx/enp-unam/03-EstructuraDelGenoma/animaciones/secuencia.swf" target="_blank" rel="external">网址</a>为sanger测序法制作了一个小短片，形象而生动。<br><p>值得注意的是，就在测序技术起步发展的这一时期中，除了Sanger法之外还出现了一些其他的测序技术，如焦磷酸测序法、链接酶法等。其中，焦磷酸测序法是后来Roche公司454技术所使用的测序方法<sup>2&ndash;4</sup>，而连接酶测序法是后来ABI公司SOLID技术使用的测序方法<sup>2,4</sup>，但他们的共同核心手段都是利用了Sanger<sup>1</sup>中的可中断DNA合成反应的dNTP。</p>

<p><img src="http://images.cnitblog.com/blog/346148/201308/02215416-9a87f1e5ab1048a8bbc62cb7605f1bcf.png" alt="Figure2"></p>
<p align="center"><a href="http://images.cnitblog.com/blog/346148/201308/02215416-9a87f1e5ab1048a8bbc62cb7605f1bcf.png" target="_blank" rel="external">图2：Sanger法测序原理</a></p>

<p><strong><span style="color: #ff0000;">第二代测序技术</span></strong></p><br><p>总的说来，第一代测序技术的主要特点是测序读长可达1000bp，准确性高达99.999%，但其测序成本高，通量低等方面的缺点，严重影响了其真正大规模的应用。因而第一代测序技术并不是最理想的测序方法。经过不断的技术开发和改进，以Roche公司的454技术、illumina公司的Solexa，Hiseq技术和ABI公司的Solid技术为标记的第二代测序技术诞生了。第二代测序技术大大降低了测序成本的同时，还大幅提高了测序速度，并且保持了高准确性，以前完成一个人类基因组的测序需要3年时间，而使用二代测序技术则仅仅需要1周，但在序列读长方面比起第一代测序技术则要短很多。表1和图3对第一代和第二代测序技术各自的特点以及测序成本作了一个简单的比较<sup>5</sup>，以下我将对这三种主要的第二代测序技术的主要原理和特点作一个简单的介绍。&nbsp;</p>

<p><img src="http://images.cnitblog.com/blog/346148/201308/02215522-1d276b88d49a41439f34bb60bf8b7e9f.png" alt="Figure3"></p>
<p align="center"><a href="http://images.cnitblog.com/blog/346148/201308/02215522-1d276b88d49a41439f34bb60bf8b7e9f.png" target="_blank" rel="external">图3. 测序成本的变化</a></p>

<p><ol></ol></p>
<p><li>Illumine</li><br></p>
<p style="margin-left: 60px;">Illumina公司的Solexa和Hiseq应该说是目前全球使用量最大的第二代测序机器，这两个系列的技术核心原理是相同的<sup>2,4</sup>。这两个系列的机器采用的都是边合成边测序的方法，它的测序过程主要分为以下4步，如图4.</p><br><p>&nbsp;&nbsp;&nbsp;&nbsp; （1）DNA待测文库构建</p><br><p style="margin-left: 60px;">利用超声波把待测的DNA样本打断成小片段，目前除了组装之外和一些其他的特殊要求之外，主要是打断成200-500bp长的序列片段，并在这些小片段的两端添加上不同的接头，构建出单链DNA文库。</p><br><p>&nbsp;&nbsp;&nbsp;&nbsp; （2）Flowcell</p><br><p style="margin-left: 60px;">Flowcell是用于吸附流动DNA片段的槽道，当文库建好后，这些文库中的DNA在通过flowcell的时候会随机附着在flowcell表面的channel上。每个Flowcell有8个channel，每个channel的表面都附有很多接头，这些接头能和建库过程中加在DNA片段两端的接头相互配对（这就是为什么flowcell能吸附建库后的DNA的原因），并能支持DNA在其表面进行桥式PCR的扩增。</p><br><p>&nbsp;&nbsp;&nbsp;&nbsp; （3）桥式PCR扩增与变性</p><br><p style="margin-left: 60px;">桥式PCR以Flowcell表面所固定的接头为模板，进行桥形扩增，如图4.a所示。经过不断的扩增和变性循环，最终每个DNA片段都将在各自的位置上集中成束，每一个束都含有单个DNA模板的很多分拷贝，进行这一过程的目的在于实现将碱基的信号强度放大，以达到测序所需的信号要求。&nbsp;</p><br><p style="margin-left: 30px;">（4）测序</p><br><p style="margin-left: 60px;">测序方法采用边合成边测序的方法。向反应体系中同时添加DNA聚合酶、接头引物和带有碱基特异荧光标记的4中dNTP（如同Sanger测序法）。这些dNTP的3&rsquo;-OH被化学方法所保护，因而每次只能添加一个dNTP。在dNTP被添加到合成链上后，所有未使用的游离dNTP和DNA聚合酶会被洗脱掉。接着，再加入激发荧光所需的缓冲液，用激光激发荧光信号，并有光学设备完成荧光信号的记录，最后利用计算机分析将光学信号转化为测序碱基。这样荧光信号记录完成后，再加入化学试剂淬灭荧光信号并去除dNTP 3&rsquo;-OH保护基团，以便能进行下一轮的测序反应。Illumina的这种测序技术每次只添加一个dNTP的特点能够很好的地解决同聚物长度的准确测量问题，它的主要测序错误来源是碱基的替换，目前它的测序错误率在1%-1.5%之间，测序周期以人类基因组重测序为例，30x测序深度大约为1周。</p><br><p>&nbsp;</p>

<p><img src="http://images.cnitblog.com/blog/346148/201308/02215633-506fdf6d10af4035812ba4898c02985b.png" alt="Figure4_a"><br><img src="http://images.cnitblog.com/blog/346148/201308/02215656-baeb82e7b0ba4378a7a3417c450243b7.png" alt="Figure4_b"></p>
<p align="center"><a href="http://images.cnitblog.com/blog/346148/201308/02215633-506fdf6d10af4035812ba4898c02985b.png" target="_blank" rel="external">图4. Illumina测序流程</a></p>

<p><ol></ol></p>
<p><li>Roche 454</li><br></p>
<p style="margin-left: 60px;">Roche 454测序系统是第一个商业化运营二代测序技术的平台。它的主要测序原理是（图5 abc）<sup>2</sup>：</p><br><p style="margin-left: 30px;">（1）DNA文库制备</p><br><p style="margin-left: 30px;">454测序系统的文件构建方式和illumina的不同，它是利用喷雾法将待测DNA打断成300-800bp长的小片段，并在片段两端加上不同的接头，或将待测DNA变性后用杂交引物进行PCR扩增，连接载体，构建单链DNA文库（图5a）。</p><br><p style="margin-left: 30px;">（2）Emulsion PCR （乳液PCR，其实是一个注水到油的独特过程）</p><br><p style="margin-left: 60px;">454当然DNA扩增过程也和illumina的截然不同，它将这些单链DNA结合在水油包被的直径约28um的磁珠上，并在其上面孵育、退火。</p><br><p style="margin-left: 30px;">乳液PCR最大的特点是可以形成数目庞大的独立反应空间以进行DNA扩增。其关键技术是&ldquo;注水到油&rdquo;（水包油），基本过程是在PCR反应前，将包含PCR所有反应成分的水溶液注入到高速旋转的矿物油表面，水溶液瞬间形成无数个被矿物油包裹的小水滴。这些小水滴就构成了独立的PCR反应空间。理想状态下，每个小水滴只含一个DNA模板和一个磁珠。</p><br><p style="margin-left: 30px;">这些被小水滴包被的磁珠表面含有与接头互补的DNA序列，因此这些单链DNA序列能够特异地结合在磁珠上。同时孵育体系中含有PCR反应试剂，所以保证了每个与磁珠结合的小片段都能独立进行PCR扩增，并且扩增产物仍可以结合到磁珠上。当反应完成后，可以破坏孵育体系并将带有DNA的磁珠富集下来。进过扩增，每个小片段都将被扩增约100万倍，从而达到下一步测序所要求的DNA量。</p><br><p style="margin-left: 30px;">（3）焦磷酸测序</p><br><p style="margin-left: 30px;">测序前需要先用一种聚合酶和单链结合蛋白处理带有DNA的磁珠，接着将磁珠放在一种PTP平板上。这种平板上特制有许多直径约为44um的小孔，每个小孔仅能容纳一个磁珠，通过这种方法来固定每个磁珠的位置，以便检测接下来的测序反应过程。　　</p><br><p style="margin-left: 30px;">测序方法采用焦磷酸测序法，将一种比PTP板上小孔直径更小的磁珠放入小孔中，启动测序反应。测序反应以磁珠上大量扩增出的单链DNA为模板，每次反应加入一种dNTP进行合成反应。如果dNTP能与待测序列配对，则会在合成后释放焦磷酸基团。释放的焦磷酸基团会与反应体系中的ATP硫酸化学酶反应生成ATP。生成的ATP和荧光素酶共同氧化使测序反应中的荧光素分子并发出荧光，同时由PTP板另一侧的CCD照相机记录，最后通过计算机进行光信号处理而获得最终的测序结果。由于每一种dNTP在反应中产生的荧光颜色不同，因此可以根据荧光的颜色来判断被测分子的序列。反应结束后，游离的dNTP会在双磷酸酶的作用下降解ATP，从而导致荧光淬灭，以便使测序反应进入下一个循环。由于454测序技术中，每个测序反应都在PTP板上独立的小孔中进行，因而能大大降低相互间的干扰和测序偏差。454技术最大的优势在于其能获得较长的测序读长，当前454技术的平均读长可达400bp，并且454技术和illumina的Solexa和Hiseq技术不同，它最主要的一个缺点是无法准确测量同聚物的长度，如当序列中存在类似于PolyA的情况时，测序反应会一次加入多个T，而所加入的T的个数只能通过荧光强度推测获得，这就有可能导致结果不准确。也正是由于这一原因，454技术会在测序过程中引入插入和缺失的测序错误。&nbsp;</p>

<p><img src="http://images.cnitblog.com/blog/346148/201308/02215752-3e82bec8a4bd401e928188af86e3f129.png" alt="Figure5_a"><br><img src="http://images.cnitblog.com/blog/346148/201308/02215812-879851082dfb41e0a2903dbcdedabcd4.png" alt="Figure5_b"><br><img src="http://images.cnitblog.com/blog/346148/201308/02215833-bfc37705efd74b10be7f1e37e1f851c1.png" alt="Figure5_c"></p>
<p align="center"><a href="http://images.cnitblog.com/blog/346148/201308/02215752-3e82bec8a4bd401e928188af86e3f129.png" target="_blank" rel="external">图5. Roche 454测序流程</a></p>

<p><ol></ol></p>
<p><li>Solid技术</li><br></p>
<p style="margin-left: 60px;">Solid测序技术是ABI公司于2007年开始投入用于商业测序应用的仪器。它基于连接酶法，即利用DNA连接酶在连接过程之中测序（图6）<sup>2</sup>,<sup>4</sup>。它的原理是：</p>

<p><img src="http://images.cnitblog.com/blog/346148/201308/02215942-1a81b422e45b40d29f2075656d087391.png" alt="Figure6_a"></p>
<p align="center"><a href="http://images.cnitblog.com/blog/346148/201308/02215942-1a81b422e45b40d29f2075656d087391.png" target="_blank" rel="external">图6-a. Solid测序技术</a></p>

<p>（1）DNA文库构建</p><br><p>    片段打断并在片段两端加上测序接头，连接载体，构建单链DNA文库。</p><br><p>（2）Emulsion PCR</p><br><p style="margin-left: 60px;">Solid的PCR过程也和454的方法类似，同样采用小水滴emulsion PCR，但这些微珠比起454系统来说则要小得多，只有1um。在扩增的同时对扩增产物的3&rsquo;端进行修饰，这是为下一步的测序过程作的准备。3&rsquo;修饰的微珠会被沉积在一块玻片上。在微珠上样的过程中，沉积小室将每张玻片分成1个、4个或8个测序区域（图6-a）。Solid系统最大的优点就是每张玻片能容纳比454更高密度的微珠，在同一系统中轻松实现更高的通量。</p><br><p>（3）连接酶测序</p><br><p style="margin-left: 60px;">这一步是Solid测序的独特之处。它并没有采用以前测序时所常用的DNA聚合酶，而是采用了连接酶。Solid连接反应的底物是8碱基单链荧光探针混合物，这里将其简单表示为：3&rsquo;-XXnnnzzz-5&rsquo;。连接反应中，这些探针按照碱基互补规则与单链DNA模板链配对。探针的5&rsquo;末端分别标记了CY5、Texas Red、CY3、6-FAM这4种颜色的荧光染料（图6-a）。这个8碱基单链荧光探针中，第1和第2位碱基（XX）上的碱基是确定的，并根据种类的不同在6-8位（zzz）上加上了不同的荧光标记。这是Solid的独特测序法，两个碱基确定一个荧光信号，相当于一次能决定两个碱基。这种测序方法也称之为两碱基测序法。当荧光探针能够与DNA模板链配对而连接上时，就会发出代表第1，2位碱基的荧光信号，图6-a和图6-b中的比色版所表示的是第1，2位碱基的不同组合与荧光颜色的关系。在记录下荧光信号后，通过化学方法在第5和第6位碱基之间进行切割，这样就能移除荧光信号，以便进行下一个位置的测序。不过值得注意的是，通过这种测序方法，每次测序的位置都相差5位。即第一次是第1、2位，第二次是第6、7位&hellip;&hellip;在测到末尾后，要将新合成的链变性，洗脱。接着用引物n-1进行第二轮测序。引物n-1与引物n的区别是，二者在与接头配对的位置上相差一个碱基（图6-a. 8）。也即是，通过引物n-1在引物n的基础上将测序位置往3&rsquo;端移动一个碱基位置，因而就能测定第0、1位和第5、6位&hellip;&hellip;第二轮测序完成，依此类推，直至第五轮测序，最终可以完成所有位置的碱基测序，并且每个位置的碱基均被检测了两次。该技术的读长在2&times;50bp，后续序列拼接同样比较复杂。由于双次检测，这一技术的原始测序准确性高达99.94%，而15x覆盖率时的准确性更是达到了99.999%，应该说是目前第二代测序技术中准确性最高的了。但在荧光解码阶段，鉴于其是双碱基确定一个荧光信号，因而一旦发生错误就容易产生连锁的解码错误。</p>

<p><img src="http://images.cnitblog.com/blog/346148/201308/02220036-548ce076a79149609da6e40075129c4f.png" alt="Figure6_b"></p>
<p align="center"><a href="http://images.cnitblog.com/blog/346148/201308/02220036-548ce076a79149609da6e40075129c4f.png" target="_blank" rel="external">图6-b. Solid测序技术</a></p>

<p style="margin-left: 30px;"><strong><span style="color: #ff0000;">第三代测序技术</span></strong></p><br><p>测序技术在近两三年中又有新的里程碑。以PacBio公司的SMRT和Oxford Nanopore Technologies纳米孔单分子测序技术，被称之为第三代测序技术。与前两代相比，他们最大的特点就是单分子测序，测序过程无需进行PCR扩增。</p><br><p>其中PacBio SMRT技术其实也应用了边合成边测序的思想<sup>5</sup>，并以SMRT芯片为测序载体。基本原理是： DNA聚合酶和模板结合,4色荧光标记 4 种碱基（即是dNTP）,在碱基配对阶段,不同碱基的加入,会发出不同光,根据光的波长与峰值可判断进入的碱基类型。同时这个 DNA 聚合酶是实现超长读长的关键之一,读长主要跟酶的活性保持有关,它主要受激光对其造成的损伤所影响。PacBio SMRT技术的一个关键是怎样将反应信号与周围游离碱基的强大荧光背景区别出来。他们利用的是ZMW（零模波导孔）原理：如同微波炉壁上可看到的很多密集小孔。小孔直径有考究,如果直径大于微波波长,能量就会在衍射效应的作用下穿透面板而泄露出来，从而与周围小孔相互干扰。如果孔径小于波长,能量不会辐射到周围，而是保持直线状态（光衍射的原理）,从而可起保护作用。同理,在一个反应管(SMRTCell:单分子实时反应孔)中有许多这样的圆形纳米小孔, 即 ZMW(零模波导孔),外径 100多纳米,比检测激光波长小(数百纳米),激光从底部打上去后不能穿透小孔进入上方溶液区,能量被限制在一个小范围(体积20X 10-21 L)里,正好足够覆盖需要检测的部分,使得信号仅来自这个小反应区域,孔外过多游离核苷酸单体依然留在黑暗中,从而实现将背景降到最低。另外，可以通过检测相邻两个碱基之间的测序时间，来检测一些碱基修饰情况，既如果碱基存在修饰，则通过聚合酶时的速度会减慢，相邻两峰之间的距离增大，可以通过这个来之间检测甲基化等信息（图7）。SMRT技术的测序速度很快，每秒约10个dNTP。但是，同时其测序错误率比较高（这几乎是目前单分子测序技术的通病），达到15%,但好在它的出错是随机的，并不会像第二代测序技术那样存在测序错误的偏向，因而可以通过多次测序来进行有效的纠错。</p>

<p><img src="http://images.cnitblog.com/blog/346148/201308/02220111-aac12c31ed944097b1ef793332ed5b24.png" alt="Figure7"></p>
<p align="center"><a href="http://images.cnitblog.com/blog/346148/201308/02220111-aac12c31ed944097b1ef793332ed5b24.png" target="_blank" rel="external">图7. PacBio SMRT测序原理</a></p>

<p>Oxford Nanopore Technologies公司所开发的纳米单分子测序技术与以往的测序技术皆不同，它是基于电信号而不是光信号的测序技术<sup>5</sup>。该技术的关键之一是，他们设计了一种特殊的纳米孔，孔内共价结合有分子接头。当DNA碱基通过纳米孔时，它们使电荷发生变化，从而短暂地影响流过纳米孔的电流强度（每种碱基所影响的电流变化幅度是不同的），灵敏的电子设备检测到这些变化从而鉴定所通过的碱基（图8）。</p><br><p>该公司在去年基因组生物学技术进展年会(AGBT)上推出第一款商业化的纳米孔测序仪，引起了科学界的极大关注。纳米孔测序（和其他第三代测序技术）有望解决目前测序平台的不足，纳米孔测序的主要特点是：读长很长，大约在几十kb，甚至100 kb;错误率目前介于1%至4%，且是随机错误，而不是聚集在读取的两端;数据可实时读取;通量很高(30x人类基因组有望在一天内完成);起始DNA在测序过程中不被破坏;以及样品制备简单又便宜。理论上，它也能直接测序RNA。</p><br><p>纳米孔单分子测序计算还有另一大特点，它能够直接读取出甲基化的胞嘧啶，而不必像传统方法那样对基因组进行bisulfite处理。这对于在基因组水平直接研究表观遗传相关现象有极大的帮助。并且改方法的测序准确性可达99.8%，而且一旦发现测序错误也能较容易地进行纠正。但目前似乎还没有应用该技术的相关报道。</p>

<p><img src="http://images.cnitblog.com/blog/346148/201308/02220146-57c9245f4a1b4f0bbd3a0caf7046f7bc.png" alt="Figure8"></p>
<p align="center"><a href="http://images.cnitblog.com/blog/346148/201308/02220146-57c9245f4a1b4f0bbd3a0caf7046f7bc.png" target="_blank" rel="external">图8. 纳米孔测序</a></p>

<p>其他测序技术</p><br><p>目前还有一种基于半导体芯片的新一代革命性测序技术&mdash;&mdash;Ion Torrent<sup>6</sup>。该技术使用了一种布满小孔的高密度半导体芯片， 一个小孔就是一个测序反应池。当DNA聚合酶把核苷酸聚合到延伸中的DNA链上时，会释放出一个氢离子，反应池中的PH发生改变，位于池下的离子感受器感受到H+离子信号，H+离子信号再直接转化为数字信号，从而读出DNA序列（图9）。这一技术的发明人同时也是454测序技术的发明人之一&mdash;&mdash;Jonathan Rothberg，它的文库和样本制备跟454技术很像，甚至可以说就是454的翻版，只是测序过程中不是通过检测焦磷酸荧光显色，而是通过检测H+信号的变化来获得序列碱基信息。Ion Torrent相比于其他测序技术来说，不需要昂贵的物理成像等设备，因此，成本相对来说会低，体积也会比较小，同时操作也要更为简单，速度也相当快速，除了2天文库制作时间，整个上机测序可在2-3.5小时内完成，不过整个芯片的通量并不高，目前是10G左右，但非常适合小基因组和外显子验证的测序。&nbsp;&nbsp;&nbsp;&nbsp;</p>

<p><img src="http://images.cnitblog.com/blog/346148/201308/02220244-63a58f22939d4b59890f8c95068a4296.png" alt="Figure9_a"><br><img src="http://images.cnitblog.com/blog/346148/201308/02220317-7bd9cecbe1c04cdbad13a63e66eeafe5.png" alt="Figure9_b"><br><img src="http://images.cnitblog.com/blog/346148/201308/02220354-9cd0104936984b47b19a64f0b8ed0d68.png" alt="Figure9_c"></p>
<p align="center"><a href="http://images.cnitblog.com/blog/346148/201308/02220244-63a58f22939d4b59890f8c95068a4296.png" target="_blank" rel="external">图9. Ion Torrent</a></p>

<p align="center">&nbsp;</p><br><p><strong>小结</strong></p><br><p>以上，对各代测序技术的原理做了简要的阐述，这三代测序技术的特点比较汇总在以下表1和表2中。其中测序成本，读长和通量是评估该测序技术先进与否的三个重要指标。第一代和第二代测序技术除了通量和成本上的差异之外，其测序核心原理（除Solid是边连接边测序之外）都是基于边合成边测序的思想。第二代测序技术的优点是成本较之一代大大下降，通量大大提升，但缺点是所引入PCR过程会在一定程度上增加测序的错误率，并且具有系统偏向性，同时读长也比较短。第三代测序技术是为了解决第二代所存在的缺点而开发的，它的根本特点是单分子测序，不需要任何PCR的过程，这是为了能有效避免因PCR偏向性而导致的系统错误，同时提高读长，并要保持二代技术的高通量，低成本的优点。</p>

<p align="left">表1：测序技术的比较</p><br><div align="center"><br><table style="width: 727px; height: 1267px;" border="1" cellspacing="0" cellpadding="0"><br><tbody><br><tr><br><td width="48"><br><p align="center"><strong>第</strong><strong>X</strong><strong>代</strong></p><br></td><br><td width="48"><br><p align="center"><strong>公司</strong></p><br></td><br><td width="64"><br><p align="center"><strong>平台名称</strong></p><br></td><br><td width="59"><br><p align="center"><strong>测序方法</strong></p><br></td><br><td width="53"><br><p align="center"><strong>检测方法</strong></p><br></td><br><td width="56"><br><p align="center"><strong>大约读长</strong><strong>(</strong><strong>碱基数</strong><strong>)</strong></p><br></td><br><td width="91"><br><p align="center"><strong>优点</strong></p><br></td><br><td width="112"><br><p align="center"><strong>相对局限性</strong></p><br></td><br></tr><br><tr><br><td width="48"><br><p align="center">第一代</p><br></td><br><td width="48"><br><p align="center">ABI/生命技术公司</p><br></td><br><td width="64"><br><p align="center">3130xL-3730xL</p><br></td><br><td width="59"><br><p align="center">桑格-毛细管电泳测序法</p><br></td><br><td width="53"><br><p align="center">荧光/光学</p><br></td><br><td width="56"><br><p align="center">600-1000</p><br></td><br><td width="91"><br><p align="left">高读长，准确度一次性达标率高，能很好处理重复序列和多聚序列</p><br></td><br><td width="112"><br><p align="left">通量低；样品制备成本高，使之难以做大量的平行测序</p><br></td><br></tr><br><tr><br><td width="48"><br><p align="center">第一代</p><br></td><br><td width="48"><br><p align="center">贝克曼</p><br></td><br><td width="64"><br><p align="center">GeXP遗传分析系统</p><br></td><br><td width="59"><br><p align="center">桑格-毛细管电泳测序法</p><br></td><br><td width="53"><br><p align="center">荧光/光学</p><br></td><br><td width="56"><br><p align="center">600-1000</p><br></td><br><td width="91"><br><p align="left">高读长，准确度一次性达标率高，能很好处理重复序列和多聚序列；易小型化</p><br></td><br><td width="112"><br><p align="left">通量低；单个样品的制备成本相对较高</p><br></td><br></tr><br><tr><br><td width="48"><br><p align="center">第二代</p><br></td><br><td width="48"><br><p align="center">Roche/454</p><br></td><br><td width="64"><br><p align="center">基因组测序仪FLX系统</p><br></td><br><td width="59"><br><p align="center">焦磷酸测序法</p><br></td><br><td width="53"><br><p align="center">光学</p><br></td><br><td width="56"><br><p align="center">230-400</p><br></td><br><td width="91"><br><p align="left">在第二代中最高读长；比第一代的测序通量大</p><br></td><br><td width="112"><br><p align="left">样品制备较难；难于处理重复和同种碱基多聚区域；试剂冲洗带来错误累积；仪器昂贵</p><br></td><br></tr><br><tr><br><td width="48"><br><p align="center">第二代</p><br></td><br><td width="48"><br><p align="center">Illumina</p><br></td><br><td width="64"><br><p align="center">HiSeq2000,HiSeq2500/MiSeq</p><br></td><br><td width="59"><br><p align="center">可逆链终止物和合成测序法</p><br></td><br><td width="53"><br><p align="center">荧光/光学</p><br></td><br><td width="56"><br><p align="center"><strong>2x150</strong></p><br></td><br><td width="91"><br><p align="left">很高测序通量</p><br></td><br><td width="112"><br><p align="left">仪器昂贵；用于数据删节和分析的费用很高</p><br></td><br></tr><br><tr><br><td width="48"><br><p align="center">第二代</p><br></td><br><td width="48"><br><p align="center">ABI/Solid</p><br></td><br><td width="64"><br><p align="center">5500xlSolid系统</p><br></td><br><td width="59"><br><p align="center">连接测序法</p><br></td><br><td width="53"><br><p align="center">荧光/光学</p><br></td><br><td width="56"><br><p align="center">25-35</p><br></td><br><td width="91"><br><p align="left">很高测序通量；在广为接受的几种第二代平台中，所要拼接出人类基因组的试剂成本最低</p><br></td><br><td width="112"><br><p align="left">测序运行时间长；读长短，造成成本高，数据分析困难和基因组拼接困难；仪器昂贵</p><br></td><br></tr><br><tr><br><td width="48"><br><p align="center">第二代</p><br></td><br><td width="48"><br><p align="center">赫利克斯</p><br></td><br><td width="64"><br><p align="center">Heliscope</p><br></td><br><td width="59"><br><p align="center">单分子合成测序法</p><br></td><br><td width="53"><br><p align="center">荧光/光学</p><br></td><br><td width="56"><br><p align="center">25-30</p><br></td><br><td width="91"><br><p align="left">高通量；在第二代中属于单分子性质的测序技术</p><br></td><br><td width="112"><br><p align="left">读长短，推高了测序成本，降低了基因组拼接的质量；仪器非常昂贵</p><br></td><br></tr><br><tr><br><td width="48"><br><p align="center">第三代</p><br></td><br><td width="48"><br><p align="center">太平洋生物科学公司</p><br></td><br><td width="64"><br><p align="center">PacBio RS</p><br></td><br><td width="59"><br><p align="center">实时单分子DNA测序</p><br></td><br><td width="53"><br><p align="center">荧光/光学</p><br></td><br><td width="56"><br><p align="center">~1000</p><br></td><br><td width="91"><br><p align="left">高平均读长，比第一代的测序时间降低；不需要扩增；最长单个读长接近3000碱基</p><br></td><br><td width="112"><br><p align="left">并不能高效地将DNA聚合酶加到测序阵列中；准确性一次性达标的机会低（81-83%）；DNA聚合酶在阵列中降解；总体上每个碱基测序成本高（仪器昂贵）；</p><br></td><br></tr><br><tr><br><td width="48"><br><p align="center">第三代</p><br></td><br><td width="48"><br><p align="center">全基因组学公司</p><br></td><br><td width="64"><br><p align="center">GeXP遗传分析系统</p><br></td><br><td width="59"><br><p align="center">复合探针锚杂交和连接技术</p><br></td><br><td width="53"><br><p align="center">荧光/光学</p><br></td><br><td width="56"><br><p align="center">10</p><br></td><br><td width="91"><br><p align="left">在第三代中通量最高；在所有测序技术中，用于拼接一个人基因组的试剂成本最低；每个测序步骤独立，使错误的累积变得最低</p><br></td><br><td width="112"><br><p align="left">低读长；&nbsp;模板制备妨碍长重复序列区域测序；样品制备费事；尚无商业化供应的仪器</p><br></td><br></tr><br><tr><br><td width="48"><br><p align="center">第三代</p><br></td><br><td width="48"><br><p align="center">Ion Torrent/生命技术公司</p><br></td><br><td width="64"><br><p align="center">个人基因组测序仪（PGM）</p><br></td><br><td width="59"><br><p align="center">&nbsp;合成测序法</p><br></td><br><td width="53"><br><p align="center">以离子敏感场效应晶体管检测pH值变化</p><br></td><br><td width="56"><br><p align="center">100-200</p><br></td><br><td width="91"><br><p align="left">对核酸碱基的掺入可直接测定；在自然条件下进行DNA合成（不需要使用修饰过的碱基）</p><br></td><br><td width="112"><br><p align="left">一步步的洗脱过程可导致错误累积；阅读高重复和同种多聚序列时有潜在困难；</p><br></td><br></tr><br><tr><br><td width="48"><br><p align="center">第三代</p><br></td><br><td width="48"><br><p align="center">牛津纳米孔公司</p><br></td><br><td width="64"><br><p align="center">&nbsp;gridION</p><br></td><br><td width="59"><br><p align="center">纳米孔外切酶测序</p><br></td><br><td width="53"><br><p align="center">电流</p><br></td><br><td width="56"><br><p align="center">尚未定量</p><br></td><br><td width="91"><br><p align="left">有潜力达到高读长；可以成本生产纳米孔；无需荧光标记或光学手段</p><br></td><br><td width="112"><br><p align="left">切断的核苷酸可能被读错方向；难于生产出带多重平行孔的装置</p><br></td><br></tr><br></tbody><br></table><br></div><br><p align="center">&nbsp;&nbsp;</p><br><p align="left">表2：主流测序机器的成本测序比较</p>

<p><img src="http://images.cnitblog.com/blog/346148/201308/02220513-79289588a8bf40e081f3ab8c9cdda24d.png" alt="Table"></p>
<p>以下图10展示了当前全球测序仪的分布情况。图中的几个热点区主要分布在中国的深圳（主要是华大），南欧，西欧和美国。</p>
<p><img src="http://images.cnitblog.com/blog/346148/201308/02220617-ca30733231724ac4b357b0b98616e898.png" alt="Figure10"></p>
<p align="center"><a href="http://omicsmaps.com/" target="_blank" rel="external">图10. 测序仪全球分布</a></p>

<p><em>参考文献</em></p>
<blockquote><br><p>1. Sanger, F. &amp; Nicklen, S. DNA sequencing with chain-terminating. <strong>74</strong>, 5463&ndash;5467 (1977).</p><br><p>2. Mardis, E. R. Next-generation DNA sequencing methods. <em>Annual review of genomics and human genetics</em> <strong>9</strong>, 387&ndash;402 (2008).</p><br><p>3. Shendure, J. &amp; Ji, H. Next-generation DNA sequencing. <em>Nature biotechnology</em> <strong>26</strong>, 1135&ndash;45 (2008).</p><br><p>4. Metzker, M. L. Sequencing technologies - the next generation. <em>Nature reviews. Genetics</em> <strong>11</strong>, 31&ndash;46 (2010).</p><br><p>5. Niedringhaus, T. P., Milanova, D., Kerby, M. B., Snyder, M. P. &amp; Barron, A. E. Landscape of Next-Generation Sequencing Technologies. 4327&ndash;4341 (2011).</p><br><p>6. Rothberg, J. M. <em>et al.</em> An integrated semiconductor device enabling non-optical genome sequencing. <em>Nature</em> <strong>475</strong>, 348&ndash;52 (2011).&nbsp;</p><br></blockquote>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
            <category> 基因组学 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NGS </tag>
            
            <tag> 测序技术 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[测序量估计]]></title>
      <url>/2013/07/11/2013-07-11-Sequence-estimate.html</url>
      <content type="html"><![CDATA[<p>考虑这样一个问题，如果要保证基因组上95%的区域其覆盖深度在30x以上的话，那么最低的平均测序深度应该是多少?</p>
<p>关于测序量的估计，对于做生物信息的人来讲应算是家常便饭了，多数时候我们都能直接根据以往项目的经验来获得，或是说的更具体些，在变异检测中一般要有25x以上的覆盖度才能得到一个比较靠谱的结果，于是以此为目的给出测序量的估计值；当然少数情况下也会有直接拍脑袋拍出一个值来的疯狂行为，不过嘛，虽说是拍脑袋，但也不是随便拍的，拍脑袋的背景靠的是身后丰富的经验。相对更好一些的估计方式就是直接模拟数据，不过总是用模拟数据还是让人觉得麻烦，最好是能不用花多少时间，也不用做很多的计算就能脱口给出。我想在这里说一下这种情况下我的解法。当然了并不一定完全准确，仅作交流，欢迎各位拍砖。</p>
<p>闲话说完，回到上面的问题，在不通过数据模拟也不拍脑袋的情况下，要如何才能估算出一个合理的值呢？其实在作出任何推断之前我们都应当要先有一个合理的前提假设，或者说是理论依据来作为后续分析的基础。我们都知道短序列测序的一个特点是，在理论情况下位点被覆盖到的深度符合泊松分布（测序没什么问题的话，实际的情形也相差不多），但实际上在这种情况下用正态分布来考虑也是合理的，作为一个估计值，误差也是能够接受的，这是我们的基础。之所以想用正态分布来考虑，是因为正态分布有许多方便于计算的性质。其中一个很有用的法则，就是<code>68-95-99</code>法则，意思就是距离均值一个标准差的区域围起来的面积大约是总体的68%，2个标准差的区域范围的面积是总体的95%，3个标准差区域范围占到了总体的99%，如果你自己想要验证这一法则也并不困难，只需做些积分就能算出来，但这里就不做计算了。如下图，均值用$\mu$表示，标准差用$\sigma$表示。</p>
<p><img src="http://upload.wikimedia.org/wikipedia/commons/8/8c/Standard_deviation_diagram.svg" alt="Normal distribution"></p>
<p>现在事情就很简单了，从图中我们可以看出，只要30x深度的位置在$-2\sigma$以下，那么就能达到理论的要求。要得到这一结果，问题就只剩下一个了，此时我们只需要知道测序深度分布的标准差就能粗略估计出此时我们所需要的最低平均测序深度。虽然这个标准差跟许多因素有关，这里以illumina公司的Hiseq系列测序仪为例子，依照以往基因组重测序的经验，$\sigma$约等于10x。那么，简单算一下，此刻，理论上我们只需要测50x就可以使得基因组上有97.7%的区域其覆盖深度在30x以上了，注意这里不是95%了，因为我们的区域实际上是$[-2\sigma, +\infty)$，而不是$[-2\sigma,+2\sigma]$! 再除掉一些边边角角的误差，50x这个值在这里应当是合理的了.</p>
<p>以上计算都是以正态分布为基础而做出的估计。当然了，如果一定要用泊松分布去推算也可以，只是运算起来会麻烦很多。此外，如果是不同系列或是不同公司的测序仪，&sigma;就不一定是10了。</p>
]]></content>
      
        <categories>
            
            <category> 生物信息 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NGS </tag>
            
            <tag> 测序技术 </tag>
            
            <tag> 估计 </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
