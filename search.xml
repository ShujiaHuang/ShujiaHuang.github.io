<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[人类基因组的Phasing原理是什么？]]></title>
    <url>%2F2018%2F04%2F30%2F2018-04-30-What-Is-Phasing.html</url>
    <content type="text"><![CDATA[什么是Phasing？Phasing，或者说Genotype Phasing，它的中文名有很多：基因定相、基因分型、单倍体分型、单倍体构建等在不同的语境下都有人说过。但不管如何，所谓Phasing就是要把一个二倍体（甚至是多倍体）基因组上的等位基因（或者杂合位点），按照其亲本正确地定位到父亲或者母亲的染色体上，最终使得所有来自同一个亲本的等位基因都能够排列在同一条染色体里面。 现在流行的NGS测序技术，都是把序列打乱混在一起测序的，测完之后，我们是无法直接区分这些序列中哪一个是父源，哪一个是母源的。我们通常都只是检测出基因组上有哪些变异，以及这些变异的碱基组成（纯合、杂合），也就是平时所说的基因型（Genotype）。只有经过Phasing，才能够实现这个区分（图1）。 图1. 变异位点经过Phasing和不经过Phasing的示意图。右上图代表通常的Genotype，右下图代表Phasing之后的情况，实现了亲本的区分。 为什么要Phasing因为Phasing很重要。Phasing的重要性可以分为两个方面。一方面, Phasing与遗传变异的功能诠释密切相关。这体现在遗传咨询师或者科学家需了解基因突变的相位后, 才能更好地判断基因突变是否会产生临床表型。比如在一个基因上发生多个Loss of function variants(LOF)，通常当这些变异出于不同的单倍型时(这称为trans-configuration)，即两个拷贝的姐妹基因都发生了变异, 才会导致基因表达计量(Gene expression dosage)的错误且产生危害。而当它们出于同一个单倍型时(这称为cis-configuration)，因为还有一个正常拷贝的基因（作为备胎）, 基因表达很可能不会发生改变也不会产生危害。 另一方面, Phasing在遗传学研究中也有诸多应用，具体如下： 第一、人群Phasing后形成的单倍型参考序列集(Reference panel)是基因型推断（Imputation）必须的数据材料。而基因型推断（Imputation）是基因型-表型关联分析研究中必不可少的环节。高质量的Reference Panel能提升关联分析的统计功效； 第二、除了Reference Panel的制造需要使用Phasing技术之外，对被研究的对象进行预先Phasing(Pre-phasing)也可以极大地提高基因型推断（Imputation）的准确性； 第三、使用多个位点组成的Haplotype，而不是简单的单位点基因型, 可实现群体遗传历史的推断； 第四、可通过Phased后的家系人群单倍型序列，估算染色体重组率、重组热点等重要遗传参数； 第五、Phasing可用于探测频发突变、选择信号以及基因表达的顺势调控。 Phasing说起来容易，做起来却很难虽然Phasing理解起来并不难，但实现起来却不容易，即使在理论上也是如此。这需要相关的统计学和计算机算法技术，求解的过程往往还是一个NP问题。目前通常采用马尔科夫链蒙特卡洛算法来完成，因此，Phasing算法本身基本都是计算密集型的，做起来也比较耗时间，有时即使是在超算集群中也得跑很长时间。 Phasing的方法有哪些Phasing的方法总结起来主要有三个：家系分型(Related individuals Phasing)、群体LD分型(LD Phasing)和物理分型(Physical Phasing)。下面我就来逐一展开对其方法进行说明。 目前，基因定相最准确的方法是利用家系数据来实现。具体来说，就是除了被研究的这个个体之外，同时对其父亲和母亲的基因组进行测序。有了这三个人的数据之后，就可以很容易地区分出这个样本的两个单倍体。为了便于理解，我打个比方，比如我们知道他/她的基因组某一个位置上的基因型是AB，而父亲的基因型是AA，母亲的基因型是BB，那么我们就可以清楚地知道他/她这个基因上的A是来自于父亲染色体，而B则是属于母亲染色体的，更多的具体情况可以参看下面这个示意图。 图2. 家系数据实现对子/女基因组的Phasing示意图 这个方法的一大优点就是定相（Phasing）的过程 非常直接、简单，不需要进行复杂的统计学计算，就可以准确地实现长距离的定相，并且还能够知道每一个基因型的亲本来源到底是什么，比如在上面的例子中，我们可以知道A和B分别属于父本和母本（如上图）。这个亲本来源的问题对于研究或者治疗许多复杂疾病的意义是十分重大的，比如最近发表在《Science》上的一个研究中发现，影响小孩发生孤独症（也称自闭症）的基因突变中父亲的影响更大，除此之外还有很多母源或者父源性的疾病（这里面其实还涉及到Transmitted和Non-Transmitted Chromosome的问题），这些类型的结果如果没有家系的数据是无法得出的。 对于这个方法来说，家系越庞大它的Phasing效果会越好。万一很不幸我们没能凑齐一家三口（Trio样本）仅有双样本的情况，也不用灰心，虽然效果会差一些，但还是会比没有任何族谱信息的数据要好。 家系Phasing的这个方法虽有很多难以比拟的好处，但也有一些比较明显的缺点。比如，我们为了对这个人进行定相分析，就不得不多测另外两个人的基因组。这一方面大大增加了原有的成本；另一方面则是有些人由于各种各样的原因已经难以获取其双亲的样本数据了；另外，这个方法其实也无法完成对该个体所有变异的完全定相，比如当碰到父、母和子/女都是杂合突变的位点时，就无法区分了。这样的位点虽然在基因组上不是最主要的，但是也大约占到了总变异位点数的13%左右，或者说有大约五分之一的杂合突变位点（注意只是占所有杂合的比例）是这种不可Phasing的状态，详细的分类情况可以参考下表： 表1. 能够被Phasing和不能够被Phasing的SNPs位点分类 LD Phasing是另外一个非常常用的基因定相方法，它是利用群体中大量无血缘关系的个体，依据基本的连锁不平衡(Linkage disequilibrium，LD)遗传原理和相关数学模型，推断群体中每个个体的单倍体的方法，因此它也是计算量最大的一个。 我们知道人这个物种在减数分裂产生生殖细胞的过程中姐妹染色单体会发生重组，这个重组的发生率每代大约是10^-8，虽然很低，但是随着一代接一代不断地繁衍下去，经过足够长的时间之后（比如说 无穷！无穷！无穷！）。 那么从理论上来讲，来自同一祖先的两条染色就会被均匀地重组一个遍。然而，遗憾的是我们现代人还是 Too Youg Too Simple！从最早的证据来看现代人的共同祖先大概起源于15万-19万年前（第三次走出非洲的时间），所以至今我们也不过才经历了6,000-7,600代而已。 这么少的代数也就意味着染色体的重组其实还很有限，因此人类基因组中许多相邻的区域往往都是“黏”在一起遗传下去的，这也就是所谓的存在连锁不平衡的遗传现象，这些“黏”在一起的区块称为“连锁不平衡区块”（LD Block）。 常见变异——那些在人群中频率占比达到5%以上的变异——所存在的连锁不平衡区块(LD Block，Tajima’s D &gt; 0.5)的长度大多集中在50Kbp－60Kbp。并且LD区块的长度在不同的人群中是不同的，比如，非洲人的LD区块就比欧洲人和亚洲人的更短。为什么呢？这是因为非洲人比欧洲人或者亚洲人都要更古老，他们的基因组相比于另外的两个人群发生了更多次数的重组，所以LD区块的长度就更短了。 LD区块的存在就意味着我们可以通过构建相关的数学模型，来把这样的连锁关系求解出来。在开展大规模的基因组研究计划时（如Hapmap、国际千人基因组、Haplotype reference consortium以及各国家的国家基因组计划），通过构建基于隐马尔可夫模型（HMM）等的Phasing算法就可以依据测序数据或者芯片数据，反推出每个个体最有可能的单倍体，完成Phasing。 目前，适合于以上两种Phasing方法（家系和LD Phasing）的最好工具是Beagle和Shapeit。这两个工具都同时包含了用于家系（Related individual Phasing）和LD Phasing的模块。并且都可以用于测序数据和芯片数据，但其中的差别在此不赘述。 回过头来想想LD Phasing方法的缺点是什么？其实通过上面的介绍，我想大家或多或少也注意到了，由于这个方法需要依据群体的信息，那么 它所能够Phasing的精度就会受到群体的制约。通常来说它只能针对群体中常见的变异（如频率在5%以上的变异），在这方面它的效果确实非常棒，很多基因检测公司甚至会把这个作为公司产品的买点，但对于罕见突变和个体特有的变异就不行了。虽然随着人群基数的增大，它所能够Phasing的变异范围也会随着不断增加，比如从只能Phasing 5%以上频率的变异，增大到能够Phasing 1%以上频率的变异，但说到底它还是难以实现对一个个体单倍体的完全定相。 那么，到底该怎么做才能实现完全定相呢？ 正所谓，求人不如求己。由于有了以上的种种限制，于是科学家们就研发了第三类方法：Physical Phasing——「物理定相（或叫物理分型）」。它不需要家系数据，也不借助LD关系，完全依赖自身的测序数据，就可以完成基因的定相。 我们都知道在第二代或者三代测序中，一条read、一对reads或者一个clone上的每一个碱基都必定来自同一个染色体（也就是同一个单倍体）。对于每一个这样的测序片段而言，它本身就是某一个单倍体的一个“局部”，因此现在的问题就变成了要如何把这些一个一个的小”局部“连成一个整体，接出完整的单倍体，从而实现定相，这就是Physical Phasing。而且如果测序序列足够长(比如三代测序数据)，深度足够深，那么它就能够实现个体的完全定相，而且有必要的话还可以同时把这两个单倍体的完整DNA序列组装出来，形成姐妹染色单体，这两个优点是另外两个方法难以比拟的。 长序列可以来自特殊建库，如长度约是40Kbp的Fosmid建库后的测序和组装，或者是华大测序仪的Long Fragment Read(LFR)测序技术，当然也可以是第三代测序的数据。 这也是我（矿工）在华大基因深度负责的第一个研究课题（我的博士论文也是基于该课题）。当时这一块还比较空白，第三代测序技术也还不是很成熟，当时为了获得长序列，我们采用了基于Fosmid构建大长度克隆片段然后进行二代测序并组装的方法（如下图），成果发表在2015年的《Nature Biotechnology》上，我也是共同第一作者。 图3. 基于Fosmid和二代测序技术相结合的de novo Phasing方法 由于我当时已经为课题中的一些细节写过两篇文章，因此这里就不再展开，感兴趣的话你也可以查看本文最后的推荐阅读，这里我只介绍物理定相的基本原理。总的来说，要把局部的小片段连成一个大片段，从而实现Phasing，这个过程要做的好就需要充分借助小片段上的杂合SNPs作为区分的标记。通过每个杂合位点上各个小片段中所含碱基的异同和彼此之间的重叠关系，我们可以把绝大部分的小片段分成两类，然后通过一系列的连接、二分图构建、二分图求解和重新组装等方法，最后就可以把小片段逐步连成大片段，从而构建出单倍体了，如下图（请横着看）。 图4.物理定相示意图 物理定相的方法，往往要求每个片段中都能包含较多的杂合SNPs位点，但由于人类基因组中杂合SNPs位点之间的距离普遍在1.5Kbp左右——还是比较长的，因此测序片段本身就要足够长，这就需要使用包括三代测序技术在内的一些测序方法，因此它的成本会比较高。我目前所知道的在Physica Phasing方面做得比较好的机构中，除了我们自己当时的小组之外，还有德国的马克普朗克研究所（ Max Planck Institute）Margret教授团队和华大基因Brock Peters博士所在的研究组，他们建立了LFR的实验和信息方法。 小结关于Phasing原理的介绍到此就告一段落了，这里在介绍LD Phasing和物理定相的时候没有从数学原理方面去展开，希望可以看起来比较通俗易懂，并且所有的Phasing算法都只对二倍体基因组比较有效，多倍体更加困难。在实际的项目中，我们还是需要根据样本的特点、测序策略和结果预期，有针对性地选择其中的一种或者多种进行组合，从而达到最有效的Phasing效果，评价Phasing效果好坏的指标有两个： 第一，能够被Phasing的变异位点越多越好； 第二，正确被Phasing的位点数占比越高越好。 推荐阅读 华大基因组装迄今最完整人类单倍体水平基因组的重要技术细节（上） 华大基因组装迄今最完整人类单倍体水平基因组的重要技术细节（下） GATK4.0和全基因组数据分析实践（上） GATK4.0和全基因组数据分析实践（下） 该如何自学入门生物信息学 我的微信公众号：解螺旋的矿工 欢迎关注更及时了解更多信息。 这是我的知识星球：解螺旋技术交流圈，是一个与读者朋友们的私人朋友圈，欢迎你的加入。我有9年前沿而完整的生物信息学、NGS领域的工作经历，在该领域发有多篇Nature级别的科学文章。 这是知识星球上第一个真正与基因组学和生物信息学强相关的圈子。我旨在营造一个高质量的组学知识圈和人脉圈，通过提问、彼此分享、交流经验、心得等，更好地学习生物信息学知识提升基因组数据分析和解读的能力。 在这里你可以结识到全国优秀的基因组学和生物信息学专家，同时可以分享你的经验、见解和思考，有问题也可以向我提问或者向圈里的星友们提问。 知识星球邀请链接：「解螺旋技术交流圈」]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[我是解螺旋的矿工，我热爱生命科学]]></title>
    <url>%2F2018%2F03%2F30%2F2018-03-30-I-am-helixminer.html</url>
    <content type="text"><![CDATA[这应该是首次正式地介绍这个公众号，我想通过这篇文章让大家能够更好地了解『解螺旋的矿工』。 其实，我在2015年就注册了这个公众号，当时名字叫：泛基因（fungenomics）——还有一个同名小网站。从时间上来讲应该说注册得还算早，但惭愧的是毫无敏感度，对“媒体”的力量一无所知，自己自然也不重视，再加上平时事情很多，几乎没有更新文章。直到2017年8月份才决定要写好这个公众号（原因下文会说），并在随后不久改成了现在的名字。 这期间我想了很久，『解螺旋的矿工』这个公众号要写些什么？定位是啥？是帮助读者们解决各种各样的生物信息和基因组学问题吗？是要让大家觉得只要来看我的文章，所有问题就能够迎刃而解，做一个哆啦A梦的大口袋吗？还是让读者每次来都有一种茅塞顿开之感，让读者有收获感呢？ 我决定选择后者，因为前者的承诺，我实现不了。可能你今天来了，读了一篇文章，发现能够解决你的问题，觉得很棒，很兴奋，但是这种情况并不确定。也许明天你带着另外的问题来了，结果翻遍所有的文章，却发现不行，搞不定，你就会很失望，就会离开，因为我不能给你带来确定性。反之，如果我选择后者，保证你来了就会有所收获，保证你能够看到别人难以写出来的生物信息和基因组学文章，那这个我能做到，这样确定性就会大很多**。 另外，你会发现这里分享的内容基本都是以组学和生物信息技术为主的，不会有各种业内八卦，或者为了浏览量而跟风写作的热点报道，或者所谓的行业爆点，只会有我的观点。目前主要是NGS，接下来还会有GWAS和群体遗传学，这是我所擅长的领域，在这个过程中我也会分享自己在应用各类技术（包括机器学习、云计算）处理基因组学数据过程中的思路和体会，除了这些之外，你就很难看到其它的内容了。 虽然，我只在业余时间写文章，更新的频率也不高——一周很少超过2篇，然而每一篇我都坚持品质至上。务必真诚，务必提供价值，是我在输出内容过程中的行为准则。 毕竟，这里是我自由分享经验和观点的地方，文章的读者首先是自己，需要先过自己的关。好在我不用刻意为谁做营销，也不必哗众取宠，我尊重自己的时间更尊重每一位读者朋友的时间。我把自己的经验和知识分享出来，就是希望你在读完之余，**觉得时间花得值，能够有所启发，对你的学习和工作能够有所帮助。当然这非常不容易，可以说这个要求是在倒逼我自己进一步成长。 在这里要感谢每一位关注和支持我的读者，感谢你们的耐心和包容。古人说，“文章千古事，得失寸心知”，我也看过很多生信类的文章，不少写的实在是很差劲，从很大程度上来说我决定要写好这个公众号也是因为受了那些内容的刺激，心中不愤——我有更好的！对于知识，对于技术，对于文字，我还是有我自己的敬畏和坚持。 所幸，这些付出逐渐有了回报，得到了越来越多的认可！下面的这些溢美之词是读者朋友们的信任（后台较多实在无法一一贴出），它们不断激励我精益求精地写好每一篇文章，回答好每一个问题，认真分享好基因组学知识和技术。 最后，分享一段我很喜欢的话，来自@韩建： 我佩服那些干着的人，蔑视那些只说不干的人。不实干就没有发言权；也只有实干者才有犯错和改错的特权。是实干还是虚干？你可以去看结果，也可以去看人品。在开口评论他人以前，最好看看自己都干了什么。评论自己的权利可以从书本里面得到，评论别人的权利要靠实干的血汗换得。而许多真正实干一辈子的人都已经没有精力去评论他人了，因为干了一辈子的人都会犯很多的错误，于是对他人在干中出现的错误就有了更多的理解。 推荐阅读 GATK4.0和全基因组数据分析实践（上） GATK4.0和全基因组数据分析实践（下） 该如何自学入门生物信息学 从零开始完整学习全基因组测序数据分析：第4节 构建WGS主流程 从零开始完整学习全基因组测序数据分析：第3节 数据质控 从零开始完整学习全基因组测序数据分析：第1节 测序技术 这是我的知识星球：解螺旋技术交流圈，是一个与读者朋友们的私人朋友圈，是付费的，虽然欢迎你的加入，但不支持退费因此加入前还请思量好，这是邀请链接，可以微信扫描。 我擅长人方向的基因组学和生物信息学，算起来在这一块已有接近9年前沿而完整的学习和工作经。这个交流圈也是知识星球上第一个真正与基因组学和生物信息学强相关的圈子，目标是共同营造高质量的组学知识圈和人脉圈，让圈子里的人都能够受益。在这里你可以结识到全国优秀的基因组学和生物信息学专家，同时可以分享你的经验、见解和思考，有问题也可以向我提问或者向圈里的星友们提问。 知识星球邀请链接：「解螺旋技术交流圈」]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[GATK4.0和全基因组数据分析实践（下）]]></title>
    <url>%2F2018%2F03%2F23%2F2018-03-23-WGS-Best-Practics-2.html</url>
    <content type="text"><![CDATA[前言在上一篇文章中我已经用例子仔细跟大家分享了WGS从原始数据到变异数据（Fastq-&gt;VCF）的具体执行过程。那么，在这一篇文章里，我们就来好好谈谈后续非常重要的一个环节——也是本次实践分析的最后一个部分—— 变异的质控。 什么是质控？我们不妨先给它下个定义：质控的含义和目的是指通过一定的标准，最大可能地剔除假阳性的结果，并尽可能地保留最多的正确数据。有了这么一个定义之后，我们也就能够更加清晰地知道接下来该做些什么了。 在上次的文章里我已经说到在GATK HaplotypeCaller之后，首选的质控方案是GATK VQSR，它通过机器学习的方法利用多个不同的数据特征训练一个模型（高斯混合模型）对变异数据进行质控，然而不幸的是使用VQSR需要具备以下两个条件： 第一，需要一个精心准备的已知变异集，它将作为训练质控模型的真集。比如，对于我们人来说，就有Hapmap、OMNI，1000G和dbsnp等这些国际性项目的数据，这些可以作为高质量的已知变异集。GATK的bundle主要就是对这四个数据集做了精心的处理和选择，然后把它们作为VQSR时的真集位点。这里我强调一个地方：是真集的『位点』而不是真集的『数据』！还请大家多多注意。因为，VQSR并不是用这些变异集里的数据来训练的，而是用我们自己的变异数据。这个对于刚接WGS的同学来说特别容易搞混，不要因为VQSR中用了那四份变异数据，就以为是用它们的数据来训练模型。 实际上，这些已知变异集的意义是 告诉我们群体中哪些位点存在着变异，如果在其他人的数据里能观察到落入这个集合中的变异位点，那么这些被已知集包括的变异就有很大的可能是正确的。也就是说，我们可以从数据中筛选出那些和真集『位点』相同的变异，把它们当作是真实的变异结果。接着，进行VQSR的时候，程序就可以用这个筛选出来的数据作为真集数据来训练，并构造模型了。 关于VQSR的内在原理，前不久在我的知识星球中我做过简单的回答（下图），这里就不展开了，感兴趣的同学看下图的内容基本上也是足够的，虽然不详细，但应该可以帮你建立一个关于VQSR的基本认识。对于希望深入理解算法细节的同学来说，我的建议是直接阅读GATK这一部分的代码。但要注意，类似这样的过滤算法实际上还可以用很多不同的机器学习算法来解决，比如SVM，或者用深度学习来构造这个质控模型也都是OK的。 第二，要求新检测的结果中有足够多的变异，不然VQSR在进行模型训练的时候会因为可用的变异位点数目不足而无法进行。 由于条件1的限制，会导致很多非人的物种在完成变异检测之后没法使用GATK VQSR的方法进行质控。而由于条件2，也常常导致一些小panel甚至外显子测序，由于最后的变异位点不够，也无法使用VQSR。这个时候，我们就不得不选择硬过滤的方式来质控了。 那什么叫做硬过滤呢？所谓硬过滤其实就是通过人为设定一个或者若干个指标阈值（也可以叫数据特征值），然后把所有不满足阈值的变异位点采用一刀切掉的方法。 那么如何执行硬过滤？首先，需要我们确定该用哪些指标来评价变异的好坏。这个非常重要，选择对了事半功倍，选得不合理，过滤的结果有时还不如不过滤的。如果把这个问题放在从前，我们需要做比较多的尝试才能确定一些合适的指标，但现在就方便很多了，可以直接使用GATK VQSR所用的指标——毕竟这些指标都是经过精挑细选的。我想这应该不难理解，既然VQSR就是用这些指标来训练质控模型的，那么它们就可以在一定程度上描述每个变异的质量，我们用这些指标设置对应的阈值来进行硬过滤也将是合理的。VQSR使用的数据指标有6个（这些指标都在VCF文件的INFO域中，如果不是GATK得到的变异，可能会有所不同，但知道它们的含义之后也是可以自己计算的），分别是： QualByDepth（QD） FisherStrand (FS) StrandOddsRatio (SOR) RMSMappingQuality (MQ) MappingQualityRankSumTest (MQRankSum) ReadPosRankSumTest (ReadPosRankSum) 指标有了，那么阈值应该设置为多少？下面我想先给出一个硬过滤的例子，然后再逐个来对其进行分析，以便大家能够更好地理解变异质控的思路。值得注意的是不同的数据，有不同的情况，它的阈值有时是不同的。不过不用担心，当你掌握了如何做的思路之后完全有能力根据具体的情况举一反三。 执行硬过滤首先是硬过滤的例子，这个过程我都用最新的GATK来完成。GATK 4.0中有一个专门的VariantFiltration模块（继承自GATK 3.x），它可以很方便地帮我们完成这个事情。不过，过滤的时候，需要分SNP和Indel这两个不同的变异类型来进行，它们有些阈值是不同的，需要区别对待。在下面的例子里，我们还是用上一节中最后得到的变异数据（E_coli_K12.vcf.gz）为例子，这是具体的执行命令： 12345678910111213141516171819202122232425262728293031323334# 使用SelectVariants，选出SNPtime /Tools/common/bin/gatk/4.0.1.2/gatk SelectVariants \ -select-type SNP \ -V ../output/E.coli/E_coli_K12.vcf.gz \ -O ../output/E.coli/E_coli_K12.snp.vcf.gz# 为SNP作硬过滤time /Tools/common/bin/gatk/4.0.1.2/gatk VariantFiltration \ -V ../output/E.coli/E_coli_K12.snp.vcf.gz \ --filter-expression "QD &lt; 2.0 || MQ &lt; 40.0 || FS &gt; 60.0 || SOR &gt; 3.0 || MQRankSum &lt; -12.5 || ReadPosRankSum &lt; -8.0" \ --filter-name "Filter" \ -O ../output/E.coli/E_coli_K12.snp.filter.vcf.gz# 使用SelectVariants，选出Indeltime /Tools/common/bin/gatk/4.0.1.2/gatk SelectVariants \ -select-type INDEL \ -V ../output/E.coli/E_coli_K12.vcf.gz \ -O ../output/E.coli/E_coli_K12.indel.vcf.gz# 为Indel作过滤time /Tools/common/bin/gatk/4.0.1.2/gatk VariantFiltration \ -V ../output/E.coli/E_coli_K12.indel.vcf.gz \ --filter-expression "QD &lt; 2.0 || FS &gt; 200.0 || SOR &gt; 10.0 || MQRankSum &lt; -12.5 || ReadPosRankSum &lt; -8.0" \ --filter-name "Filter" \ -O ../output/E.coli/E_coli_K12.indel.filter.vcf.gz# 重新合并过滤后的SNP和Indeltime /Tools/common/bin/gatk/4.0.1.2/gatk MergeVcfs \ -I ../output/E.coli/E_coli_K12.snp.filter.vcf.gz \ -I ../output/E.coli/E_coli_K12.indel.filter.vcf.gz \ -O ../output/E.coli/E_coli_K12.filter.vcf.gz# 删除无用中间文件rm -f ../output/E.coli/E_coli_K12.snp.vcf.gz* ../output/E.coli/E_coli_K12.snp.filter.vcf.gz* ../output/E.coli/E_coli_K12.indel.vcf.gz* ../output/E.coli/E_coli_K12.indel.filter.vcf.gz* 与上一篇文章的目录逻辑一样，我们把这些shell命令都写入到bin目录下一个名为“variant_filtration.sh”的文件中，然后运行它。最后，只要符合了上面任意一个阈值的变异都会被设置为“Filter”，剩下的会被认为是正常的变异，并标记为“PASS”。流程的最后，我们需要把分开质控的SNP和Indel结果重新合并在一起，然后再把那些不必要的中间文件删除掉。 在具体的项目中，你如果需要使用硬过滤的策略，这个例子中的参数可以作为参考，特别是对于高深度数据而言。接下来我结合GATK所提供的资料与大家分享如何理解这些指标以及得出这些阈值的思路。 如何理解硬过滤的指标和阈值的计算考虑到SNP和Indel在判断指标和阈值方面的思路是一致的，因此没必要重复说。所以，下面我只以SNP为例子，告诉大家设定阈值的思路。强调一下，为了更具有通用价值，这些阈值是借用NA12878（来自GIAB）的高深度数据进行计算获得的，所以如果你的数据（或者物种）相对比较特殊（不是哺乳动物），那么就不建议直接套用了，但可以依照类似的思路去寻找新阈值。 QualByDepth（QD）QD是变异质量值（Quality）除以覆盖深度（Depth）得到的比值。这里的变异质量值就是VCF中QUAL的值——用来衡量变异的可靠程度，这里的覆盖深度是这个位点上所有 含有变异碱基的样本的覆盖深度之和，通俗一点说，就是这个值可以通过累加每个含变异的样本（GT为非0/0的样本）的覆盖深度（VCF中每个样本里面的DP）而得到。举个例子： 11 1429249 . C T 1044.77 . . GT:AD:DP:GQ:PL 0/1:48,15:63:99:311,0,1644 0/0:47,0:47:99:392,0,0 1/1:0,76:76:99:3010,228,0 这个位点是1:1429249，VCF格式，但我把FILTER和INFO的信息省略了，它的变异质量值QUAL=1044.77。我们可以从中看到一共有三个样本，其中一个是杂合变异（GT=0/1），一个纯合的非变异（GT=0/0），最后一个是纯合的变异（GT=1/1）。每个样本的覆盖深度都在其各自的DP域上，分别是：63，47和76。按照定义，这个位点的QD值就应该等于质量值除以另外两个含有变异的样本的深度之和（排除中间GT=0/0这个不含变异的样本），也就是: 1QD = 1044.77 / (63+76) = 7.516 QD这个值描述的实际上就是单位深度的变异质量值，也可以理解为是对变异质量值的一个归一化，QD越高一般来说变异的可信度也越高。在质控的时候，相比于QUAL或者DP（深度）来说，QD是一个更加合理的值。因为我们知道，原始的变异质量值实际上与覆盖的read数目是密切相关的，深度越高的位点QUAL一般都是越高的，而任何一个测序数据，都不可避免地会存在局部深度不均的情况，如果直接使用QUAL或者DP都会很容易因为覆盖深度的差异而带来有偏的质控结果。 在上面『执行硬过滤』的例子里面，我们看到认为好的SNP变异，QD的值不能低于2，但 问题是为什么是2，而不是3或者其它数值呢？ 要回答这个问题，我们可以通过利用NA12878 VQSR质控之后的变异数据和原始的变异数据来进行比较，并把它说明白。 首先，我们可以先把所有变异位点的QD值都提取出来，然后画一个密度分布图（Y轴代表的是对应QD值所占总数的比例，而不是个数），看看QD值的总体分布情况（如下图，来自NA12878的数据）。 从这个图里，我们可以看到QD的范围主要集中在0~40之间。同时，我们可以明显地看到有两个峰值（QD=12和QD=32）。这两个峰所反映的恰恰是杂合变异和纯合变异的QD值所集中的地方。这里大家可以思考一下，哪一个是代表杂合变异的峰，哪一个是代表纯合变异的峰呢？ 回答是，第一个峰（QD=12）代表杂合，而第二峰（QD=32）代表纯合，为什么呢？因为对于纯合变异来说，贡献于质量值的read是杂合变异的两倍，同样深度的情况下，QD会更大。对于大多数的高深度测序数据来说，QD的分布和上面的情况差不多，因此这个分布具有一定的代表性。 接着，我们同时画出VQSR之后所有 可信变异（FILTER=Pass）和 不可信变异的QD分布图，如下，浅绿色代表可信变异的QD分布图，浅红色代表不可信变异的QD分布图。 你可以看到，大多数Fail的变异，都集中在左边的低QD区域，而且红波峰恰好是QD=2的地方，这就是为什么硬过滤时设置QD&gt;2的原因了。 可是在上面的图里，我想你也看到了，有很多Fail的变异它的QD还是很高的，有些甚至高于30，通过这样的硬过滤参数所得到的结果中就会包含这部分本该要过滤掉的坏变异；而同样的，在低QD（&lt;2）区域其实也有一些是好的变异，但是却被过滤掉了。这其实也是硬过滤的一大弊端，它不会像VQSR那样，通过多个不同维度的数据构造合适的高维分类模型，从而能够更准确地区分好和坏的变异，而仅仅是一刀切。 当你理解了上面有关QD的计算和阈值选择的过程之后，要弄懂后面的指标和阈值也就容易了，因为用的也都是同样的思路。 FisherStrand（FS）FS是一个通过Fisher检验的p-value转换而来的值，它要描述的是测序或者比对时对于只含有变异的read以及只含有参考序列碱基的read是否存在着明显的正负链特异性（Strand bias，或者说是差异性）。这个差异反应了测序过程不够随机，或者是比对算法在基因组的某些区域存在一定的选择偏向。如果测序过程是随机的，比对是没问题的，那么不管read是否含有变异，以及是否来自基因组的正链或者负链，只要是真实的它们就都应该是比较均匀的，也就是说，不会出现链特异的比对结果，FS应该接近于零。 这里多说一句，在VCF的INFO中有时除了FS之外，有时你还会看到SB或者SOR。它们实际上是从不同的层面对链特异的现象进行描述。只不过SB给出的是原始各链比对数目，而FS则是对这些数据做了精确Fisher检验；SOR原理和FS类似，但是用了不同的统计检验方法计算差异程度，它更适合于高覆盖度的数据。 与QD一样，我们先来看一下质控前所有变异的FS总体密度分布图（如下）。很明显与QD相比，FS的范围更加的大，从0到好几百的都有。不过从图中也可以看出，绝大部分的变异还是在100以下的。 下面这一个图则是经过VQSR之后，画出来的FS分布图。跟上面的QD一样，浅绿色代表好变异，浅红色代表坏变异。我们可以看到，大部分好变异的FS都集中在0~10之间，而且坏变异的峰值在60左右的位置上，因此过滤的时候，我们把FS设置为大于60。其实设置这么高的一个阈值是比较激进的（留下很多假变异），但是从图中你也可以看到，不过设置得多低，我们总会保留下很多假的变异，既然如此我们就干脆选择尽可能保留更多好的变异，然后祈祷可以通过『执行硬过滤』里其他的阈值来过滤掉那些无法通过FS过滤的假变异。 StrandOddsRatio（SOR）关于SOR在上面讲到FS的时候，我就在注释里提及过了。它同样是对链特异（Strand bias）的一种描述，但是从上面我们也可以看到FS在硬过滤的时候并不是非常给力，而且由于很多时候read在外显子区域末端的覆盖存在着一定的链特异（这个区域的现象其实是正常的），往往只有一个方向的read，这个时候该区域中如果有变异位点的话，那么FS通常会给出很差的分值，这时SOR就能够起到比较好的校正作用了。计算SOR所用的统计检验方法也与FS不同，它用的是symmetric odds ratio test，数据是一个2×2的列联表（如下），公式也十分简单，我把公式进行了简单的展开，从中可以清楚地看出，它考虑的其实就是ALT和REF这两个碱基的read覆盖方向的比例是否有偏，如果完全无偏，那么应该等于1。 1sor = (ref_fwd/ref_rev) / (alt_fwd/alt_rev) = (ref_fwd * alt_rev) / (ref_rev * alt_fwd) OK，那么同样的，我们先看一下这个值总体的密度分布情况（如下）。总的来说，这个分布明显集中在0~3之间，这也和我们的预期比较一致。不过也有比较明显的长尾现象，这个时候我们也没必要定下太过明确的阈值预期，先看VQSR的分布结果。 下面这个图就是在VQSR之后，区分了好和坏变异之后，SOR的密度分布。很明显，好的变异基本就在1附近。结合这个分布图，我们在上面的例子里把它的阈值定为3基本上也不会过损失好的变异了，虽然单靠这个阈值还是会保留下不少假的变异，但是至少不合理的长尾部分可以被砍掉。 RMSMappingQuality（MQ）MQ这个值是所有比对至该位点上的read的比对质量值的均方根（先平方、再平均、然后开方，如下公式）。 它和平均值相比更能够准确地描述比对质量值的离散程度。而且有意思的是，如果我们的比对工具用的是bwa mem，那么按照它的算法，对于一个好的变异位点，我们可以预期，它的MQ值将等于60。 下面是所有未过滤的变异位点上MQ的密度分布图。基本上就只在60的地方存在一个很瘦很高的峰。可以说这是目前为止这几个指标中图形最为规则的了，在这个图上，我们甚至就可以直接定出MQ的阈值了，比如所有小于50的就可以过滤掉了。 但是，理性告诉我们还是要看一下VQSR的对比结果（下图）。 你会发现似乎所有好的变异都紧紧集中在60旁边了，其它地方就都是假的变异了，所以MQ的阈值设置为50也是合理的。但是同样要注意到的地方是，60这个范围实际上依然有假的变异位点在那里，我们把这个区域放大来看，如下图，这里你就会发现其实假变异的密度分布图也覆盖到60这个范围了。 考虑到篇幅的问题，接下来MappingQualityRankSumTest（MQRankSum）和ReadPOSRankSumTest（ReadPOSRankSum）的阈值设定原理，我不打算再细说下去了 ，思路和上面的4个是完全一样的。都是通过比较VQSR之后的密度分布图，最后确定了硬过滤的阈值。 但请不要以为这只是适用于GATK得到的变异，实际上，只要我们弄懂了这些指标选择的原因和过滤的思路，那么通过任何其他的变异检测工具也是依旧可以适用的，区别就在于GATK帮我们把这些要用的指标算好了。 同样地，这些指标也不是一成不变的，可以根据实际的情况换成其他，或者我们自己重新计算。 Ti/Tv处于合理的范围Ti/Tv的值是物种在与自然相互作用和演化过程中在基因组上留下来的一个统计标记，在物种中这个值具有一定的稳定性。因此，一般来说，在完成了以上的质控之后，还会看一下这些变异位点Ti/Tv的值是多少，以此来进一步确定结果的可靠程度。 Ti（Transition）指的是嘌呤转嘌呤，或者嘧啶转嘧啶的变异位点数目，即A&lt;-&gt;G或C&lt;-&gt;T；Tv（Transversion）指的则是嘌呤和嘧啶互转的变异位点数目，即A&lt;-&gt;C，A&lt;-&gt;T，G&lt;-&gt;C和G&lt;-&gt;T。（如下图） 另外，在哺乳动物基因组上C-&gt;T的转换比较多，这是因为基因组上的胞嘧啶C在甲基化的修饰下容易发生C-&gt;T的转变。 说了这么多，Ti/Tv的比值应该是多少才是正常的呢？如果没有 选择压力的存在，Ti/Tv将等于0.5，因为从概率上讲Tv将是Ti的两倍。但现实当然不是这样的，比如对于人来说，全基因组正常的Ti/Tv在2.1左右，而外显子区域是3.0左右，新发的变异（Novel variants）则在1.5左右。 最后多说一句，Ti/Tv是一个被动指标，它是对最后质控结果的一个反应，我们是不能够在一开始的时候使用这个值来进行变异过滤的。 小结虽然本文一直在谈论的是如何做好硬过滤，但不管我们的指标和阈值设置的多么完美，硬过滤的硬伤都是一刀切，它并不会根据多个维度的数据自动设计出更加合理的过滤模式。硬过滤作为一个简单粗暴的方法，不到不得已的时候不推荐使用，即使使用了，也一定要明白每一个过滤指标和阈值都意味着什么。 最后，我也希望通过这一篇文章能够完整地为你呈现一个变异质控的思路。 本文首发于我的个人公众号：解螺旋的矿工，欢迎扫码关注，更及时了解更多基因组学信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>WGS</tag>
        <tag>流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Indel，我该往左还是该往右]]></title>
    <url>%2F2018%2F03%2F01%2F2018-03-01-LeftAlignment-Or-RightAlignment-For-Indel.html</url>
    <content type="text"><![CDATA[实质上，两种做法都可以。但是在NGS数据分析中，约定俗成的做法是：左移（left alignment）！然而也有一种情况是例外，当变异使用的是HGVS规则命名的时候，它们却是右移（right alignment）！ 在探讨这个问题之前，我们来理解一下为什么Indel需要左移或者右移。 首先，会碰到这个问题的场景是我们需要比较多个不同VCF数据集中Indel结果是否一致的时候。比如，需要把自己项目的VCF数据和标准集比较——这个标准集可以是1000 Genomes Variants（该公开数据集已经左移），也可以是GIAB（Genome In a Bottle）的数据集，也可以是医学检测报证做质评的标准集等等。这个时候我们需要统一Indel的坐标（特别是Deletion的坐标），这是需要「移」目的。 然后，我们来理解一下为什么会出现「移」这个问题的原因。主要有两个： 第一，宏观上，基因组本身存在重复性序列，比如在人类基因组中重复性序列占比约50%； 第二，微观上，基因组的局部区域存在很多相似序列和短串联重复序列。 在这些地方——特别是原因二，序列比对往往会懵圈，不知道应该在哪个位置上开GAP——Indel（主要是Deletion）在比对上所体现出来的现象就是GAP，才合适。由此，导致的结果就是比对时开GAP的位置每！一！次！都！会！有！差！异！也就是说即使是同一个数据，多次比对，结果也不同。 比如下图，尽管这个区域中实际上只有一个Deletion，但是三次比对的结果，Deletion的断点都不一样，但是从比对结果看，它们的罚分却一模一样，根本无法区分！ 理解了上面这一点之后，我们就知道不能在不同数据集中随便对Indel进行直接比较了。我们需要先对这些变异进行规范化的处理（一般指左移），确保所有的Indel的断点都是开在同一个方向上的，用NGS的术语，管这个做法叫：Variant Normalization。 那么，有什么工具可以干这个事情吗？有很多。并且进行左移的合适时机有两个，都有不同的工具可以用。 第一个，在call变异之前，对比对文件SAM/BAM/CRAM直接进行左移，能用的工具很多，比如GATK4.0中的LeftAlignIndels模块。 第二个，已经有了VCF变异集，这个时候GATK中就没有合适的模块来处理了，推荐使用vt——一个很不错的变异处理工具，它有一个vt normalize模块，直接就可以对VCF中的Indel断点进行左移。 123456789101112131415161718description : normalizes variants in a VCF file.usage : vt normalize [options] &lt;in.vcf&gt;options : -o output VCF file [-] -d debug [false] -q do not print options and summary [false] -m warns but does not exit when REF is inconsistent with masked reference sequence for non SNPs. This overides the -n option [false] -n warns but does not exit when REF is inconsistent with reference sequence for non SNPs [false] -f filter expression [] -w window size for local sorting of variants [10000] -I file containing list of intervals [] -i intervals [] -r reference sequence fasta file [] -? displays help 那。。。如果要右移呢？这是一个难得一见的操作，github有一个程序可以用，这里。 或许你也好奇，既然右移很少见，那么，为什么会有HGVS这个例外呢？ 我也不知道真正的原因，有一种说法是，搞NGS的人和搞医学检测的人对变异的理解有差异，导致各自的处理方式不同，且没有相互沟通过，等发现差异的时候已经迟了。但我不知道这个说法是否属实，姑且听之任之吧。 本文首发于我的个人公众号：解螺旋的矿工，欢迎扫码关注，更及时了解更多基因组学信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我如何看待用1公斤DNA存储全球信息这个事情]]></title>
    <url>%2F2018%2F02%2F24%2F2018-02-24-What-Do-I-Think-About-DNA-storage.html</url>
    <content type="text"><![CDATA[最近发现关于DNA存储的文章刷屏了，源自于今年2月19号华盛顿大学和微软研究院合作在《Nature biotechnology》上发表的一篇有关DNA存储的研究成果。对此我想发表一点自己的观点，受限于我的认知，观点不一定正确。 诚然，进入21世纪之后，这个世界的数据增长速度太快了，数据量级越来越大，按照现有发展速度传统硅基存储介质是否还能撑住，就成为了许多人关心的一个问题，大家都在探讨是否会有枯竭的那一天，如果枯竭了我们还能用什么东西来存储我们的数据。于是存储生命遗传密码的介质——DNA就成了一个非常有希望的选项。 在谈论我们的话题之前，先来了解一下DNA是如何存储数据的原理本身并不复杂。 我们知道，电脑上存储的数据都是 依据电压的高和低代表0和1来表示的，每一个数字、字符和标点符号都由唯一的一串01组合来构成。比如小写字母「e」的代码是：01100101，因此，任何数字化的内容（视频，图片，文字）本质上都只是一串串的0和1而已。 DNA存储的原理示意图，首先把英文字母转变成对应的01串，然后把这个0和1的数据串转变成由碱基A、C、G、T表示的DNA序列；编码的时候就是合成这个序列，解码的时候测序解读（图片来自Science） 那么，DNA的存储原理实际上就是把原本这些用0和1来表示的内容，换成用碱基：A，C，G，T来表示，这是一个从数字信号到化学信号的过程。而且由于碱基有四个，相比起原本的0和1，我们可以用来多表示两个状态，比如，我们可以假设用A代表00，C代表01，G代表10，T代表11。一个本来要用8bit代表的字符用DNA编码的话，只需要用4个化学碱基，比如上面的小写字符「e」编码成为DNA序列就是：CGCC。 下图是哈佛大学医学院两年前做的一个事情，他们第一次利用这样的技术把这一张“奔跑的骏马”的Gif放进了活大肠杆菌的DNA里，而且还能重新测序并解码出来。 原始影像（左）和从DNA中提取还原的gif（右），除了部分稍有模糊，准确度达90%左右 2016年的时候，华盛顿大学和微软研究院的团队（本次NBT的成果的团队），他们更进了一步，把莎士比亚的十四行诗、马丁•路德金的演讲原声、医学论文等资料共计739KB的数据编码成了DNA序列，并存储起来，这个技术以此为标记取得了巨大的进步。 DNA存储结构和磁盘不同，它存储的密度极高，1克的DNA就能够存下天量的信息，如果要存下当前全世界的所有数据，更是只需要1千克左右的DNA就足够了！不需要成千上万个阿里巴巴或者AWS的数据中心，看起来还更加经济实惠，貌似一切都很美好，但是，凡事最怕但是。 目前DNA存储要发展成为真正具有实际应用价值的东西，至少还需要解决以下几个问题： 合成成本高DNA要存储信息，首先要做的就是依据信息合成DNA序列。那么现在的合成成本是多少呢？大约0.5-1.0美元一个碱基！也就是说存储2bit（一个碱基）的数据需要花费大约5-10块钱人民币。按照目前的信息存储技术，一般是8bit为一个字节（Byte），2个字节（Byte）才代表一个字符——也就是说8个碱基可以编码一个字符，那么你看看，要存储200MB的数据需要花费100百万-200百万美刀的巨资！而200MB的大小的文件还不够一个长一点的短视频大啊！更何况现在动不动就几个GB的电影呢。因此，碱基合成的成本是第一个需要解决的难题，如果成本无法降低一百万倍，那么无法进入实用环节，而如果不能降低几亿倍甚至几十亿倍，那么我认为这个技术将很难被大规模使用。 合成速度慢这个问题可能更要命。我们现在磁盘的存储速度是多快呢？磁盘的读写毕竟是电磁信号，信息状态的改变是以光的速度在发生的——当然磁盘在读写数据的时候需要进行非常多的定位、查询、比较、校验等一系列复杂的操作，因此远低于光速。然而即便如此，目前普通的SSD硬盘读写速度也有300MB/s-500MB/s，差一些的高速硬盘也在100MB/s左右！而DNA的合成速度有多快呢？DNA的合成依赖于一系列的化学反应，大肠杆菌的DNA（合成）复制速度大约是1000碱基/秒，看起来很快了，但它的速度在电磁面前根本不值一提，我们可以算一下合成200MB的数据需要多久呢？200 1024 1024 * 8 / 1000 /86400 = 19 天！也就是说现在磁盘1秒钟写入的数据，我们大约需要花差不多三周的时间才能完成！ 这是什么概念？据统计截至2017年全球数据大约有16 ZB（仅指数字化的数据），那么假设我们要把这个量级的数据存到DNA中，大概要花多长时间？我斗胆计算了一下，发现竟然需要40亿年！40亿年啊，同志们，地球才多老啊？这还是在不考虑数据校验的状态下。 更有甚者，据说到了2020年，全球数据更是要达到惊人的44ZB的量级！当然，上面的结果是在单个反应下的合成速度，事实上，我们可以让全世界成千上万的实验室或者机构一起来做，同时随着技术的发展可以设计出DNA大规模并行合成技术，就如同大规模并行测序一般，通过工程上的规模化弥补先天的缺陷，将速度提高几百万到几亿倍，但这对合成的技术就提出了更高的要求，因为这个过程不可避免的会导致我们放弃数据原有的连续性，那么该如何把这些打散的数据在读取的时候重新正确地组合到一起也将成一个重要的问题。除此之外，还有实时合成呢？ 数据读取无法实时DNA存储的数据要读取出来目前是通过测序这条路。虽然相比于DNA合成，测序的问题小了很多。按照当前最新的测序技术，一台NovaSeq测序仪基本上能够在两天的时间内完成3Tb-6Tb数据的解码。成本相比于DNA合成也基本低了一百万倍左右。即便如此，真要实用，依然有许多问题必须解决。比如我们在看电影的时候，你不会真的希望对着一台测序仪看吧，另外刷微信、微博、头条、知乎等的操作是多么频繁和快速，DNA解码要如何做到实时并且保障信息的可逆回滚，挑战不小啊（中间通过磁盘来缓存吗？）。 数据随机读取仍需进一步解决所谓随机读取数据的意思就是我想打开哪一份文件就打开哪一份，并且我想读取其中的哪一段就读取哪一段，而且这个操作必须要在很短的时间内实现。这对于存储在DNA中的数据文件来说要如何才能够做到？ 2月19日，华盛顿大学和微软研究院合作发表在《Nature biotechnology》上的这篇文章《Random access in large-scale DNA data storage》，就是为了解决这一个问题。它最大的突破是设计了一种办法来解决这个随机读取的问题——文章的名字也能够看出来。他们把35份相互独立的数据文件（大小约200MB）合成为DNA序列存储起来，并且精心设计特定的引物，标记每一个文件在DNA序列上的地址（如同硬盘的存储路径一样）。这个时候，当我们要重新读取这些数据的时候能够按照需要快速跳到特定某份文件的位置上进行测读。比如我们想要获取第10份文件上的内容，如果放在从前，我们只能全部测序了才能得到，但是借助这个技术，我们可以直接跳到这份文件所在的位置上，把它测读出来。 虽然这个技术已经做到了这一步，应该说取得了不小的进步，但也应该清晰地认识到它距离真正应用还有不小的距离。另外，依我愚见，这个方案也还有不完美的地方： 第一，定位精细度不够，虽然可以定位到特定的文件，但还不能够实现在文件内部的随意跳转，更加不能检索； 第二，效率还是太低了，而且为了保证信息的准确，还得进行较高深度的测序，并需要进行序列组装。虽然说测序速度在提高，但若做不到实时，应用价值依然是大打折扣； 第三，灵活性有待商酌，引物需要精心设计，这次是35份，如果是350份或者更多呢？当我们合成了很多份这样的序列之后，如何保存才能保证测读的时候，不会因为相同引物的问题而导致测读不准确？ DNA存储技术会颠覆现有的计算机存储技术吗？我认为不会，即便DNA存储技术成熟了，两者也将共存很长时间。DNA存和读的效率远不及磁盘的速度，这是自然原理所决定的，不是一时半会能够解决的，但它对数据保存的耐久性却很好。因此，DNA存储更可能的是替代磁带存储，把不需要经常使用的「冷」数据归档保存，把重要的数据进行冷存备份，而且鉴于DNA本身体积小、几乎不耗电的特点、保存也方便，确实可以节省很多的社会资源。 小结当然，我不是DNA合成领域的专家，对该领域的了解也不深入，写这一篇文章更加不是为了抨击DNA存储的成果，相反，我非常认同DNA存储技术的发展，更希望看到它在未来的应用。但我也很谨慎，我会想这是否真的是最好的方法。我们说DNA对数据存储的密度远高于现在的磁盘，但是回头想想，如果我们能够操纵原子的量子状态，利用原子的量子状态（比如：自旋）存储数据那样密度岂不是更加高?而且还不会有速度限制上的问题？另外，我也看不惯有些媒体的盲目夸大，甚至罔顾事实，一旦发现一个新东西就总觉得它是万能的，总认为它将如何“颠覆”一切等诸如此类的言论。过分的夸大甚至曲解对于科学技术的发展不是好事，也不能引导公众对其做出客观的判断。技术的发展有其自身的规律性，该到它颠覆一切的时候，不用说也会自然发生，现在就耐心看它长大。 本文首发于我的个人公众号：解螺旋的矿工，欢迎扫码关注，更及时了解更多基因组学信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>合成生物学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GATK4.0和全基因组数据分析实践（上）]]></title>
    <url>%2F2018%2F02%2F20%2F2018-02-20-WGS-Best-Practics.html</url>
    <content type="text"><![CDATA[前言在前面的一系列WGS文章中，我讲述了很多基因数据分析的来龙去脉，虽然许多同学觉得很有帮助，但是却缺了一个重要的环节——没有提供实际可用的数据来实战完成具体的流程，不能得到直观的体会。许多读者也纷纷在后台留言反馈这个问题，特别是在我写了如何入门生物信息学的文章之后情况尤甚。所以我决定要再写一篇文章来解决这个问题，但碰巧今年年末事情稍多，在写作的过程中也曾多次中断，直至今天才完成，各位久等了。本次文章分为上下两篇，这是第一篇，都是WGS第四节的延伸，本意是结合具体的数据，让更多的人能够 更好地理解整个WGS数据的分析和处理过程，我也结合自身的工作经验给出一些做项目过程中的建议，以作参考，希望能够对你有更多的帮助。另外，接下来我将系统写一个关于全基因组关联分析（GWAS）的文章，同时还会有更多全面而且紧扣前沿的技术文章分享出来。 那么，事不宜迟我们马上开始。考虑到实际的数据分析环境，我们只在Linux命令行终端（Terminal）中进行，执行步骤都写在shell，因此不会有窗口式的操作（使用Mac OS的同学可以使用Mac自带的Terminal，与Linux操作一致），在这篇文章中我们会用到以下几个工具。 sratoolkit bgzip tabix bwa samtools GATK 4.0 这些软件都可以在github上找到（包括GATK），需要各位自行安装。这里补充一句，目前GATK4.0的正式版本已经发布，它的使用方式与之前相比有着一些差异（变得更加简单，功能也更加丰富了），增加了结构性变异检测和很多Spark、Cloud-Only的功能，并集成了Mutect2和picard的所有功能（以及其他很多有用的工具），这为我们减少了许多额外的工具，更加有利于流程的构建和维护，4.0之后的GATK是一个新的篇章，大家最好是掌握这一个版本！另外，3.x的版本貌似也已经不提供下载通道了，如果你还想使用3.x的话可以在公众号后台回复“GATK3”，我为你准备了一个GATK官方3.7的版本。我们这里则使用最新的4.0版本。 项目目录结构清晰的目录结构是管理众多项目的有效途径，经久不忘，随时可查。虽然看起来有些原始，但在Linux终端下面，我目前还没有发现更好的文件管理办法。这个项目的目录结构，我的建议是按照时间+项目的规则来命名，下面是我的目录结构： 1234./201802_wgs_practice/├── bin├── input└── output 顶层的项目名就是20180203_wgs_practice，下面有三个主目录： input：存储所有输入数据 output：存储所有输出数据 bin：存放所有执行程序和代码 output只存放结果数据，它是由input和bin中的数据和程序流程生成的。这样做的好处是层次分明，流程逻辑清楚，数据互不干扰。 使用E.coli K12完成比对和变异检测人类基因组数据很大，参考序列长度是3Gb。而一个人的高深度测序数据往往是这个数字的30倍——100Gb。如果直接用这样的数据来完成本文的分析，那么许多同学需要下载大量的原始数据。除了下载时间很长之外，如果没有合适的集群，只是在自己的桌面电脑上干这样的事情，那么硬盘空间也将很快不够用。而且，要在单机电脑上完成这样一个高深度WGS数据的分析，处理对机器性能有要求之后，跑起来也需要连续花上差不多140个小时——相信大家都等不起呀。 因此，为了解决这个问题，我找了E.coli K12（一种实验用的大肠杆菌）的数据作为代替，用来演示 数据比对和变异检测这两个最消耗计算资源和存储空间的步骤。E.coli K12的特点是数据很小，它的基因组长度只有4.6Mb，很适合大家用来快速学习WGS的数据分析，遇到人类的数据时，再做替换就行了。 下载E.coli K12的参考基因组序列熟悉的同学应该第一时间能够知道，这些物种的基因组参考序列都可以在NCBI上获取，我们这里也是一样，可以在NCBI网站上直接搜索这个序列，为了简化步骤，我直接给出E.coli K12参考序列的ftp地址给大家下载之用： 1ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gz 你可以在Linux（或者Mac OSX）命令行上直接使用wget，将这个fasta下载下来，由于它很小，所以几秒之后我们就可以得到这个fasta序列。 1$ wget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gz 为了接下来表达上的清晰和操作上的方便，我们使用bgzip将这个序列文件进行解压并把名字重命名为E.coli_K12_MG1655.fa，这样就一目了然了。 1$ gzip -dc GCF_000005845.2_ASM584v2_genomic.fna.gz &gt; E.coli_K12_MG1655.fa E.coli K12只有一条完整的染色体，你打开文件后将会看到和我一样的内容： 123&gt;NC_000913.3 Escherichia coli str. K-12 substr. MG1655, complete genomeAGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGCTTCTGAACTGGTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGAC 接着，我们用samtools为它创建一个索引，这是为方便其他数据分析工具（比如GATK）能够快速地获取fasta上的任何序列做准备。 1$ /Tools/common/bin/samtools faidx E.coli_K12_MG1655.fa 这时会生成一份E.coli_K12_MG1655.fa.fai文件。除了方便其他工具之外，我们可以通过这样的索引来获取fasta文件中任意位置的序列或者任意完整的染色体序列。可以很方便地完成对参考序列（或者任意fasta文件）特定区域序列的提取。举个例子： 1$ samtools faidx E.coli_K12_MG1655.fa NC_000913.3:1000000-1000200 我们就获得了E.coli K12参考序列上的这一段序列： 12345&gt;NC_000913.3:1000000-1000200GTGTCAGCTTTCGTGGTGTGCAGCTGGCGTCAGATGACAACATGCTGCCAGACAGCCTGAAAGGGTTTGCGCCTGTGGTGCGTGGTATCGCCAAAAGCAATGCCCAGATAACGATTAAGCAAAATGGTTACACCATTTACCAAACTTATGTATCGCCTGGTGCTTTTGAAATTAGTGATCTCTATTCCACGTCGTCGAGCG 这个小技巧在特定的时候非常实用。 下载E.coli K12的测序数据基因组参考序列准备好之后，接下来我们需要下载它的测序数据。E.coli K12作为一种供研究使用的模式生物，自然已经有许多的测序数据在NCBI上了，在这里我们选择了其中的1个数据——SRR1770413。这个数据来自Illumina MiSeq测序平台（不用担心平台的事情），read长度是300bp，测序类型Pair-End（没了解过PE read同学可以参考我前面WGS系列的第四节文章）。你可以在NCBI上直接搜到：这里 在NCBI给出的信息页面中，我们可以清楚地看到这个数据的大小（如下图）——差不多200MB，一般家庭网速也能够较快下载完成。 从NCBI上下载下来的测序数据，不是我们熟悉的fastq格式，而是SRA（一种NCBI自己设计的测序数据存储格式，具较高的压缩率），我们需要对其进行转换，下文详述。现在我们先下载，有两个下载方式（我在这里告诉大家的方法同样适用于其他类型的数据），第一个是如上面所说搜索到SRR1770413这个数据的ftq地址，然后直接在命令行中执行wget进行下载，如下： 1$ wget ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR177/SRR1770413/SRR1770413.sra 注意，下载下来的这个SRA文件虽然只有一份，但是里面其实存了read1和read2的测序数据，我们要把它解出来，转换成为我们所需的fastq格式。这个时候，我们就需要用到NCBI的官方工具包sratoolkit，大家下载对应系统的版本，直接解压之后就可以使用了。 sratoolkit是一个工具包，所有的执行程序都在它解压后的bin文件夹下，我们要把SRA转换为fastq，只需直接通过工具包中的fastq-dump即可完成。 1$ /Tools/sratoolkit/2.8.2/bin/fastq-dump --split-files SRR1770413.sra 然后我们就会得到这个E.coli K12数据的read1和read2了： 12SRR1770413_1.fastqSRR1770413_2.fastq 另一个数据下载的方法就是用上面说到的sratoolkit，我也比较推荐这个方法，操作很简单。同样是使用fastq-dump（没错，与上面一样，区别在于下载的时候，输入的是数据的SRA编号），它可以在我们下载的过程中就直接将SRA转换为两个fastq。 1$ /Tools/sratoolkit/2.8.2/bin/fastq-dump --split-files SRR1770413 下载完成后，我们最好用bgzip（不推荐gzip）将其压缩为.gz文件，这样可以节省空间，而且也不会对接下来的数据分析产生影响。 12$ /Tools/common/bin/bgzip -f SRR1770413_1.fastq$ /Tools/common/bin/bgzip -f SRR1770413_1.fastq 至此，E.coli K12相关的数据我们就都准备好了，先看一眼我现在的目录结构。 1234567891011121314./201802_wgs_practice/├── bin├── input│ ├── E.coli│ ├── fasta│ │ ├── E.coli_K12_MG1655.fa│ │ ├── E.coli_K12_MG1655.fa.fai│ │ └── work.log.sh│ └── fastq│ ├── SRR1770413_1.fastq.gz│ ├── SRR1770413_2.fastq.gz│ └── work.log.sh ├── output└── work.log.sh 其中各个目录下的 work.log.sh记录了我在该目录下的所有重要操作——这是我的个人习惯，目的是方便以后反查数据的需要。 数据准备完毕之后，接下来就可以进行具体的分析了。 质控质控是必须做的，我们需要完整认识原始的测序数据质量到底如何，该步骤不能省略。我专门为此单独写了一篇文章（WGS系列第三节），在正式的数据分析过程中，大家可以参考它来完成数据的质控，然后再进行接下来的分析。本篇文章为了控制篇幅和尽可能扣住核心内容，就不再对此深入展开，大家如果碰到问题，可以到在后台留言，或者加入我的交流圈（知识星球：解螺旋技术交流圈）和更多有经验的人一起交流。 比对首先是比对。所谓比对就是把测序数据定位到参考基因组上，确定每一个read在基因组中的位置。这里，我们依然用目前使用最广的BWA来完成这个工作。在正式比对之前，需要先为参考序列构建BWA比对所需的FM-index（比对索引）。 1$ /Tools/common/bin/bwa index E.coli_K12_MG1655.fa 由于这个序列很短，只需几秒就可以完成这个索引文件的构建（对于人类基因组则需要3个小时的时间）。创建完毕之后，将多出5份以E.coli_K12_MG1655.fa为前缀的序列索引文件。 12345E.coli_K12_MG1655.fa.ambE.coli_K12_MG1655.fa.annE.coli_K12_MG1655.fa.bwtE.coli_K12_MG1655.fa.pacE.coli_K12_MG1655.fa.sa 现在我们使用bwa完成比对，用samtools完成BAM格式转换、排序并标记PCR重复序列。步骤分解如下： 1234567891011121314151617#1 比对time /Tools/common/bin/bwa mem -t 4 -R '@RG\tID:foo\tPL:illumina\tSM:E.coli_K12' /Project/201802_wgs_practice/input/E.coli/fasta/E.coli_K12_MG1655.fa /Project/201802_wgs_practice/input/E.coli/fastq/SRR1770413_1.fastq.gz /Project/201802_wgs_practice/input/E.coli/fastq/SRR1770413_2.fastq.gz | /Tools/common/bin/samtools view -Sb - &gt; /Project/201802_wgs_practice/output/E.coli/E_coli_K12.bam &amp;&amp; echo "** bwa mapping done **"#2 排序time /Tools/common/bin/samtools sort -@ 4 -m 4G -O bam -o /Project/201802_wgs_practice/output/E.coli/E_coli_K12.sorted.bam /Project/201802_wgs_practice/output/E.coli/E_coli_K12.bam &amp;&amp; echo "** BAM sort done"rm -f /Project/201802_wgs_practice/output/E.coli/E_coli_K12.bam#3 标记PCR重复time /Tools/common/bin/gatk/4.0.1.2/gatk MarkDuplicates -I /Project/201802_wgs_practice/output/E.coli/E_coli_K12.sorted.bam -O /Project/201802_wgs_practice/output/E.coli/E_coli_K12.sorted.markdup.bam -M /Project/201802_wgs_practice/output/E.coli/E_coli_K12.sorted.markdup_metrics.txt &amp;&amp; echo "** markdup done **"#4 删除不必要文件(可选)rm -f /Project/201802_wgs_practice/output/E.coli/E_coli_K12.bamrm -f /Project/201802_wgs_practice/output/E.coli/E_coli_K12.sorted.bam#5 创建比对索引文件time /Tools/common/bin/samtools index /Project/201802_wgs_practice/output/E.coli/E_coli_K12.sorted.markdup.bam &amp;&amp; echo "** index done **" 从上面的命令大家也可以看到，我严格按照上文提到的项目目录规范来执行（步骤中涉及到的数据路径也都尽可能使用全路径），这个比对的shell存放在bin目录下，名称是bwa_and_markdup.sh，从名称也能够一眼可以看出是要做什么的。 简单解释一下这个shell所做的事情：首先利用bwa mem比对模块将E.coli K12质控后的测序数据定位到其参考基因组上（我们这里设置了4个线程来完成比对，根据电脑性能可以适当调大），同时通过管道（’|’ 操作符）将比对数据流引到samtools转换为BAM格式（SAM的二进制压缩格式），然后重定向(‘&gt;’操作符)输出到文件中保存下来。 -R 设置Read Group信息，虽然我在以前的文章中已经反复强调过它的重要性，但这里还是再说一次，它是read数据的组别标识，并且其中的ID，PL和SM信息在正式的项目中是不能缺少的(如果样本包含多个测序文库的话，LB信息也不要省略)，另外由于考虑到与GATK的兼容关系，PL（测序平台）信息不能随意指定，必须是：ILLUMINA，SLX，SOLEXA，SOLID，454，LS454，COMPLETE，PACBIO，IONTORRENT，CAPILLARY，HELICOS或UNKNOWN这12个中的一个。 接着用samtools对原始的比对结果按照参考序列位置从小到大进行排序（同样是4个线程），只有这个步骤完成之后才可以继续往下。 然后，我们使用GATK标记出排完序的数据中的PCR重复序列。这个步骤完成后，如无特殊需要，我们就可以直接删除前面那两个BAM文件了（原始比对结果和排序后的结果）——后续几乎不会再用到那两份文件了。关于标记PCR重复序列的操作比较简单，不再细说（如果希望了解更多有关重复序列特征的信息可以回看WGS系列第四节中的内容）。 最后，我们再用samtools为E_coli_K12.sorted.markdup.bam创建索引。我认为不论是否有后续分析，为BAM文件创建索引应该作为一个常规步骤，它可以让我们快速地访问基因组上任意位置的比对情况，这一点非常有助于我们随时了解数据。 至于每个步骤最前面的time，则是用于记录执行时间的，有助于我们清楚地知道每一个分析过程都花了多少时间，当需要优化流程的时候这个信息会很有用。 变异检测接下来是用GATK完成变异检测。但在开始之前之前我们还需要先为E.coli K12的参考序列生成一个.dict文件，这可以通过调用CreateSequenceDictonary模块来完成(这是原来picard的功能)。 1$ /Tools/common/bin/gatk/4.0.1.2/gatk CreateSequenceDictionary -R E.coli_K12_MG1655.fa -O E.coli_K12_MG1655.dict &amp;&amp; echo "** dict done **" 唯一需要注意的是.dict文件的名字前缀需要和fasta的一样，并跟它在同一个路径下，这样GATK才能够找到。 OK，现在我们就可以进行变异检测了，同样使用GATK 4.0的HaplotypeCaller模块来完成。由于我们只有一个样本，要完成这个工作其实很简单，直接输入比对文件和参考序列就行了，但是考虑到实际的情况，我想告诉大家一个更好的方式（虽然这会多花些时间），就是：先为每个样本生成一个GVCF，然后再用GenotypeGVCFs对这些GVCF进行joint calling，如下 ，我把命令都写在gatk.sh中，并执行。 123456789101112#1 生成中间文件gvcftime /Tools/common/bin/gatk/4.0.1.2/gatk HaplotypeCaller \ -R /Project/201802_wgs_practice/input/E.coli/fasta/E.coli_K12_MG1655.fa \ --emit-ref-confidence GVCF \ -I /Project/201802_wgs_practice/output/E.coli/E_coli_K12.sorted.markdup.bam \ -O /Project/201802_wgs_practice/output/E.coli/E_coli_K12.g.vcf &amp;&amp; echo "** gvcf done **"#2 通过gvcf检测变异time /Tools/common/bin/gatk/4.0.1.2/gatk GenotypeGVCFs \ -R /Project/201802_wgs_practice/input/E.coli/fasta/E.coli_K12_MG1655.fa \ -V /Project/201802_wgs_practice/output/E.coli/E_coli_K12.g.vcf \ -O /Project/201802_wgs_practice/output/E.coli/E_coli_K12.vcf &amp;&amp; echo "** vcf done **" 很快我们就获得了E.coli K12这个样本初步的变异结果——E_coli_K12.vcf。之所以非要分成两个步骤，是因为我想借此告诉大家，变异检测不是一个样本的事情，有越多的同类样本放在一起joint calling结果将会越准确，而如果样本足够多的话，在低测序深度的情况下也同样可以获得完整并且准确的结果，而这样的分步方式是应对多样本的好方法。 最后，我们用bgzip对这个VCF进行压缩，并用tabix为它构建索引，方便以后的分析。 12345#1 压缩 time /Tools/common/bin/bgzip -f /Project/201802_wgs_practice/output/E.coli/E_coli_K12.vcf#2 构建tabix索引time /Tools/common/bin/tabix -p vcf /Project/201802_wgs_practice/output/E.coli/E_coli_K12.vcf.gz bgzip压缩完成之后，原来的VCF文件会被自动删除。 为了保持一致，现在再看一下完成到这里之后我们的目录长什么样了，供大家对照。 123456789101112131415161718192021222324252627282930./201802_wgs_practice/├── bin│ ├── bwa_and_markdup.sh│ └── gatk.sh├── input│ └── E.coli│ ├── fasta│ │ ├── E.coli_K12_MG1655.dict│ │ ├── E.coli_K12_MG1655.fa│ │ ├── E.coli_K12_MG1655.fa.amb│ │ ├── E.coli_K12_MG1655.fa.ann│ │ ├── E.coli_K12_MG1655.fa.bwt│ │ ├── E.coli_K12_MG1655.fa.fai│ │ ├── E.coli_K12_MG1655.fa.pac│ │ ├── E.coli_K12_MG1655.fa.sa│ │ └── work.log.sh│ └── fastq│ ├── SRR1770413_1.fastq.gz│ ├── SRR1770413_2.fastq.gz│ └── work.log.sh├── output│ └── E.coli│ ├── E_coli_K12.g.vcf│ ├── E_coli_K12.g.vcf.idx│ ├── E_coli_K12.sorted.markdup.bam│ ├── E_coli_K12.sorted.markdup.bam.bai│ ├── E_coli_K12.sorted.markdup_metrics.txt│ ├── E_coli_K12.vcf│ └── E_coli_K12.vcf.idx└── work.log.sh 如果大家仔细看过WGS系列第四节的话，会发现我这里缺少了两个步骤：重比对和BQSR。没有执行BQSR是因为E.coli K12没有那些必须的known变异集（或者有但我没找到），所以无法进行；但没有重比对，则是因为我在GATK 4.0中没发现IndelRealigner这个功能，虽然我们使用GATK HaplotypeCaller或者Mutect2的话确实可以省略这个步骤，但如果是其他软件来进行变异检测那么该步骤依然十分重要，我目前不太清楚为何GATK 4.0没有将这个功能单独分离出来。 后面要谈到的就是变异的质控了。很遗憾我们这个E.coli K12的变异结果并不适合通过VQSR来进行过滤，原因上面也提到了一些，它不像人类的基因组数据，有着一套适合用来训练过滤模型的已知变异集（dbSNP，1000G，Hapmap和omini等）。其实这种情况有时候我们在工作中也会碰到，比如有些捕获测序（Panel测序数据，甚至外显子测序）的数据，由于它的区域较小，获得的变异也不多，导致最终没法满足VQSR进行模型训练时所需的最低变异数要求，那时你也不能通过这个方式协助变异质控。那么碰到这种情况的时候该怎么办？我将这部分的内容放在了下一篇文章中，在那里我们再来讨论这个问题。我也会告诉大家变异质控的基本逻辑，而不是简单罗列一个命令，同时也会再用NA12878这个人的数据来进一步告诉大家如何比较和评估变异结果。 小结至此，这个篇文章的上半部分就到此为止了。除了那些重要的内容之外，在上文中，你会看到我反复提到了创建“索引”这个事情，比如为fasta，为BAM，为VCF。我为什么非要反复强调这个事情不可呢？因为我发现许多初学者并不知道索引的作用，当被问到如何从巨大的比对文件或者变异文件中提取某个信息时，总是要走弯路——努力写程序去提取，既慢又费力，结果还不一定好，甚至有些有一定经验的同学也不知道使用bgzip和tabix的好处，因此我才反复在文章里提及。 本文首发于我的个人公众号：解螺旋的矿工，欢迎扫码关注，更及时了解更多基因组学信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>WGS</tag>
        <tag>流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我如何看待今日央视宣布我国启动“中国10万人基因组计划”]]></title>
    <url>%2F2017%2F12%2F28%2F2017-12-28-What-Do-I-Think-About-The-100K-Chinese-Genome-Project.html</url>
    <content type="text"><![CDATA[​我今天下午第一时间看到这个研究计划的时候，正在买东西，第一反应是：可惜了！ 以下是我的愚蠢看法，各位请吐槽。 它应该算是目前国内 最有上层优势的一个大规模中国人群基因组研究项目——新闻里说的是首个重大国家计划。 但说实话，据我所知自十三五之后，国内这两年里并不乏这类研究项目。例如，福建厦门的基因大数据项目、江苏扬子国投的百万人群基因组项目，还有华大基因、诺禾致源、贝瑞和康、安诺优达等的各类大型基因科技公司所启动的大规模人群项目等少则几千，多则几十万，所以准确地说这个项目不是世界上最大规模的人类基因组计划（即便抛开国内的不讲，美国也早有百万人群，英国也早已启动10万人的基因组计划——GenomicEngland）。 但国内的这些项目，它们要么是在自下而上地推行着，要么就是组织较为松散复杂，利益不容易分配和平衡，说实话它们都很艰难。而那些自下而上推动的也很难有国家经费的支撑，只能靠企业自费研究。 今日宣布的这个“中国10万人基因组计划”，据我所知，应该是第一个上了央视的国字辈项目，这其实很了得，背后也有大科学家和院士领头。然而，很可惜的是，它的侧重点却是在健康人群队列上！ 我并不是说，那样不对，只是在这 2-3年内一定有大几千、上万甚至几万的汉族人健康队列出来。所以在汉族人健康队列上，几年后它很可能会失去先发优势，科学价值可能不高了，从结果上看，最多就是跟风——我认为作为重大国家计划是应该会考虑科学的先发价值的。 当然作为医学和疾病研究的背景数据肯定依然有用——不管如何背景数据能够越多自然会越好。 样本数量越大虽然从结果看会越好。但其实是有边际效应的，越往后新增的样本能带来的新变异数量是会不断变少的，也就是说新样本带来的价值是在不断递减的。这是源自于同一祖先所致，特别是汉族人，这个人群内部的分化差异并不大! 所以过多的健康样本对这个群体来说意义不一定大。 我认为，有这个资源何不在搜集一定量的健康样本后，干脆集中精力研究一个/两个特定的疾病队列，那样是否更具有领先意义？ 这个项目的推进速度是4年完成。按照目前国内的基因测序通量，虽然1年完成10万人的基因组测序没有问题，但前期的样本搜集应该要消耗不少时间，我臆测，包括搜集样本、知情同意和测序要花2年时间，然后2年完成所有分析。这个压力其实不小。所以后续进度如何拭目以待吧。 对于这个项目我觉得 唯一的看点是里面提到的9个少数民族，这是目前的空白，而且短时间内估计很难有其它项目能够填补这个空白。它对于广大少数民族同胞来说意义更大。而且，它对于我们如何更好地认识汉族人与少数民族之间的遗传差异和关系都有重要的价值。但不知到时具体样本数目是几何。 以上，便是我的愚见。 做点补充：对于纯粹健康人队列是不需要全部测高深度的，特别是样本量在万级以上的水平，这个在数学上是可以推算出来的，因此我认为是根本不需要每个样本测30x的。并且对于其中的汉族人，如果数目够多只需要测几层就够了（甚至是1x）。对于其中的少数民族，就看所取样量的多寡来适当区别对待。所以根本是不需要9000T（9Pb）的原始数据量的，而且如果中间步骤控制的好，中间数据的产出可以控制在原始数据的3倍以下，最终数据则更加会远少于原始数据量。如果是1x，那就是300T的数据量（如果保守点，多测一些，按倍数来乘），测序成本不会真的很高。而且，在joint calling之前，全部可以并行，按照这个数据量，现在通常的云计算平台是完全能够承受的，并不算多，解决方法也多。 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>观点</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[该如何自学入门生物信息学]]></title>
    <url>%2F2017%2F12%2F16%2F2017-12-16-How-To-Learn-Bioinformatics-By-Yourself.html</url>
    <content type="text"><![CDATA[前几天在知乎上，我看到了这么一个问题“如何自学生物信息学？”，看了一圈回答后，发现很多答主自己也是初学者，可能还在摸索中，回答也并没有真正抓到重点。作为一个过来人，我觉得这个问题其实还挺重要的，相信也困扰着不少人，因此在这里我想尝试回答一下这个问题。 什么是生物信息学生物信息学与以往的传统生物学不同，它本身是一个混合体，而且在今天看来它应当还要包含现在的NGS和基因组学。我认为，它重在数据，因此在这个领域中 比较重要的是数学和计算机——计算机我指的是：编程能力和算法设计能力。这是我的切身体会，许多生物知识其实可以往后慢慢学，不必一开始花费大量时间补充生物知识。 但生物信息毕竟还是和生物有关，毫无生物知识其实也说不过去。对于初学者来说，想要进入这个领域，我觉得一开始需要重点搞清楚几个基本概念。比如，什么是基因组，什么是转录组，什么是蛋白组，什么是染色体，什么是基因，什么是染色体重组，什么是进化/演化，什么是表观遗传，什么是变异，变异类型有哪些，NGS技术是什么，测序仪的工作原理是什么，DNA是如何被测出来的等这些东西。因为，你只有真正了解数据是如何来的，才能更好地明白数据该如何处理和分析，以及如何才能有效地挖掘出它背后隐含的生物知识。 至于 分子生物学中诸多涉及细胞机制机理的知识我觉得在刚开始的时候反而可以缓一缓。我本来是学物理的，做生物信息可以说是半路出家，但其实没觉得有什么障碍，后来在实际需要的时候慢慢补上即可。推荐大家买本 《基因X》屯着——这是一本有厚度的基因必读书（曾经是《基因8》），作为参考书，平时有空的时候记得多翻翻。 此外，我觉得尽量避免去看生物信息学导论那类书，很多都太旧了，用处不大是一个方面，关键是还会浪费不少时间。 好奇心、兴趣和目标这应该是促进我们学习的第一要素。所谓，“知之者不如好之者，好之者不如乐之者”。兴趣的力量是强大的，基本上是你乐于继续下去的动力来源。如果仅是以赚钱为目的的话，那么我觉得还是另择一行吧，有很多其它更适合赚钱的领域。那么话说回来，要培养兴趣的话，我们可以先了解现在整个基因科技行业的发展情况，了解学习这些知识都可以做些什么，能够解决什么问题。然后再定一个目标，完成一个具体的项目，自学最怕没有目的性，在没有他人可以指导的情况下，很容易迷失方向。但如果没条件的话，可以尝试利用现有的数据（比如：千人基因组，GIAB等）复现它们的成果，甚至构建一个分析流程也行，这样子学起来才会比较高效，同时也有利于夯实所学的知识。 使用Google不要用百度，如果不会翻墙就花点钱买个VPN，与时间比起来，那点钱不算什么。在这个信息时代中信息已经足够多了，使用Google至少可以更快让你找到想要的东西，而学习生物信息，我们经常需要找东西。关于VPN，顺带安利一下，枫叶主机的网络加速套餐就挺不错的（是的，它叫网络加速套餐，不直接叫翻墙VPN，大家意会就好！），一个季度几十块60G流量，足够使用，点击链接注册并选择即可。 在有了上面这些基本的认识和目标之后，我们就可以开始了。 Linux基因数据分析，极度不推荐在Windows下完成，有很多的工具不支持，而且不利于学习，也不利于我们对数据的理解。因此掌握Linux，特别是直接在Terminal中进行数据分析是必须的。 不过，不用太担心，我们不需要成为Linux专家。对于生物信息研究人员来说，只有了解Linux的文件系统结构，能够在Terminal中灵活运用基本的Linux命令就足够了，不过vi需要掌握——我们编写程序时需要用到它。但要 达成这些目标不需要看大部头的Linux书籍——这个也是我想给所有要学习生物信息的小伙伴们提个醒，这样可以节省很多不必要花的时间，不然等你看完一大本Linux书，恐怕兴趣也都被磨得差不多了，要直奔目标。 我自己使用Linux的时间超过8年，，但是所使用的命令，合起来频度超过99%的竟然不超过20条！基本上就是在接触生信的第一天学会的，而对于聪明如你们的人来说，相信一定可以用更短的时间融汇贯通，对于这几个命令我简单列一下（注意都是在Terminal模式下）： pwd：列出当前目录的完整路径，明确你在哪； cd ：跳转到其他目录，两个好用的cd命令，”cd -“ 跳回最近一次的目录，”cd ..” 退回上一层目录； ls：列出当前目录内容，最好加上 -l -rt 参数，会更加清晰，目的是明确目录下都有什么； mkdir：创建目录； rm：删除文件或者目录； mv：重命名文件或者目录； cat：打开文本文件，内容输出到屏幕； less -SN：打开文本文件，这个是查看文本文件更合适的方式； head -n：查看文件前n行； tail -n：查看文件尾n行； wc -l：计算文本文件的行数； ”|“： 管道操作； grep命令； awk命令； sed命令； sort命令； du -sh ./: 检查当前目录所占空间大小； bc -l：启动Terminal下的计算器，可以在这里进行简单的数学运算，输入”quit”就可以退出； chmod：修改文件或者目录权限； 这里推荐看一下”极客学院“中那一篇”一步一步学Linux“的文章，我觉得那篇博文就足够了。值得你多花些时间研究的是Linux中 “|” 管道命令的妙处。另外，掌握基本的grep，sed，awk操作。我之所以推荐这三个命令是因为，它们很适合快速进行简单的文本操作，可以让很多工作直接快速地在命令行上完成，而不需要编写程序。比如抽取一个文本文件特定的几列信息、匹配相关信息、修改输出等，用awk实现起来非常简单。上面这些掌握了之后，你再学习如何利用这些命令和相关执行程序组建简单的shell任务流程，到这个阶段，Linux部分基本就OK了。至于如何在Terminal中编译程序或者安装软件包，一般都有教程，按照教程来就可以了，真碰到问题可以多Google。 至少掌握一门高级编程语言生物信息是一定离不开程序设计的，而且你不能只会R。我建议先学Python（不推荐Perl），它很容易上手——被称为”可执行的伪代码”，社区强大而活跃，碰到问题很容易找到解决办法。而且，支持组学数据分析的工具包也很丰富。还被誉为数据科学第一语言！不但可以进行文本处理，还可以进行统计分析，机器学习，或者作出精美的数据图等等，比起曾经的Perl真的强大很多。 在掌握了Python之后，我推荐的另一门高级语言是C（或者C++），它是难的，我之所以依然觉得有必要，是因为它可以让你具有干大事的能力。 虽然在实际的工作中Python已经足够强大，基本上可以应对项目中的各类大小事宜。但在我看来，如果你希望技术上做得更强，C一定是绕不开的，学会C/C++至少有两个好处： 让你理解机器的工作原理，理解你的程序是如何运作的。这非常有助于你以后写出更加优秀的代码； 设计高效率的算法模块，往往需要借助C/C++，而且设计出来的模块还可以很方便地包装起来用在Python中。同时，也有很多优秀的组学数据处理包是C/C++写的，比如，SSW、Bamtools和SeqAn——这个包很强大——我们可以用它编写比对算法，变异检测算法等。 Python如何调用C++ 另外，我认为R是很容易学的，这里也无意起语言之争，虽然Python比R好。只要你有兴趣，在有了上面的基础之后，可以在很短的时间内学会。 Python教程很多！我这里推荐一下gitbook上的这个《简明Python教程》。另外，廖雪峰的Python教程也不错。 掌握常用的组学数据分析软件生物信息的工具众多，不过数据分析过程中常用的工具和软件还是可以列出来的，主要是：bwa，samtools，picard，GATK，bedtools，bcftools，vcftools，FastQC，MultiQC，VEP这些。基本都是在构造如WGS、WES这类分析流程的时候需要被用到的。另外，还有关于GWAS的一系列分析工具等，这些其实可以根据后续的具体项目逐步深入。除了工具之外，基本的数据文件格式也必须认识，比如：Fasta，Fastq，BAM，gff，vcf等，我在下文中推荐的《Bioinformatics Data Skills》这本书里面就系统讲了诸多在基因数据分析过程用到的工具和文件格式解析，值得一读。 实践“书上得来终觉浅，绝知此事要躬行”。实践是必须的，如果没条件的话可以到Rosalind这个网站上做些训练题，这上面有着很多有意义的生物信息题目，从难到易的都有，涵盖的面也比较广（包括RNA，DNA，蛋白），值得一战。另外要积极寻找到大型基因科技公司（比如华大基因）或者基因研究所实习的机会。 构建一个流程，敢于造轮子在掌握了Linux和编程知识之后，建议利用公开的数据构造一个完整的数据分析流程，比如全基因组数据分析流程或者复现一个项目的全过程。现在最好的一个公开数据来自Genome in a bottle（GIAB） 。你可以用这个数据参考GATK的最佳实践或者不久前我写的一个“从零开始完整学习全基因组数据分析系列”的文章，构造一个WGS数据分析流程。需要注意的是，你在构建、复现甚至重造的时候，要尝试去理解各个环节的意义，不要只是机械地将一个分析过程串接起来，因为你的目的是学习，碰到问题时也尽量自己解决，这样才能真正掌握它。 数理知识生物信息离不开数学，准确地说是离不开统计学。有太多的数据分析都需要统计学知识的参与，包括常用的假设检验，贝叶斯推断、随机森林，SVM，回归分析，PCA等。因此，在你获得初步的基因数据处理能力之后，更进一步应该去做的就是加强这方面的知识。 多看优秀的组学算法站在巨人的肩膀上才能看得更远。生物信息领域有一个比较突出的特点，就是绝大多数的东西都是开源的，因此很多优秀的算法和程序你都能够在github上直接找到，比如，比对软件bwa和后缀树算法，Smith-waterman局部比对算法；基因组组装软件SOAPdenovo2的de Bruijn graph；变异检测GATK、freebayse、Platypus应用到的贝叶斯、最大似然、EM、Pair-HMM和高斯混合模型等；变异注释工具VEP，GWAS的一系列方法等。特别是GATK，它的问写的很优秀，因此值得多泡在它的一系列文档中。通过学习它们的源码，不但可以精进你的编程能力、算法设计能力，更重要的是还能让你深刻理解诸多组学数据分析的奥秘！这会在不知不觉中提升你对基因数据的理解，就如同维纳斯的面纱在你面前一点点被揭开了一样，有种豁然开朗的感觉，你不再觉得那堆东西很神秘了。 不要固步自封最后，一定要紧跟前沿。生物信息学以及现在的基因组学领域，知识的更新迭代可谓一日千里，可能半年不留神就会落后，所以平时一定要多看前沿的文章成果。但那么多杂志应该看哪些呢？我的建议是直接看最顶级的CNNS（Cell，Nature，NEJM，Science）杂志，如果时间不够用那么更加不必看其它的，原因有三个： 这些顶级杂志所代表的基本上就是领域的最前沿——也正因此它们才会在CNNS上发表。虽然其它杂志也有可能，但概率低很多，因此不必浪费时间，要看就看最好的； 这些杂志上的文章大多都能给你带来新的视角，能够开阔你的眼界。带来看待问题、解决问题的新思路和新想法，这会有助于你做出创新性的工作； 我认为多看这类文章，也会有更高的概率在这类杂志上发表成果。 另外，也可以多看看生物探索、奇点网这些公众号，上面每天都会报道很多关于这个领域的前沿信息，当然也别忘了请一定要多！多！关！注！我！（认真脸）另外，加入一些优质的生物信息交流圈，有机会的话参加一些重要的基因组学会议，千万不要关起门来闷头学，一定要看着外面的世界。 书和课程推荐推荐两本基础的基因数据处理书籍，都是基于Python语言的： 来自OReilly《Bioinformatics Data Skills- Reproducible.and.Robust.Research.with.Open.Source.Tools》主要偏重工具的使用和数据文件的处理，虽然讲的不是很深入，但是作为生物信息初学者的入门书来说还是十分有价值的，2015年出版的，也不算旧。 第二本是《Bioinformatics with Python Cookbook》这一本相比于第一本来说会难一些，它会侧重于一些主题性质的内容，比如群体遗传学，基因大数据等。 这两本书都有pdf电子版。感兴趣的小伙伴可以在公众号后台回复“入门书籍”这四个字就可以获得了。 最后，再推荐几个在线课程。第一个是Coursera上的一个课程：genomic data science 这是约翰霍普金斯大学组织的一个系列课程，紧扣现在主流的组学数据分析，非常适合于入门学习，是一个精品系列，讲的很好，力荐，但是需要收费，只有7天的免费体验时间。 另外，EMBO上也搞了一个，在这里，不过我认为没有Coursera的系统全面，但它是免费的。 好了，最后，祝你学习快乐。 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>WGS</tag>
        <tag>入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用Python处理BAM]]></title>
    <url>%2F2017%2F12%2F07%2F2017-12-06-How-To-Use-Pysam.html</url>
    <content type="text"><![CDATA[在上一篇的文章里我详细介绍了BAM（SAM/CRAM）的格式和一些需要注意的细节，还说了该如何使用samtools在命令行中对其进行操作。但是很多时候这些操作是不能满足我们的实际需要的，比如统计比对率、计算在某个比对质量值之上的read有多少，或者计算PE比对的插入片段长度分布，甚至需要你根据实际情况编写一个新的变异检测算法等。这个时候往往难以直接通过samtools来实现【注】，而是需要编写专门的程序进行计算。因此，在这一篇文章里我们就一起来学习应该如何在程序中借助Pysam来处理BAM文件。 【注】关于统计比对率其实是可以通过samtools stats计算获得的。不过我们这篇文章不是为了争辩samtools能做什么，不能做什么，而是要跟大家讨论该如何编写程序处理BAM。 不过，在开始之前我想稍微再补充一下上一节中提到的CRAM——我习惯将其称为BAM的高压缩格式，因为它和BAM/SAM的格式基本相同，但有四点我们需要注意一下： CRAM的高压缩是通过借助参考序列和对其他信息的进一步编码来实现的，它相比于BAM有着更高的压缩率，能够节省30%-50%的空间； CRAM目前的IO效率没有BAM高（压得密嘛），约慢30%，但在不断进步，现在已经更新到了3.x版本了； CRAM和BAM可以通过samtools或者picard方便地实现互转； CRAM一定会取代BAM，这话并不是我说的，而是baw/samtools的作者lh3说的。 什么是PysamPysam是一个专门用来处理（BAM/CRAM/SAM）比对数据和变异数据（VCF和BCF）的Python包。它的核心是htslib——一个高通量数据处理API（来自samtools和bwa的核心，基于C语言），开发者们用Python对它直接进行轻量级包装，因此能够在Python中方便地进行调用，并且保证了它与原生C-API功能上的高度一致。 为什么是Pysam因为Pysam可以说是最为官方的版本，有比较固定的开发者在维护，它的稳定性和可靠性都很高。虽然还有一些其它的包同样能够处理BAM但其实它们大多绕不开对htslib的使用，但却没有pysam周全。而且Pysam还集成了tabix的接口，所以除了比对数据之外，还能够用于处理所有用tabix构建过索引的文件，总之就是全且可靠。 如果是文本格式的sam的话，其实也可以直接将其当作普通文本文件来处理，不需借助任何程序包（这在早期的数据分析中经常看到这种操作），只是要麻烦很多（必须自己在程序中处理所有细节，包括解析FLAG和CIGAR信息，以前我也干过不少类似的事情），甚至我还看到有人直接在程序中调用samtools view把BAM转换成SAM之后再处理的。。。这样的做法实在不推荐。 所以，只要你用的是Python，那么Pysam真的是目前看来比较好的选择。当然如果你用C/C++那么直接用htslib或者bamtools，如果是Java，那么直接使用htsjdk——htslib的java版本。 如何使用Pysam 首先，要为我们的Python环境安装这个包，如果已安装过的话可以忽略这一步。有两个方法，pip和bioconda，都比较简单，我们这里以pip——Python的包管理工具来进行： 1$ pip install pysam 安装完成之后我们就可以在Python程序中调用pysam了。 读取BAM/CRAM/SAM文件Pysam中的函数有很多，但是重要的读取函数主要有： AlignmentFile：读取BAM/CRAM/SAM文件 VariantFile：读取变异数据（VCF或者BCF） TabixFile：读取由tabix索引的文件； FastaFile：读取fasta序列文件； FastqFile：读取fastq测序序列文件； 等以上几个，其中尤以AlignmentFile和VariantFile为核心。需要我们注意到的地方是，Pysam中的有些函数由于历史原因存在重复，比如名字上只有大小写的差异，但功能却是一样的（比如下图的TabixFile），有些则只是简化了函数名，这些情况用的时候留个心眼就行了。 另外，这篇文章的目的是介绍如何处理比对文件，所以我打算只介绍AlignmentFile。 读取比对文件前，我建议先使用samtools index为比对文件构建好索引。当然如果是SAM文件就不必了——它是文本文件，索引的作用是让我们可以对文件进行随机读取，而不必总是从头开始。 下面我先用一个例子作为引子： 12import pysambf = pysam.AlignmentFile('in.bam', 'rb') 我在这个例子里面，先在程序中导入pysam包，然后调用AlignmentFile函数读取in.bam文件，并把句柄赋值给了bf，bf其实是一个迭代器——Python中的术语，意思就是适合在for循环中进行遍历的对象。 这样我们就是可以通过bf获取这份比对文件中的内容了。比如我们想把in.bam中每一条read的比对位置（包含染色体编号和位置信息），比对质量值和插入片段长度输出（我们的in.bam来自PE测序数据的结果），那么可以这样做： 1234import pysambf = pysam.AlignmentFile('in.bam', 'rb')for r in bf: print r.reference_name, r.pos, r.mapq, r.isize 是不是很简单！打开in.bam文件之后，用for循环对其从头到尾地遍历，并把每个值都赋给r，r在这里代表的就是比对的read信息，它是一个对象（在Pysam由AlignedSegment定义），通过它就可以获取所有的比对信息，比如上面例子中： r.reference_name代表read比对到的参考序列染色体id； r.pos代表read比对的位置； r.mapq代表read的比对质量值； r.isize代表PE read直接的插入片段长度，有时也称Fragment长度； 这里例子的结果如下： 1234chrM 160 50 235chrM 161 30 -283chrM 314 60 -207... 另外，由于bf是一个迭代器，我们其实还可以用bf.next()一个一个地对其进行访问，而不必在for循环中遍历，这在一些特殊的情况下，这个做法是非常有用且方便的。 当然，上面这个例子其实非常简单，实际上r变量中还有很多其它关于比对的信息，下面这个截图，就是变量中能够获取到的所有比对相关的信息，有好几十个。 眼尖的同学可能也发现了，这里面存在一些名字类似的变量，如：r.mapping_quality 和 r.mapq，它们其实都是比对质量值。类似的也还有几个，这都是上面我提到的历史原因所致，不过这种多余变量随着Pysam的维护也正在逐步变少。 此外，Pysam中的位点坐标体系是0-base（意思是染色体的起始位置是从0而不是1开始算的）而不是1-base，所以上面的输出的160，其实真实位置应该要+1，也就是161。 还有，上文我也说过，AlignmentFile除了能够读/写BAM之外，还同样能够读/写CRAM和SAM。区别就在于函数中的第二个参数，比如上面例子中的字符’b’就是用于明确指定BAM文件，’r’字符代表“只读”模式（read首字母）。如果要打开CRAM文件，只需要把b换成c（代表CRAM）就行了，如下： 12import pysamcf = pysam.AlignmentFile('in.cram', 'rc') 那么，如果是SAM文件呢？去掉b或c即可： 12import pysamsf = pysam.AlignmentFile('in.sam', 'r') 读取特定比对区域内的数据有时候我们并不需要遍历整一份BAM文件，我们可能只想获得区中的某一个区域（比如chrM中301-310中的信息），那么这个时候可以用Alignmen模块中的fetch函数： 12345import pysambf = AlignmentFile('in.bam', 'rb')for r in bf.fetch('chrM', 300, 310)： print rbf.close() 通过fetch函数就可以定位特定区域了，非常方便。不过，这个时候输入文件in.bam就必须要有索引，不然无法实现这种读取操作。最后用完了，要记得关闭文件——bf.close()。 来个稍微难一点的例子问题：如何输出覆盖在某个位置上，比对质量值大于30的所有碱基？ 这个问题包含两个部分： 固定的某个位置（我们这里还是用chrM 301这个位置） read比对质量值必须是大于30 如何做呢？这个时候我们要用AlignmentFile模块的另一个函数——pileups来协助解决，代码如下： 1234567891011import pysambf = pysam.AlignmentFile("in.bam", "rb" )for pileupcolumn in bf.pileup("chrM", 300, 301): for read in [al for al in pileupcolumn.pileups if al.alignment.mapq&gt;30]: if not read.is_del and not read.is_refskip: if read.alignment.pos + 1 == 301: print read.alignment.reference_name,\ read.alignment.pos + 1,\ read.alignment.query_sequence[read.query_position]bf.close() 这段代码看起来虽然简单，但其实包含了很多信息。总的来说，就是通过pileup获取了所有覆盖到该位置的read，并将其存到pileupcolumn中。然后，对pileupcolumn调用pileups（注意多了一个s）获得一条read中每个比对位置的信息（一条read那么长，并非只覆盖了一个位置），然后通过判断语句留下覆盖到目标位点（301）的碱基。代码中的read.alignment是Pysam中AlignedSegment对象，它包含的内容和上述其它例子中的r是一样的。read.alignment.pos + 1还是0-base的原因。最后结果如下： 1234567891011chrM 301 AchrM 301 AchrM 301 AchrM 301 CchrM 301 CchrM 301 CchrM 301 CchrM 301 CchrM 301 CchrM 301 C... 创建BAM/CRAM/SAM文件最后这个例子，我想告诉大家该如何用Pysam输出BAM/CRAM/SAM格式，具体还是看代码吧，这里想输出结果是BAM文件，所以输出模式是“wb”，例子中我们只输出一条比对结果作为说明。 12345678910111213141516171819202122232425import pysamheader = &#123;'HD': &#123;'VN': '1.0'&#125;, 'SQ': [&#123;'LN': 1575, 'SN': 'chr1'&#125;, &#123;'LN': 1584, 'SN': 'chr2'&#125;]&#125;tmpfilename = "out.bam"with pysam.AlignmentFile(tmpfilename, "wb", header=header) as outf: a = pysam.AlignedSegment() # 定义一个AlignedSegment对象用于存储比对信息 a.query_name = "read_28833_29006_6945" a.query_sequence="AGCTTAGCTAGCTACCTATATCTTGGTCTTGGCCG" a.flag = 99 a.reference_id = 0 a.reference_start = 32 a.mapping_quality = 20 a.cigar = ((0,10), (2,1), (0,25)) a.next_reference_id = 0 a.next_reference_start=199 a.template_length=167 a.query_qualities = pysam.qualitystring_to_array("&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;:&lt;9/,&amp;,22;;&lt;&lt;&lt;") a.tags = (("NM", 1), ("RG", "L1")) outf.write(a) 小结我写这篇文章的目的主要有两个：第一，充实上一篇文章中关于如何操作BAM的内容；第二，介绍Pysam这一个值得使用的包给大家。另外，我上面列举的例子其实都比较偏于基础操作，这可能和我自身对认知的看法有关。我一直认为，只有真正理解并灵活地应用基础操作，才可以灵活地解决一切复杂的问题。 而且，上面几个例子中用到的模块和函数其实都是比较常用的，所以我比较推荐优先掌握它们。这些例子里面用到的数据我已放到github了，感兴趣的同学可以在公众号后台回复“WGS”即可获得，后续也会陆续有其它的代码和数据可供参考。 最后，Pysam的内容其实还有很多，我所介绍的也仅在对于比对数据的处理，其它很多的模块和函数，包括对Fasta，Fastq，VCF，BCF和Tabix文件的处理，我就不进行一一介绍了，建议大家在使用的时候多看看它的完整文档。 本文首发于我的个人公众号：解螺旋的矿工，欢迎关注，更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>BAM</tag>
        <tag>Pysam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始完整学习全基因组测序数据分析：第5节 理解并操作BAM文件]]></title>
    <url>%2F2017%2F11%2F27%2F2017-11-27-Begining-WGS-Data-Analysis-BAM-CRAM-And-SAM.html</url>
    <content type="text"><![CDATA[经过了第四节的长文，我想大家基本上已经知道了一个WGS流程该如何构建起来了吧。但在那一节中限于篇幅有两个很重要的文件我没能展开来讲，分别是：BAM和VCF文件。这篇我们先说BAM文件。 什么是BAMBAM是目前基因数据分析中最通用的比对数据存储格式，它既适合于短read也适合于长read，最长可以支持128Mbp的超大read！除了后缀是.bam之外，有些同学可能还会看到.cram，甚至.sam后缀的文件，其实它们一个是BAM的高压缩格式(.cram)——IO效率比原来的BAM要略差；另一个是BAM的纯文本格式（.sam）。当然格式都是一样的，因此为了描述上的清晰，我下面都统一用BAM。 BAM文件格式其实一开始它的名字是SAM（The Sequencing Alignment/Map Format的英文简称），第一次出现的时候，它是bwa比对软件的标准输出文件，但原生的SAM是纯文本文件，十分巨大（比如：一个人30x全基因组测序的sam大小超过600G），非常容易导致存储空间爆满！为了解决这个问题，bwa的开发者李恒（lh3）设计了一种比gz更加高效的压缩算法，对其进行压缩，这就是我们所说的BAM，它的文件大小差不多只有原来的1/6。 在2007年，NGS技术刚刚兴起之时，各类短序列比对软件层出不穷，输出格式也是各有特点，各家各有一套，并没有什么真正的标准可言，可以说那是一个谁都说我最好的时期。 但逐渐的，研究者们发现BAM格式对Mapping信息的记录是最全面的，用起来也是最灵活的。bwa的作者还为BAM文件开发了一个非常好用的工具包——Samtools，使得人们对BAM文件的处理变得十分便利，拓展性也变得非常强，后来还有类似于IGV等专门支持BAM的工具也越来越多，因此它就逐渐成为了主流。 现在基本上所有的比对数据都是用BAM格式存储的，俨然已经成为了业内的默认标准。 在2013年，研究者们还专门将Samtools的处理核心剥离出来，并将其打包成为一个专门用于处理高通量数据的API——htslib，除了C语言版本之外还有Java和Python版本，这些在github上都能直接找到。后续许多与NGS数据处理有关的工具基本都会使用这个API进行相关功能的开发，可见其影响力。 ok，背景的介绍就先到此为止了，我们回归主题。下面这个图是我从一份刚刚完成比对的bam文件中截取出来的内容： 由于屏幕所限，无法把全部的内容都包含进来，特别是header信息，贴在这里仅是为了让还没见过BAM文件的同学们能够对它有一个总体的感觉。 如果是SAM文件，同时你也熟悉linux操作的话，直接在linux终端用less打开即可（注意：不要试图在本地使用文本编辑器，如vim等直接打开文件，会撑死机子的），但如果我们要查看的是BAM，那么必须通过Samtools（可以到samtools的网站下载并安装）。 12$ less -SN in.sam # 打开sam文件$ samtools view -h in.bam # 打开bam文件 BAM文件分为两个部分：header和record。这里额外说一句，许多NGS组学数据的存储格式都是由header和record两部分组成的。 以上例子，在samtools view中加上-h参数目的是为了同时把它的header输出出来，如果没有这个参数，那么header默认是不显示的。 header内容不多，也不会太复杂，每一行都用‘@’ 符号开头，里面主要包含了版本信息，序列比对的参考序列信息，如果是标准工具（bwa，bowtie，picard）生成的BAM，一般还会包含生成该份文件的参数信息（如上图），@PG标签开头的那些。这里需要重点提一下的是header中的@RG也就是Read group信息，这是在做后续数据分析时专门用于区分不同样本的重要信息。它的重要性还体现在，如果原来样本的测序深度比较深，一般会按照不同的lane分开比对，最后再合并在一起，那么这个时候你会在这个BAM文件中看到有多个RG，里面记录了不同的lane，甚至测序文库的信息，唯一不变的一定是SM的sample信息，这样合并后才能正确处理。 其实，关于这一点我在上一篇文章中（引用第四节）讲序列比对时的也特意强调了这些方面，不记得的同学们也可以翻看上一篇的相关内容。 接下来重点要说的是BAM的核心：record（有时候也叫alignment section，即，比对信息）。这是我们通常所说的序列比对内容，每一行都是一条read比对信息，它的记录看起来是这样的： 我这里借用了网上的一张图片来辅助说明，recoed中的每一个信息都是用制表符tab分开的。 下面我们就来仔细瞧瞧这里的每一个信息分别都是什么。 以上，前11列是所有BAM文件中都必须要有的信息，而且从描述中我们也能够比较清楚地知道其所代表的含义。但其中，有几个信息实在太重要了，以至于我认为有必要对其进行详细说明。 第一，Flag信息这是一个非常特别并且重要的数字，也是一个容易被忽视的数字，这可能和许多生信工程师也并不完全理解这个值有关。许多同学在第一次看到其官方文档中的描述之后依然会觉得十分困惑，但它里面实际上记录了许多有关read比对情况的信息。想要读懂它的一个关键点是我们不能够将其视为一个数字，而是必须将其转换为一串由0和1组成的二进制码，这一串二进制数中的每一个位(注意是“位”，bit的意思)都代表了一个特定信息，它一共有12位（以前只有8位），所以一般会用一个16位的整数来代表，这个整数的值就是12个0和1的组合计算得来的，因此它的数值范围是0~2048（2的12次方，计算机科学的同学对这种计算应该不陌生）。 那么下面我就结合其文档和自己的实践经验对这12个位的含义用更加通俗易懂的语言来重新描述，如下表： 所以，通过上面这个表的信息，我们就可以清楚地知道每一个FLAG中都包含了什么信息。比如看到FLAG = 77时，我们第一步要做的就是将其分解为二进制序列（也可以理解为分解成若干个2的n次方之和）： 77 = 000001001101 = 1 + 4 + 8 +64，这样就得到了这个FLAG包含的意思：PE read，read比对不上参考序列，它的配对read也同样比不上参考序列，它是read1。 当然，如果你希望自己在程序中写一段处理FLAG的代码，那么显然是不会像我们这个例子那样去分解这个整数的，多麻烦啊！那么该如何做呢？其实也很简单，比如我们 要获得其中某个位（假设第N位）的值——只需要将这个FLAG值和2的N次方做与的运算即可。在与运算时，FLAG值首先会被转换成一串二进制序列（如77=000001001101），而2的N次方除了第N位是1之外，其它的都是0，“与”了之后其它信息就会被屏蔽掉。比如，我们想知道该read是否比对上了参考序列，那么只需要计算FLAG &amp; 4 的值就行了，如果结果是1那么就是比对上了，如果是0则代表没有比上。 不过，在实际工作中，除非遇到特殊的情况，否则我一般更推荐调用官方的htslib这个包来协助处理，它是一个C语言库，如果你用Python，则是pysam——htslib的python包（Java则是htsjdk），包中已经帮我们做了这些处理，可以直接得到结果，下一篇文章里我会用pysam举例说明如何用它来操作bam文件。 另外，下面这一段代码是htslib（samtools的核心库）中定义的12个与flag值进行与操作获取对应位信息的变量，感兴趣的同学可以再htslib里面的sam.h文件中找到，在做一些需要触达基础性原理的开发时或许你会用到。 123456789101112131415161718192021222324 /*! @abstract the read is paired in sequencing, no matter whether it is mapped in a pair */#define BAM_FPAIRED 1/*! @abstract the read is mapped in a proper pair */#define BAM_FPROPER_PAIR 2/*! @abstract the read itself is unmapped; conflictive with BAM_FPROPER_PAIR */#define BAM_FUNMAP 4/*! @abstract the mate is unmapped */#define BAM_FMUNMAP 8/*! @abstract the read is mapped to the reverse strand */#define BAM_FREVERSE 16/*! @abstract the mate is mapped to the reverse strand */#define BAM_FMREVERSE 32/*! @abstract this is read1 */#define BAM_FREAD1 64/*! @abstract this is read2 */#define BAM_FREAD2 128/*! @abstract not primary alignment */#define BAM_FSECONDARY 256/*! @abstract QC failure */#define BAM_FQCFAIL 512/*! @abstract optical or PCR duplicate */#define BAM_FDUP 1024/*! @abstract supplementary alignment */#define BAM_FSUPPLEMENTARY 2048 第二，CIGARCIGAR是Compact Idiosyncratic Gapped Alignment Report的首字母缩写，称为“雪茄”字符串。 作为一个字符串，它用数字和几个字符的组合形象记录了read比对到参考序列上的细节情况，读起来要比FLAG直观友好许多，只是记录的是不同的信息。比如，一条150bp长的read比对到基因组之后，假如看到它的CIGAR字符串为：33S117M，其意思是说在比对的时候这条read开头的33bp在被跳过了（S），紧接其后的117bp则比对上了参考序列（M）。这里的S代表软跳过（Soft clip），M代表匹配（Match）。CIGAR的标记字符有“MIDNSHP=XB”这10个，分别代表read比对时的不同情况： 除了最后‘=XB’非常少见之外，其它的标记符通常都会在实际的BAM文件中碰到。另外，对于M还是再强调一次，CIGAR中的M，不能觉得它代表的是匹配就以为是百分百没有任何miss-match，这是不对的，多态性碱基或者单碱基错配也是用M标记！ 第三，MAPQ，比对质量值这个值同样非常重要，它告诉我们的是这个read比对到参考序列上这个位置的可靠程度，用错误比对到该位置的概率值（转化为Phred scale）来描述：$$-10logP{错比概率}$$。 因此MAPQ（mapping quality）值大于30就意味着错比概率低于0.001（千分之一），这个值也是我们衡量read比对质量的一个重要因子。 剩下的几列在上面的格式表中描述的也比较清楚，基本没有过于隐藏的信息，因此我就不打算再一一细说了，如果大家依然有困惑可以到后台留言。 此外，细心的同学可能也已经发现了：fastq的所有信息都被涵盖到了BAM文件中了，包括比对不上的read也在，因此获得了BAM其实也等于获得了所有的read。而且，fastq有时也会被转换成一种uBam文件，指的就是un-mapping BAM——没有做过比对的BAM文件。它相比于Fastq可以用metadata存储更多有用的信息，不过这不是我们这篇文章想说的内容。 最后，还是再说明一次：BAM文件中除了必须的前11列信息之外，不同的BAM文件中后面记录metadata的列是不固定的，在不同的处理软件中输出时也会有所不同，我们也可以依据实际的情况增删不同的metadata信息。 使用samtools view查看BAM文件BAM文件由于是特殊的二进制格式，因此没办法通过文本的形式直接打开，要用samtools的view功能在终端上进行查看（上文也已经说到这里在进行系统补充），如： 1$ samtools view in.bam 如果不想从头开始看，希望快速地跳转到基因组的其它位置上，比如chr22染色体，那么可以先用samtools index生成BAM文件的索引(如果已经有索引文件则不需该步骤)，然后这样操作： 1234$ samtools index in.bam # 生成in.bam的索引文件in.bam.bai$ samtools view in.bam chr22 # 跳转到chr22染色体$ samtools view in.bam chr22:16050103 # 跳转到chr22:16050103位置$ samtools view in.bam chr22:16050103-16050103 # 只查看该位置 IGV或者samtools tview查看比对情况以上，我基本上列举了我们会在终端上如何查看BAM文件的几个最常用操作。但如果你想更直观查看的BAM文件，IGV是目前最好的一个选择，但仅适合于文件还比较小的情况，效果如下： 如果你的BAM文件很大，都超过了你的本地电脑磁盘了，你还是想看该怎么办？你有两个选择： 第一，把你想查看的那部分区域用samtools view提取出来，生成一份小一些的BAM，然后下载下来，在导入到IGV中。 1$ samtools view -h in.bam chr22:16050103-16050203 | samtools view -Sb - &gt; small.bam 第二，不下载，直接在终端用samtools tview进行查看。samtools tview有类似于IGV的功能，虽然体验会稍差一些。 1$ samtools tview --reference hg38.fa in.bam 在该模式下，按下键盘‘g’后，会跳出一个Goto框，在里面输入想要调整过去的位置，就行了，比如： 按下esc键则可以取消。另外，为了节省空间，加快查询效率，read中与参考序列相同的部分被用一串串不同颜色的点表示，只留下miss-match的碱基和发生indel变异的区域。其中圆点表示正链比对，逗号表示负链比对。不同的颜色代表不同的比对质量值：白色&gt;=30，黄色20-29，绿色10-19，蓝色0-9。如果你还想知道的其他的功能，可以在tview模式里按下“?”问号，就会弹出类似下面这样的帮助窗口，然后按照指引做就行了。 虽然看起来不如IGV体验那样好，功能也比较单一（仅可以查看比对情况），但可贵之处在于可以在终端里面直接操作，当需要快速查看某个位置的比对情况时，操作效率非常高。而如果要退出该模式，也非常简单，按下q键就可以了。 小结那么，有关BAM格式的内容我们就暂且先到这里吧，大家如果有疑惑或者感兴趣的内容都可以到后台留言，我都会定时进行回复。在下一篇文章中，我们将重点介绍如何使用pysam来操作bam文件了。 本文首发于我的个人公众号：解螺旋的矿工，欢迎关注，更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>WGS</tag>
        <tag>数据格式</tag>
        <tag>BAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始完整学习全基因组测序（WGS）数据分析：第4节 构建WGS主流程]]></title>
    <url>%2F2017%2F09%2F19%2F2017-09-19-Begining-WGS-Data-Analysis-The-pipeline.html</url>
    <content type="text"><![CDATA[这篇文章很长，超过1万字，是本系列中最重要的一篇，因为我并非只是在简单地告诉大家几条硬邦邦的操作命令。对于新手而言不建议碎片时间阅读，对于有一定经验的老手来说，相信依然可以有所收获。在开始之前，我想先说一句：流程的具体形式其实是次要的，WGS本质上只是一个技术手段，重要的是，我们要明白自己所要解决的问题是什么，所希望获取的结果是什么，然后再选择合适的技术。这是许多人经常忽视的一个重要问题。 好了，以下进入正文。 这是WGS数据分析的流程图。流程的目的是准确检测出每个样本（这里特指人）基因组中的变异集合，也就是人与人之间存在差异的那些DNA序列。我把整个分析过程按照它们实际要完成的功能，将其分成了三个大的模块： 原始数据质控 数据预处理 变异检测 这或许和很多人看到的WGS分析流程，在结构梳理上有些差异（比如GATK的最佳实践），但过程中的各个步骤和所要完成的事情是一模一样的。 0.准备阶段在开始之前，我们需要做一些准备工作，主要是部署好相关的软件和工具。我们在这个WGS数据分析过程中用到的所有软件都是开源的，它们的代码全部都能够在github上找到，具体如下： BWA（Burrow-Wheeler Aligner）: 这是最权威，使用最广的NGS数据比对软件，目前已经更新到0.7.16版本； Samtools: 是一个专门用于处理比对数据的工具，由BWA的作者（lh3）所编写； Picard: 它是目前最著名的组学研究中心-Broad研究所开发的一款强大的NGS数据处理工具，功能方面和Samtools有些重叠，但更多的是互补，它是由java编写的，我们直接下载最新的.jar包就行了。 GATK: 同样是Broad研究所开发的，是目前业内最权威、使用最广的基因数据变异检测工具。值得注意的是，目前GATK有3.x和4.x两个不同的版本，代码在github上也是分开的。4.x是今年新推出的，在核心算法层面并没太多的修改，但使用了新的设计模式，做了很多功能的整合，是更适合于大规模集群和云运算的版本，后续GATK团队也将主要维护4.x的版本，而且它的代码是100%开源的，这和3.x只有部分开源的情况不同。看得出GATK今年的这次升级是为了应对接下来越来越多的大规模人群测序数据而做出的改变，但现阶段4.x版本还不稳定，真正在使用的人和机构其实也还不多。短期来看，3.x版本还将在业内继续使用一段时间；其次，3.x对于绝大分部的分析需求来说是完全足够的。我们在这里也以GATK3.8（最新版本）作为流程的重要工具进行分析流程的构建。 事实上，对于构造WGS分析流程来说，以上这个四个工具就完全足够了。它们的安装都非常简单，除了BWA和Samtools由C编写的，安装时需要进行编译之外，另外两个只要保证系统中的java是1.8.x版本及以上的，那么直接下载jar包就可以使用了。操作系统方面推荐linux（集群）或者Mac OS。 1.原始数据质控数据的质控，由于我已经在上一节的文章中讲的比较详细了，因此在本篇中就不再进行详细的讨论了。而且质控的处理方法都是比较一致的，基本不需要为特定的分析做定制化的改动，因此，我们可以把它作为WGS主流程之外的一环。但还是再强调一下，数据质控的地位同样重要，不然我也不必专门为其单独写一篇完整的文章。 2.数据预处理序列比对先问一个问题：为什么需要比对？ 我们已经知道NGS测序下来的短序列（read）存储于FASTQ文件里面。虽然它们原本都来自于有序的基因组，但在经过DNA建库和测序之后，文件中不同read之间的前后顺序关系就已经全部丢失了。因此，FASTQ文件中紧挨着的两条read之间没有任何位置关系，它们都是随机来自于原本基因组中某个位置的短序列而已。 因此，我们需要先把这一大堆的短序列捋顺，一个个去跟该物种的 参考基因组【注】比较，找到每一条read在参考基因组上的位置，然后按顺序排列好，这个过程就称为测序数据的比对。这 也是核心流程真正意义上的第一步，只有完成了这个序列比对我们才有下一步的数据分析。 【注】参考基因组：指该物种的基因组序列，是已经组装成的完整基因组序列，常作为该物种的标准参照物，比如人类基因组参考序列，fasta格式。 序列比对本质上是一个寻找最大公共子字符串的过程。大家如果有学过生物信息学的话，应该或多或少知道BLAST，它使用的是动态规划的算法来寻找这样的子串，但在面对巨量的短序列数据时，类似BLAST这样的软件实在太慢了！因此，需要更加有效的数据结构和相应的算法来完成这个搜索定位的任务。 我们这里将用于流程构建的BWA就是其中最优秀的一个，它将BW(Burrows-Wheeler)压缩算法和后缀树相结合，能够让我们以较小的时间和空间代价，获得准确的序列比对结果。 以下我们就开始流程的搭建。 首先，我们需要为参考基因组的构建索引——这其实是在为参考序列进行Burrows Wheeler变换（wiki: 块排序压缩），以便能够在序列比对的时候进行快速的搜索和定位。 1$ bwa index human.fasta 以我们人类的参考基因组（3Gb长度）为例，这个构造过程需要消耗几个小时的时间（一般3个小时左右）。完成之后，你会看到类似如下几个以human.fasta为前缀的文件： 123456.├── human.fasta.amb├── human.fasta.ann├── human.fasta.bwt├── human.fasta.pac└── human.fasta.sa 这些就是在比对时真正需要被用到的文件。这一步完成之后，我们就可以将read比对至参考基因组了： 1$ bwa mem -t 4 -R '@RG\tID:foo_lane\tPL:illumina\tLB:library\tSM:sample_name' /path/to/human.fasta read_1.fq.gz read_2.fq.gz &gt; sample_name.sam 大伙如果以前没使用过这个比对工具的话，那么可能不明白上面参数的含义。我们这里调用的是bwa的mem比对模块，在解释这样做之前，我们不妨先看一下bwa mem的官方用法说明，它就一句话： 1Usage: bwa mem [options] &lt;idxbase&gt; &lt;in1.fq&gt; [in2.fq] 其中，[options]是一系列可选的参数，暂时不多说。这里的 &lt; idxbase&gt;要输入的是参考基因组的BW索引文件，我们上面通过bwa index构建好的那几个以human.fasta为前缀的文件便是；&lt; in1.fq&gt;和 [in2.fq]输入的是质控后的fastq文件。但这里输入的时候为什么会需要两个fq（in1.fq和in2.fq）呢？我们上面的例子也是有两个：read_1.fq.gz和read_2.fq.gz。这是因为这是双末端测序（也称Pair-End）的情况，那什么是“双末端测序”呢？这两个fq之间的关系又是什么？这个我需要简单解析一下。 我们已经知道NGS是短读长的测序技术，一次测出来的read的长度都不会太长，那为了尽可能把一个DNA序列片段尽可能多地测出来，既然测一边不够，那就测两边，于是就有了一种从被测DNA序列两端各测序一次的模式，这就被称为双末端测序（Pair-End Sequencing，简称PE测序）。如下图是Pair-End测序的示意图，中间灰色的是被测序的DNA序列片段，左边黄色带箭头和右边蓝色带箭头的分别是测序出来的read1和read2序列，这里假定它们的长度都是100bp。虽然很多时候Pair-End测序还是无法将整个被测的DNA片段完全测通，但是它依然提供了极其有用的信息，比如，我们知道每一对的read1和read2都来自于同一个DNA片段，read1和read2之间的距离是这个DNA片段的长度，而且read1和read2的方向刚好是相反的（这里排除mate-pair的情形）等，这些信息对于后面的变异检测等分析来说都是非常有用的。 Pair-End 测序 另外，在read1在fq1文件中位置和read2在fq2文件中的文件中的位置是相同的，而且read ID之间只在末尾有一个’/1’或者’/2’的差别。 read1 ID和read2 ID的差别 既然有双末端测序，那么与之对应的就有单末端测序（Single End Sequecing，简称SE测序），即只测序其中一端。因此，我们在使用bwa比对的时候，实际上，in2.fq是非强制性的（所以用方括号括起来），只有是双末端测序的数据时才需要添加。 回到上面我们的例子，大伙可以看到我这里除了用法中提到的参数之外，还多了2个额外的参数，分别是：-t，线程数，我们在这里使用4个线程；-R 接的是 Read Group的字符串信息，这是一个非常重要的信息，以@RG开头，它是用来将比对的read进行分组的。不同的组之间测序过程被认为是相互独立的，这个信息对于我们后续对比对数据进行错误率分析和Mark duplicate时非常重要。在Read Group中，有如下几个信息非常重要： (1) ID，这是Read Group的分组ID，一般设置为测序的lane ID（不同lane之间的测序过程认为是独立的），下机数据中我们都能看到这个信息的，一般都是包含在fastq的文件名中； (2) PL，指的是所用的测序平台，这个信息不要随便写！特别是当我们需要使用GATK进行后续分析的时候，更是如此！这是一个很多新手都容易忽视的一个地方，在GATK中，PL只允许被设置为：ILLUMINA，SLX，SOLEXA，SOLID，454，LS454，COMPLETE，PACBIO，IONTORRENT，CAPILLARY，HELICOS或UNKNOWN这几个信息。基本上就是目前市场上存在着的测序平台，当然，如果实在不知道，那么必须设置为UNKNOWN，名字方面不区分大小写。如果你在分析的时候这里没设置正确，那么在后续使用GATK过程中可能会碰到类似如下的错误： 1ERROR MESSAGE: The platform (xx) associated with read group GATKSAMReadGroupRecord @RG:xx is not a recognized platform. 这个时候你需要对比对文件的header信息进行重写，就会稍微比较麻烦。 我们上面的例子用的是PL:illumina。如果你的数据是CG测序的那么记得不要写成CG！而要写COMPLETE。 (3) SM，样本ID，同样非常重要，有时候我们测序的数据比较多的时候，那么可能会分成多个不同的lane分布测出来，这个时候SM名字就是可以用于区分这些样本。 (4) LB，测序文库的名字，这个重要性稍微低一些，主要也是为了协助区分不同的group而存在。文库名字一般可以在下机的fq文件名中找到，如果上面的lane ID足够用于区分的话，也可以不用设置LB； 除了以上这四个之外，还可以自定义添加其他的信息，不过如无特殊的需要，对于序列比对而言，这4个就足够了。这些信息设置好之后，在RG字符串中要用制表符（\t）将它们分开。 最后在我们的例子中，我们将比对的输出结果直接重定向到一份sample_name.sam文件中，这类文件是BWA比对的标准输出文件，它的具体格式我会在下一篇文章中进行详细说明。但SAM文件是文本文件，一般整个文件都非常巨大，因此，为了有效节省磁盘空间，一般都会用samtools将它转化为BAM文件（SAM的特殊二进制格式），而且BAM会更加方便于后续的分析。所以我们上面比对的命令可以和samtools结合并改进为： 1$ bwa mem -t 4 -R '@RG\tID:foo_lane\tPL:illumina\tLB:library\tSM:sample_name' /path/to/human.fasta read_1.fq.gz read_2.fq.gz | samtools view -S -b - &gt; sample_name.bam 我们通过管道(“|”)把比对的输出如同引导水流一样导流给samtools去处理，上面samtools view的-b参数指的就是输出为BAM文件，这里需要注意的地方是-b后面的’-‘，它代表就是上面管道引流过来的数据，经过samtools转换之后我们再重定向为sample_name.bam。 关于BWA的其他参数，我这里不打算对其进行一一解释，在绝大多数情况下，采用默认是合适的做法。 [Tips] BWA MEM比对模块是有一定适用范围的：它是专门为长read比对设计的，目的是为了解决，第三代测序技术这种能够产生长达几十kb甚至几Mbp的read情况。一般只有当read长度≥70bp的时候，才推荐使用，如果比这个要小，建议使用BWA ALN模块。 排序以上，我们就完成了read比对的步骤。接下来是排序： 排序这一步我们也是通过使用samtools来完成的，命令很简单： 1Usage: samtools sort [options...] [in.bam] 但在执行之前，我们有必要先搞明白为什么需要排序，为什么BWA比对后输出的BAM文件是没顺序的！原因就是FASTQ文件里面这些被测序下来的read是随机分布于基因组上面的，第一步的比对是按照FASTQ文件的顺序把read逐一定位到参考基因组上之后，随即就输出了，它不会也不可能在这一步里面能够自动识别比对位置的先后位置重排比对结果。因此，比对后得到的结果文件中，每一条记录之间位置的先后顺序是乱的，我们后续去重复等步骤都需要在比对记录按照顺序从小到大排序下来才能进行，所以这才是需要进行排序的原因。对于我们的例子来说，这个排序的命令如下： 1$ time samtools sort -@ 4 -m 4G -O bam -o sample_name.sorted.bam sample_name.bam 其中，-@，用于设定排序时的线程数，我们设为4；-m，限制排序时最大的内存消耗，这里设为4GB；-O 指定输出为bam格式；-o 是输出文件的名字，这里叫sample_name.sorted.bam。我会比较建议大伙在做类似分析的时候在文件名字将所做的关键操作包含进去，因为这样即使过了很长时间，当你再去看这个文件的时候也能够立刻知道当时对它做了什么；最后就是输入文件——sample_name.bam。 【注意】排序后如果发现新的BAM文件比原来的BAM文件稍微小一些，不用觉得惊讶，这是压缩算法导致的结果，文件内容是没有损失的。 去除重复序列（或者标记重复序列） 在排序完成之后我们就可以开始执行去除重复（准确来说是 去除PCR重复序列）的步骤了。 首先，我们需要先理解什么是重复序列，它是如何产生的，以及为什么需要去除掉？要回答这几个问题，我们需要再次理解在建库和测序时到底发生了什么。 我们在第1节中已经知道，在NGS测序之前都需要先构建测序文库：通过物理（超声）打断或者化学试剂（酶切）切断原始的DNA序列，然后选择特定长度范围的序列去进行PCR扩增并上机测序。 因此，这里重复序列的来源实际上就是由PCR过程中所引入的。因为所谓的PCR扩增就是把原来的一段DNA序列复制多次。可是为什么需要PCR扩增呢？如果没有扩增不就可以省略这一步了吗？ 情况确实如此，但是很多时候我们构建测序文库时能用的细胞量并不会非常充足，而且在打断的步骤中也会引起部分DNA的降解，这两点会使整体或者局部的DNA浓度过低，这时如果直接从这个溶液中取样去测序就很可能漏掉原本基因组上的一些DNA片段，导致测序不全。而PCR扩增的作用就是为了把这些微弱的DNA多复制几倍乃至几十倍，以便增大它们在溶液中分布的密度，使得能够在取样时被获取到。所以这里大家需要记住一个重点，PCR扩增原本的目的是为了增大微弱DNA序列片段的密度，但由于整个反应都在一个试管中进行，因此其他一些密度并不低的DNA片段也会被同步放大，那么这时在取样去上机测序的时候，这些DNA片段就很可能会被重复取到相同的几条去进行测序（下图为PCR扩增示意图）。 PCR扩增示意图：PCR扩增是一个指数扩增的过程，图中原本只有一段双链DNA序列，在经过3轮PCR后就被扩增成了8段 看到这里，你或许会觉得，那没必要去除不也应该可以吗？因为即便扩增了多几次，不也同样还是原来的那一段DNA吗？直接用来分析对结果也不会有影响啊！难道不是吗？ 会有影响，而且有时影响会很大！最直接的后果就是同时增大了变异检测结果的假阴和假阳率。主要有几个原因： DNA在打断的那一步会发生一些损失，主要表现是会引发一些碱基发生颠换变换（嘌呤-变嘧啶或者嘧啶变嘌呤），带来假的变异。PCR过程会扩大这个信号，导致最后的检测结果中混入了假的结果； PCR反应过程中也会带来新的碱基错误。发生在前几轮的PCR扩增发生的错误会在后续的PCR过程中扩大，同样带来假的变异； 对于真实的变异，PCR反应可能会对包含某一个碱基的DNA模版扩增更加剧烈（这个现象称为PCR Bias）。如果反应体系是对含有reference allele的模板扩增偏向强烈，那么变异碱基的信息会变小，从而会导致假阴。 PCR对真实的变异检测和个体的基因型判断都有不好的影响。GATK、Samtools、Platpus等这种利用贝叶斯原理的变异检测算法都是认为所用的序列数据都不是重复序列（即将它们和其他序列一视同仁地进行变异的判断，所以带来误导），因此必须要进行标记（去除）或者使用PCR-Free的测序方案（这个方案目前正变得越来越流行，特别是对于RNA-Seq来说尤为重要，现在著名的基因组学研究所——Broad Institute，基本都是使用PCR-Free的测序方案）。 那么具体是如何做到去除这些PCR重复序列的呢？我们可以抛开任何工具，仔细想想，既然PCR扩增是把同一段DNA序列复制出很多份，那么这些序列在经过比对之后它们一定会定位到基因组上相同的位置，比对的信息看起来也将是一样的！于是，我们就可以根据这个特点找到这些重复序列了！ 事实上，现有的工具包括Samtools和Picard中去除重复序列的算法也的确是这么做的。不同的地方在于，samtools的rmdup是直接将这些重复序列从比对BAM文件中删除掉，而Picard的MarkDuplicates默认情况则只是在BAM的FLAG信息中标记出来，而不是删除，因此这些重复序列依然会被留在文件中，只是我们可以在变异检测的时候识别到它们，并进行忽略。 考虑到尽可能和现在主流的做法一致（但我并不是说主流的做法就一定是对的，要分情况看待，只是主流的做法容易被做成生产流程而已），我们这里也用Picard来完成这个事情： 1234java -jar picard.jar MarkDuplicates \ I=sample_name.sorted.bam \ O=sample_name.sorted.markdup.bam \ M=sample_name.markdup_metrics.txt 这里只把重复序列在输出的新结果中标记出来，但不删除。如果我们非要把这些序列完全删除的话可以这样做：12345java -jar picard.jar MarkDuplicates \ REMOVE_DUPLICATES=true \ I=sample_name.sorted.bam \ O=sample_name.sorted.markdup.bam \ M=sample_name.markdup_metrics.txt 把参数REMOVE_DUPLICATES设置为ture，那么重复序列就被删除掉，不会在结果文件中留存。我比较建议使用第一种做法，只是标记出来，并留存这些序列，以便在你需要的时候还可以对其做分析。 这一步完成之后，我们需要为sample_name.sorted.markdup.bam创建索引文件，它的作用能够让我们可以随机访问这个文件中的任意位置，而且后面的“局部重比对”步骤也要求这个BAM文件一定要有索引，命令如下： 1$ samtools index sample_name.sorted.markdup.bam 完成之后，会生成一份sample_name.sorted.markdup.bam.bai文件，这就是上面这份BAM的index。 局部重比对 接下来是局部区域重比对，通常也叫Indel局部区域重比对。有时在进行这一步骤之前还有一个merge的操作，将同个样本的所有比对结果合并成唯一一个大的BAM文件【注】，merge的例子如下： 1$ samtools merge &lt;out.bam&gt; &lt;in1.bam&gt; [&lt;in2.bam&gt; ... &lt;inN.bam&gt;] 【注意】之所以会有这种情况，是因为有些样本测得非常深，其测序结果需要经过多次测序（或者分布在多个不同的测序lane中）才全部获得，这个时候我们一般会先分别进行比对并去除重复序列后再使用samtools进行合并。 局部重比对的目的是将BWA比对过程中所发现有 潜在序列插入或者序列删除（insertion和deletion，简称Indel）的区域进行重新校正。这个过程往往还会把一些已知的Indel区域一并作为重比对的区域，但为什么需要进行这个校正呢？ 其根本原因来自于参考基因组的序列特点和BWA这类比对算法本身，注意这里不是针对BWA，而是针对所有的这类比对算法，包括bowtie等。这类在全局搜索最优匹配的算法在存在Indel的区域及其附近的比对情况往往不是很准确，特别是当一些存在长Indel、重复性序列的区域或者存在长串单一碱基（比如，一长串的TTTT或者AAAAA等）的区域中更是如此。 另一个重要的原因是在这些比对算法中，对碱基错配和开gap的容忍度是不同的。具体体现在罚分矩阵的偏向上，例如，在read比对时，如果发现碱基错配和开gap都可以的话，它们会更偏向于错配。但是这种偏向错配的方式，有时候却还会反过来引起错误的开gap！这就会导致基因组上原本应该是一个长度比较大的Indel的地方，被错误地切割成多个错配和短indel的混合集，这必然会让我们检测到很多错误的变异。而且，这种情况还会随着所比对的read长度的增长（比如三代测序的Read，通常都有几十kbp）而变得越加严重。 因此，我们需要有一种算法来对这些区域进行局部的序列重比对。这个算法通常就是大名鼎鼎的Smith-Waterman算法，它非常适合于这类场景，可以极其有效地实现对全局比对结果的校正和调整，最大程度低地降低由全局比对算法的不足而带来的错误。而且GATK的局部重比对模块，除了应用这个算法之外，还会对这个区域中的read进行一次局部组装，把它们连接成为长度更大的序列，这样能够更进一步提高局部重比对的准确性。 下图给大家展示一个序列重比对之前和之后的结果，其中灰色的横条指的是read，空白黑线指的是deletion，有颜色的碱基指的是错配碱基。 Indel局部重比对的前后的对比 相信大家都可以明显地看到在序列重比对之前，在这个区域的比对数据是多么的糟糕，如果就这样进行变异检测，那么一定会得到很多假的结果。而在经过局部重比对之后，这个区域就变得非常清晰而分明，它原本发生的就只是一个比较长的序列删除（deletion）事件，但在原始的比对结果中却被错误地用碱基错配和短的Indel所代替。 说到这里，那么具体该怎么做呢？我们的WGS分析流程从这个步骤开始就需要用到GATK (GenomeAnalysisTK.jar)了，我们的执行命令如下： 12345678910111213141516java -jar /path/to/GenomeAnalysisTK.jar \ -T RealignerTargetCreator \ -R /path/to/human.fasta \ -I sample_name.sorted.markdup.bam \ -known /path/to/gatk/bundle/1000G_phase1.indels.b37.vcf \ -known /path/to/gatk/bundle/Mills_and_1000G_gold_standard.indels.b37.vcf \ -o sample_name.IndelRealigner.intervals java -jar /path/to/GenomeAnalysisTK.jar \ -T IndelRealigner \ -R /path/to/human.fasta \ -I sample_name.sorted.markdup.bam \ -known /path/to/gatk/bundle/1000G_phase1.indels.b37.vcf \ -known /path/to/gatk/bundle/Mills_and_1000G_gold_standard.indels.b37.vcf \ -o sample_name.sorted.markdup.realign.bam \ --targetIntervals sample_name.IndelRealigner.intervals 这里包含了两个步骤： 第一步，RealignerTargetCreator ，目的是定位出所有需要进行序列重比对的目标区域（如下图）； 第二步，IndelRealigner，对所有在第一步中找到的目标区域运用算法进行序列重比对，最后得到捋顺了的新结果。 IndelRealigner.intervals文件内容示例 以上这两个步骤是缺一不可的，顺序也是固定的。而且，需要指出的是，这里的-R参数输入的human.fasta不是BWA比对中的索引文件前缀，而是参考基因组序列（FASTA格式）文件，下同。 另外，在重比对步骤中，我们还看到了两个陌生的VCF文件，分别是：1000G_phase1.indels.b37.vcf和Mills_and_1000G_gold_standard.indels.b37.vcf。这两个文件来自于千人基因组和Mills项目，里面记录了那些项目中检测到的人群Indel区域。我上面其实也提到了，候选的重比对区除了要在样本自身的比对结果中寻找之外，还应该把人群中已知的Indel区域也包含进来，而这两个是我们在重比对过程中最常用到的。这些文件你可以很方便地在GATK bundle ftp中下载，注意一定要选择和你的参考基因组对应的版本，我们这里用的是b37版本。 GATK bundle 那么既然Indel局部重比对这么好，这么重要，似乎看起来在任何情况下都应该是必须的。然鹅，我的回答是否定的！惊讶吗！ 但否定是有前提的！那就是我们后面的变异检测必须是使用GATK，而且必须使用GATK的HaplotypeCaller模块，仅当这个时候才可以减少这个Indel局部重比对的步骤。原因是GATK的HaplotypeCaller中，会对潜在的变异区域进行相同的局部重比对！但是其它的变异检测工具或者GATK的其它模块就没有这么干了！所以切记！ 重新校正碱基质量值（BQSR） 在WGS分析中，变异检测是一个极度依赖测序碱基质量值的步骤。因为这个质量值是衡量我们测序出来的这个碱基到底有多正确的重要（甚至是唯一）指标。它来自于测序图像数据的base calling。因此，基本上是由测序仪和测序系统来决定的。但不幸的是，影响这个值准确性的系统性因素有很多，包括物理和化学等对测序反应的影响，甚至连仪器本身和周围环境都是其重要的影响因素。当把所有这些东西综合在一起之后，往往会发现计算出来的碱基质量值要么高于真实结果，要么低于真实结果。那么，我们到底该如何才能获得符合真实情况的碱基质量值？ BQSR（Base Quality Score Recalibration）这个步骤就是为此而存在的，这一步同样非常重要。它主要是通过机器学习的方法构建测序碱基的错误率模型，然后对这些碱基的质量值进行相应的调整。 BQSR质量校正对比 图中，横轴（Reported quality score）是测序结果在Base calling之后报告出来的质量值，也就是我们在FASTQ文件中看到的那些；纵轴（Empirical quality score）代表的是“真实情况的质量值”。 但是且慢，这个“真实情况的质量值”是怎么来的？因为实际上我们并没有办法直接测得它们啊！没错，确实没办法直接测量到，但是我们可以通过统计学的技巧获得极其接近的分布结果（因此我加了引号）。试想一下，如果我们在看到某一个碱基报告的质量值是20时，那么它的预期错误率是1%，反过来想，就等于是说如果有100个质量值都是20的碱基，那么从统计上讲它们中将只有1个是错的！做了这个等效变换之后，我们的问题就可以转变成为寻找错误碱基的数量了。 这时问题就简单多了。我们知道人与人之间的差异其实是很小的，那么在一个群体中发现的已知变异，在某个人身上也很可能是同样存在的。因此，这个时候我们可以对比对结果进行直接分析，首先排除掉所有的已知变异位点，然后计算每个（报告出来的）质量值下面有多少个碱基在比对之后与参考基因组上的碱基是不同的，这些不同碱基就被我们认为是错误的碱基，它们的数目比例反映的就是真实的碱基错误率，换算成Phred score（Phred score的定义可以参考第2节的相关内容）之后，就是纵轴的Empirical quality score了。 上面‘BQSR质量校正对比’的图中左边是原始质量值与真实质量值的比较，在这个图的例子中我们可以发现，base calling给出的质量值并没有正确地反映真实的错误率情况，测序报告出来的碱基质量值大部分被高估了，换句话说，就是错误率被低估了。 在我们的流程中，BQSR的具体执行命令如下： 123456789101112131415java -jar /path/to/GenomeAnalysisTK.jar \ -T BaseRecalibrator \ -R /path/to/human.fasta \ -I sample_name.sorted.markdup.realign.bam \ --knownSites /path/to/gatk/bundle/1000G_phase1.indels.b37.vcf \ --knownSites /path/to/gatk/bundle/Mills_and_1000G_gold_standard.indels.b37.vcf \ --knownSites /path/to/gatk/bundle/dbsnp_138.b37.vcf \ -o sample_name.recal_data.table java -jar /path/to/GenomeAnalysisTK.jar \ -T PrintReads \ -R /path/to/human.fasta \ -I sample_name.sorted.markdup.realign.bam \ --BQSR sample_name.recal_data.table \ -o sample_name.sorted.markdup.realign.BQSR.bam 这里同样包含了两个步骤： 第一步，BaseRecalibrator，这里计算出了所有需要进行重校正的read和特征值，然后把这些信息输出为一份校准表文件（sample_name.recal_data.table） 第二步，PrintReads，这一步利用第一步得到的校准表文件（sample_name.recal_data.table）重新调整原来BAM文件中的碱基质量值，并使用这个新的质量值重新输出一份新的BAM文件。 注意，因为BQSR实际上是为了（尽可能）校正测序过程中的系统性错误，因此，在执行的时候是按照不同的测序lane或者测序文库来进行的，这个时候@RG信息（BWA比对时所设置的）就显得很重要了，算法就是通过@RG中的ID来识别各个独立的测序过程，这也是我开始强调其重要性的原因。 变异检测 事实上，这是目前所有WGS数据分析流程的一个目标——获得样本准确的变异集合。这里变异检测的内容一般会包括：SNP、Indel，CNV和SV等，这个流程中我们只做其中最主要的两个：SNP和Indel。我们这里使用GATK HaplotypeCaller模块对样本中的变异进行检测，它也是目前最适合用于对二倍体基因组进行变异（SNP+Indel）检测的算法。 HaplotypeCaller和那些直接应用贝叶斯推断的算法有所不同，它会先推断群体的单倍体组合情况，计算各个组合的几率，然后根据这些信息再反推每个样本的基因型组合。因此它不但特别适合应用到群体的变异检测中，而且还能够依据群体的信息更好地计算每个个体的变异数据和它们的基因型组合。 一般来说，在实际的WGS流程中对HaplotypeCaller的应用有两种做法，差别只在于要不要在中间生成一个gVCF： （1）直接进行HaplotypeCaller，这适合于单样本，或者那种固定样本数量的情况，也就是执行一次HaplotypeCaller之后就老死不相往来了。否则你会碰到仅仅只是增加一个样本就得重新运行这个HaplotypeCaller的坑爹情况（即，N+1难题），而这个时候算法需要重新去读取所有人的BAM文件，这将会是一个很费时间的痛苦过程； （2）每个样本先各自生成gVCF，然后再进行群体joint-genotype。这其实就是GATK团队为了解决（1）中的N+1难题而设计出来的模式。gVCF全称是genome VCF，是每个样本用于变异检测的中间文件，格式类似于VCF，它把joint-genotype过程中所需的所有信息都记录在这里面，文件无论是大小还是数据量都远远小于原来的BAM文件。这样一旦新增加样本也不需要再重新去读取所有人的BAM文件了，只需为新样本生成一份gVCF，然后重新执行这个joint-genotype就行了。 我们先以第一种（直接HaplotypeCaller）做法为例子： 1234567891011121314java -jar /path/to/GenomeAnalysisTK.jar \ -T HaplotypeCaller \ -R /path/to/human.fasta \ -I sample_name.sorted.markdup.realign.BQSR.bam \ -D /path/to/gatk/bundle/dbsnp_138.b37.vcf \ -stand_call_conf 50 \ -A QualByDepth \ -A RMSMappingQuality \ -A MappingQualityRankSumTest \ -A ReadPosRankSumTest \ -A FisherStrand \ -A StrandOddsRatio \ -A Coverage \ -o sample_name.HC.vcf 这里我特别提一下-D参数输入的dbSNP同样可以再GATK bundle目录中找到，这份文件汇集的是目前几乎所有的公开人群变异数据集。另外，由于我们的例子只有一个样本因此只输入一个BAM文件就可以了，如果有多个样本那么可以继续用-I参数输入： 12345java -jar GenomeAnalysisTK.jar \ -T HaplotypeCaller \ -R reference.fasta \ -I sample1.bam [-I sample2.bam ...] \ ... 以上的命令是直接对全基因组做变异检测，这个过程会消耗很长的时间，通常需要几十个小时甚至几天。 然而，基因组上各个不同的染色体之间其实是可以理解为相互独立的（结构性变异除外），也就是说，为了提高效率我们可以按照染色体一条条来独立执行这个步骤，最后再把结果合并起来就好了，这样的话就能够节省很多的时间。下面我给出一个按照染色体区分的例子： 123456789101112131415java -jar /path/to/GenomeAnalysisTK.jar \ -T HaplotypeCaller \ -R /path/to/human.fasta \ -I sample_name.sorted.markdup.realign.BQSR.bam \ -D /path/to/gatk/bundle/dbsnp_138.b37.vcf \ -L 1 \ -stand_call_conf 50 \ -A QualByDepth \ -A RMSMappingQuality \ -A MappingQualityRankSumTest \ -A ReadPosRankSumTest \ -A FisherStrand \ -A StrandOddsRatio \ -A Coverage \ -o sample_name.HC.1.vcf 注意到了吗？其它参数都没任何改变，就只增加了一个 -L 参数，通过这个参数我们可以指定特定的染色体（或者基因组区域）！我们这里指定的是 1 号染色体，有些地方会写成chr1，具体看human.fasta中如何命名，与其保持一致即可。其他染色体的做法也是如此，就不再举例了。最后合并： 123456789java -jar /path/to/GenomeAnalysisTK.jar \ -T CombineVariants \ -R /path/to/human.fasta \ --genotypemergeoption UNSORTED \ --variant sample_name.HC.1.vcf \ --variant sample_name.HC.2.vcf \ ... --variant sample_name.HC.MT.vcf \ -o sample_name.HC.vcf 第二种，先产生gVCF，最后再joint-genotype的做法： 12345678910111213java -jar /path/to/GenomeAnalysisTK.jar \ -T HaplotypeCaller \ -R /path/to/human.fasta \ -I sample_name.sorted.markdup.realign.BQSR.bam \ --emitRefConfidence GVCF \ -o sample_name.g.vcf #调用GenotypeGVCFs完成变异callingjava -jar /path/to/GenomeAnalysisTK.jar \ -T GenotypeGVCFs \ -R /path/to/human.fasta \ --variant sample_name.g.vcf \ -o sample_name.HC.vcf 其实，就是加了–emitRefConfidence GVCF的参数。而且，假如嫌慢，同样可以按照染色体或者区域去产生一个样本的gVCF，然后在GenotypeGVCFs中把它们全部作为输入文件完成变异calling。也许你会担心同个样本被分成多份gVCF之后，是否会被当作不同的多个样本？回答是不会！因为生成gVCF文件的过程中，GATK会根据@RG信息中的SM（也就是sample name）来判断这些gVCF是否来自同一个样本，如果名字相同，那么就会被认为是同一个样本，不会产生多样本问题。 变异检测质控和过滤（VQSR）这是我们这个流程中最后的一步了。在获得了原始的变异检测结果之后，我们还需要做的就是质控和过滤。这一步或多或少都有着一些个性化的要求，我暂时就不做太多解释吧（一旦解释恐怕同样是一篇万字长文）。只用一句话来概括，VQSR是通过构建GMM模型对好和坏的变异进行区分，从而实现对变异的质控，具体的原理暂时不展开了。 下面就直接给出例子吧： 12345678910111213141516171819202122232425262728293031323334353637383940414243## SNP Recalibratorjava -jar /path/to/GenomeAnalysisTK.jar \ -T VariantRecalibrator \ -R reference.fasta \ -input sample_name.HC.vcf \ -resource:hapmap,known=false,training=true,truth=true,prior=15.0 /path/to/gatk/bundle/hapmap_3.3.b37.vcf \ -resource:omini,known=false,training=true,truth=false,prior=12.0 /path/to/gatk/bundle/1000G_omni2.5.b37.vcf \ -resource:1000G,known=false,training=true,truth=false,prior=10.0 /path/to/gatk/bundle/1000G_phase1.snps.high_confidence.b37.vcf \ -resource:dbsnp,known=true,training=false,truth=false,prior=6.0 /path/to/gatk/bundle/dbsnp_138.b37.vcf \ -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP \ -mode SNP \ -recalFile sample_name.HC.snps.recal \ -tranchesFile sample_name.HC.snps.tranches \ -rscriptFile sample_name.HC.snps.plots.Rjava -jar /path/to/GenomeAnalysisTK.jar -T ApplyRecalibration \ -R human_g1k_v37.fasta \ -input sample_name.HC.vcf \ --ts_filter_level 99.5 \ -tranchesFile sample_name.HC.snps.tranches \ -recalFile sample_name.HC.snps.recal \ -mode SNP \ -o sample_name.HC.snps.VQSR.vcf## Indel Recalibratorjava -jar /path/to/GenomeAnalysisTK.jar -T VariantRecalibrator \ -R human_g1k_v37.fasta \ -input sample_name.HC.snps.VQSR.vcf \ -resource:mills,known=true,training=true,truth=true,prior=12.0 /path/to/gatk/bundle/Mills_and_1000G_gold_standard.indels.b37.vcf \ -an QD -an DP -an FS -an SOR -an ReadPosRankSum -an MQRankSum \ -mode INDEL \ -recalFile sample_name.HC.snps.indels.recal \ -tranchesFile sample_name.HC.snps.indels.tranches \ -rscriptFile sample_name.HC.snps.indels.plots.Rjava -jar /path/to/GenomeAnalysisTK.jar -T ApplyRecalibration \ -R human_g1k_v37.fasta\ -input sample_name.HC.snps.VQSR.vcf \ --ts_filter_level 99.0 \ -tranchesFile sample_name.HC.snps.indels.tranches \ -recalFile sample_name.HC.snps.indels.recal \ -mode INDEL \ -o sample_name.HC.snps.indels.VQSR.vcf 最后，sample_name.HC.snps.indels.VQSR.vcf便是我们最终的变异检测结果。对于人类而言，一般来说，每个人最后检测到的变异数据大概在400万左右（包括SNP和Indel）。 这篇文章已经很长了，在变异检测的这个过程中GATK应用了很多重要的算法，包括如何构建模型、如何进行局部组装和比对、如何应用贝叶斯、PariHMM、GMM、参数训练、特征选择等等，这些只能留在后面介绍GATK的专题文章中再进行展开了。 小结在这里，整篇文章就结束了。如你所见，文章非常长，这里基本包含了WGS最佳实践中的所有内容，但其实我想说的还远不止如此（包括CNV和SV的检测），只是暂时只能作罢了，否则恐怕就没人愿意看下去了，呵呵。在这个WGS主流程的构建过程中，我并非只是硬邦邦地告诉大家几条简单的命令就了事了，因为我认为那种做法要么是极其不负责任的，要么就是作者并非真的懂。而且如果都觉得只要懂得几条命令就可以了的话，那么我们就活该被机器和人工智能所取替，它们一定会操作得更好更高效。我想掌握工具和技术的目的是为了能够更好地发现并解决问题（包括科研和生产），所有的数据分析流程本质上是要服务于我们所要解决的问题的。 毕竟工具是死的，人是活的，需求总是会变的。理解我们所要处理的问题的本质，选择合适的工具，而不是反过来被工具所束缚，这一点很重要。个人的能力不能只是会跑一个流程，或者只是会创建流程，因为那都是一个“术”的问题，我觉得我们真正要去掌握的应该是如何分析数据的能力，如何发现问题和解决数据问题等的能力。 本文首发于我的个人公众号：解螺旋的矿工，欢迎扫码关注，更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>WGS</tag>
        <tag>流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始完整学习全基因组测序（WGS）数据分析：第3节 数据质控]]></title>
    <url>%2F2017%2F08%2F25%2F2017-08-25-Begining-WGS-Data-Analysis-Fastq-Data-Quality-Control.html</url>
    <content type="text"><![CDATA[从这一节开始详细讲述正式流程的搭建，我将结合具体的例子努力争取将这个系列写成比GATK最佳实践更加具体、更具有实践价值的入门指南。整个完整的流程分为以下6部分： 原始测序数据的质控 read比对，排序和去除重复序列 Indel区域重（“重新”的“重”）比对 碱基质量值重校正 变异检测 变异结果质控和过滤 在这个图中，我把WGS数据分析流程的各个步骤和关系都画下来了。这个流程虽然只针对于人，但对于其它二倍体生物来说，同样具有借鉴价值。这6个步骤，接下来我也会进行详细介绍，在本篇文章中我们首先介绍原始测序数据的质控。 认识测序数据——数据质控的意义在第1节测序技术中，我们已经知道现在的NGS测序，以illumina为首基本都是运用边合成边测序的技术。碱基的合成依靠的是化学反应，这使得碱基链可以不断地从5’端一直往3’端合成并延伸下去。但在这个合成的过程中随着合成链的增长，DNA聚合酶的效率会不断下降，特异性也开始变差，这就会带来一个问题——越到后面碱基合成的错误率就会越高【注】，这也是为何当前NGS测序读长普遍偏短的一个原因。 【注】：有时候测序仪在刚开始进行合成反应的时候也会由于反应还不够稳定，同样会带来质量值的波动，不过这个波动一般都在高质量值区域（如下图）。 测序数据的质量好坏会影响我们的下游分析。但不同的测序平台其测序错误率的图谱都是有差别的。因此，非常建议在我们分析测序数据之前先搞清楚如下两个地方： 原始数据是通过哪种测序平台产生的，它们的错误率分布是怎么样的，是否有一定的偏向性和局限性，是否会显著受GC含量的影响等； 评估它们有可能影响哪些方面的分析； 第一点是我们认识数据质量的第一步，也是我们一定要去知道的地方。除了看官方的资料之外，最好的做法是自己分析。 虽然随着NGS测序数据变得越来越普遍，整体的测序质量和错误率分布情况大家也都了解一些。在实际的工作中，我也常常发现很多人其实并不十分关心这个数据到底长啥样，拿到之后，就直接跑过滤流程，也不管这些参数或者工具是否真的是合适的，更加不看看过滤后的数据和过滤前到底有什么不同。尽管，大多数情况下问题不大，但是我想跟大家说的是： 认识你的数据，不要相信你的工具！ 这样也能够更好地避开很多不必要的坑。 因此，在本文中我将谈谈该如何更好地去认识一个测序数据，而不只是简单地告诉大家一个质控流程，当然，这也是本篇文章将要进行介绍的内容。至于第二点其实需要视情况而定，例如你的测序深度是多少，检测变异的时候，变异的位点是否过于集中在read的末尾，比对的时候是否会出现了一定的正反链偏向性等诸如此类的问题；又或者我们在进行基因组序列组装的时候，由于对read的中出错的碱基更加敏感，因此往往需要进行更严格的切除，不然会由于这些错误的碱基消耗更大的计算资源和时间。 那么说大地该如何认识一个原始的测序数据（fastq data）呢？一般我们可以从如下几个方面来分析： read各个位置的碱基质量值分布 碱基的总体质量值分布 read各个位置上碱基分布比例，目的是为了分析碱基的分离程度 GC含量分布 read各位置的N含量 read是否还包含测序的接头序列 read重复率，这个是实验的扩增过程所引入的 以上，这几个地方都弄明白了，那么这个数据的基本情况也就差不多都清楚了。 我们首先来说说read各位置的碱基质量分布。在第2节里面，我们已经知道该如何通过简单的Python代码计算出read的质量值和碱基的测序错误率了。但对于成千上万的read来说，这样做并不合适，我们需要更直观的表达方式——画出来，正所谓 一图胜千言！目前也有很多现成的工具可以高效地来完成这样的事情，比如用得最广的FastQC，它是一个java程序，能够用于给出测序数据的QC报告，报告中会同时给出上述几个方面的数据图，并提示原来的数据可能还存在着哪些问题。它可以很好地帮助我们理解测序数据的质量情况，但缺点就是 图！太！丑！ 在做read质量值分析的时候，FastQC并不单独查看具体某一条read中碱基的质量值，而是将Fastq文件中所有的read数据都综合起来一起分析。下图是一个测序质量非常好的read各位置碱基质量分布图（如下图）。 这个图的横轴是read上碱基的位置，纵轴是碱基质量值。在这个例子中，read的长度是126bp（来自HiSeq X10的测序结果），这应该算是比较长的二代测序序列了。我们可以看到read上的每一个位置都有一个黄色的箱型图表示在该位置上所有碱基的质量分布情况。除了最后一个碱基之外，其他的碱基质量值都基本都在大于30，而且波动很小，说明质量很稳定，这其实是一个非常高质量的结果。而且我们可以看到图中质量值的分布都在绿色背景（代表高质量）的区域。 那如果是质量很差的结果看起来会是怎么样的呢？我手边一时找不到这样的数据，就在网上找到了一个代替品，样子如下： 在这个图中我们可以明显看到，read各个位置上的碱基质量分布波动都比较大，特别从第18个碱基往后全部出现了大幅度的波动，而且很多read的碱基质量值都掉到非常低（红色）的区域中了，说明这个数据的测序结果真的非常差，有着大量不及格的read。最好的情况是重新测序，但如果不得不使用这个数据，就要把这些低质量的数据全都去除掉才行，同时还需留意是否还存在其他的问题，但不管如何都一定会丢掉很大一部分的数据。 除了上面read各位置的碱基质量值分布之外，FastQC还会为我们计算其他几个非常有价值的统计结果，包括： 1）碱基总体质量值分布，只要大部分都高于20，那么就比较正常。 在第2节里面我也提到了关于Q20和Q30的比例是我们衡量测序质量的一个重要指标。这其实也是从这里来进行体现的，一般来说，对于二代测序，最好是达到Q20的碱基要在95%以上（最差不低于90%），Q30要求大于85%（最差也不要低于80%）。 2）read各个位置上碱基比例分布 这个是为了分析碱基的分离程度。何为碱基分离？我们知道AT配对，CG配对，假如测序过程是比较随机的话（随机意味着好），那么在每个位置上A和T比例应该差不多，C和G的比例也应该差不多，如上图所示，两者之间即使有偏差也不应该太大，最好平均在1%以内，如果过高，除非有合理的原因，比如某些特定的捕获测序所致，否则都需要注意是不是测序过程有什么偏差。 3）GC含量分布图 GC含量指的是G和C这两种碱基占总碱基的比例。二代测序平台或多或少都存在一定的测序偏向性，我们可以通过查看这个值来协助判断测序过程是否足够随机。对于人类来说，我们基因组的GC含量一般在40%左右。因此，如果发现GC含量的图谱明显偏离这个值那么说明测序过程存在较高的序列偏向性，结果就是基因组中某些特定区域被反复测序的几率高于平均水平，除了覆盖度会有偏离之后，将会影响下游的变异检测和CNV分析。 4）N含量分布图 N在测序数据中一般是不应该出现的，如果出现则意味着，测序的光学信号无法被清晰分辨，如果这种情况多的话，往往意味着测序系统或者测序试剂的错误。 5）接头序列 在第1节 测序技术里面我们提到了在测序之前需要构建测序文库，测序接头就是在这个时候加上的，其目的一方面是为了能够结合到flowcell上，另一方面是当有多个样本同时测序的时候能够利用接头信息进行区分。当测序read的长度大于被测序的DNA片段【注】时，就会在read的末尾测到这些接头序列（如下图）。一般的WGS测序是不会测到这些接头序列的，因为构建WGS测序的文库序列（插入片段）都比较长，约几百bp，而read的测序长度都在100bp-150bp这个范围。不过在进行一些RNA测序的时候，由于它们的序列本来就比较短，很多只有几十bp长（特别是miRNA），那么就很容易会出现read测通的现象，这个时候就会在read的末尾测到这些接头序列。 【注】这些DNA片段也常被我们称之为“插入片段” 最后，这些被测到的接头序列和低质量碱基一样都是需要在正式分析之前进行切除的read片段。 当我们看完了上面的这些结果之后就可以比较清楚地了解一个测序数据的概况了。 那么，说了这么多，上述提到的FastQC该怎么用呢？ FastQC的安装非常简单，我们可以通过网页搜索或者直接到它的主页上下载最新的版本。 也可以在终端通过wget命令下载： 1$ wget https://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.11.5.zip ./ 解压之后，修改文件夹中fastqc的权限，就可以直接运行了： 123$ unzip fastqc_v0.11.5.zip$ cd FastQC $ chmod 755 fastqc FastQC的运行非常简单，直接在终端通过命令行是最有效直接的，下面我给出一个例子： 1$ /path_to_fastqc/FastQC/fastqc untreated.fq -o fastqc_out_dir/ 命令比较简单，这里 唯一值得注意的地方就是 -o 参数用于指定FastQC报告的输出目录，这个目录需要事先创建好，如果不指定特定的目录，那么FastQC的结果会默认输出到文件untreated.fq的同一个目录下。它输出结果只有两个，一个html和一个.zip压缩包。 1$ tree fastqc_out_dir/ 我们可以直接通过浏览器打开html，就可以看到FastQC给出的所有结果，zip压缩包解压后，从中我们也可以在对应的目录下找到所有的QC图表和Summary数据。 除了上述用法之外，FastQC支持同时输入多个fq文件（或者以通配符的形式输入fq），当我们的fq文件比较多时，这种用法会比较方便，如： 1$ /path_to_fastqc/FastQC/fastqc /path_to_fq/*.fq -o fastqc_out_dir/ 这个链接是FastQC官网给出的一个Online报告的模板，方便参考。 切除测序接头序列和read的低质量序列前面关于如何认识fq数据的事情已经说完了，接下来是我们本篇文章中最后的一个重点——去除测序接头和低质量序列！ 当我们理解了fq数据之后，做这些过滤就不会很难，你也完全可以自己编写工具来进行个性化的过滤。目前也已有很多工具用来切除接头序列和低质量碱基，比如SOAPnuke、cutadapt、untrimmed等不下十个，但这其中比较方便好用的是Trimmomatic（也是一个java程序）、sickle和seqtk。Trimmomatic的好处在于，它不但可以用来切除illumina测序平台的接头序列，还可以去除由我们自己指定的特定接头序列，而且同时也能够过滤read末尾的低质量序列，sickle和seqtk只能去除低质量碱基。具体的原理就是通过滑动一定长度的窗口，计算窗口内的碱基平均质量，如果过低，就直接 往后全部切除，注！意！不是挖掉read中的这部分低质量序列，而是像切菜一样，直接从低质量区域开始把这条read后面的所有其它碱基全！部！剁！掉！否则就是在人为改变实际的基因组序列情况。 如果下机的fq数据中不含有这些测序接头，那么我们除了trimmomatic之外，也可以直接使用sickle（同时支持PE和SE数据）或者seqtk（仅支持SE），这两个处理起来会更快，消耗的计算资源也更少。 现在我们说回如何用Trimmomatic构造序列过滤流程。 首先是安装Trimmomatic。我们可以到它的官网上获取最新的版本，下载打包好的binary即可，如果打算看它具体的代码，可以在github上找到。 下载后，直接解压，目录下的trimmomatic-*.jar（我下载的是0.36版本）就是执行程序，可以直接使用java来运行。 1$ java -jar trimmomatic-0.36.jar 同个目录下还有一个名为adapters的文件夹，这个文件夹中的内容对于我们去除接头序列来说非常重要。其中默认存放的是illumina测序平台的接头序列（fasta格式），在实际的使用过程中，如果需要去除接头，我们需要明确指定对应的序列作为输入参数。 那么这些接头序列具体该如何选择呢？一般来说，目前的HiSeq系列和MiSeq系列用的都是TruSeq3，TruSeq2是以前GA2系列的测序仪所用的，已经很少见了。这些信息都可以在illumina的官网上找到，至于具体该用PE（Pair End）还是SE（Single End）就按照具体的测序类型进行选择就ok了。如果用的不是illumina测序平台，那么我们也可以按照adapters文件夹下的这些文件的格式做一个新的接头序列，然后再作为参数传入。不过在自定义接头序列的时候，命名时有一些小的细节需要注意，可以参考Trimmomatic的主页文档（The Adapter Fasta），这里就不展开了。 Trimmomatic有两种运行模式：PE和SE。顾名思义，PE就是对应Pair End测序的，SE则是对应Single End测序的。 1234567$ java -jar trimmomatic-0.36.jarUsage: PE [-version] [-threads &lt;threads&gt;] [-phred33|-phred64] [-trimlog &lt;trimLogFile&gt;] [-quiet] [-validatePairs] [-basein &lt;inputBase&gt; | &lt;inputFile1&gt; &lt;inputFile2&gt;] [-baseout &lt;outputBase&gt; | &lt;outputFile1P&gt; &lt;outputFile1U&gt; &lt;outputFile2P&gt; &lt;outputFile2U&gt;] &lt;trimmer1&gt;... or: SE [-version] [-threads &lt;threads&gt;] [-phred33|-phred64] [-trimlog &lt;trimLogFile&gt;] [-quiet] &lt;inputFile&gt; &lt;outputFile&gt; &lt;trimmer1&gt;... or: -version 下面我分别给出例子来进行说明： PE模式，HiSeq PE测序： 1$ java -jar /path/Trimmomatic/trimmomatic-0.36.jar PE -phred33 -trimlog logfile reads_1.fq.gz reads_2.fq.gz out.read_1.fq.gz out.trim.read_1.fq.gz out.read_2.fq.gz out.trim.read_2.fq.gz ILLUMINACLIP:/path/Trimmomatic/adapters/TruSeq3-PE.fa:2:30:10 SLIDINGWINDOW:5:20 LEADING:5 TRAILING:5 MINLEN:50 SE模式，HiSeq SE测序：1$ java -jar /path/Trimmomatic/trimmomatic-0.36.jar SE -phred33 -trimlog se.logfile raw_data/untreated.fq out.untreated.fq.gz ILLUMINACLIP:/path/Trimmomatic/adapters/TruSeq3-SE.fa:2:30:10 SLIDINGWINDOW:5:20 LEADING:5 TRAILING:5 MINLEN:50 我们可以看到PE和SE，顾名思义，分别代表了 ‘PE模式’和‘SE’模式。 同时需要明确指明质量值体系是Phred33还是Phred64，默认是Phred64，这需要特别注意，因为我们现在的测序数据基本都是Phred33的了，所以一定要指定这个参数。 剩下的就是输入的fq和输出的fq，可以用-basein和-baseout指定，也可以不用（如上例子），以及被过滤掉的fq要输出到文件。细心的读者可能已经发现这里PE和SE有一个区别： 在SE模式中，是不需要指定文件来存放被过滤掉的read信息的，后面直接就接Trimmer信息！这是需要注意到的一个地方。 关于后面的Trimmer信息，规定了很多切除接头序列和低质量序列的细节，我挑重点的说，具体如下： ILLUMINACLIP，接头序列切除参数。LLUMINACLIP:TruSeq3-PE.fa:2:30:10（省掉了路径）意思分别是：TruSeq3-PE.fa是接头序列，2是比对时接头序列时所允许的最大错配数；30指的是要求PE的两条read同时和PE的adapter序列比对，匹配度加起来超30%，那么就认为这对PE的read含有adapter，并在对应的位置需要进行切除【注】。10和前面的30不同，它指的是，我就什么也不管，反正只要这条read的某部分和adpater序列有超过10%的匹配率，那么就代表含有adapter了，需要进行去除； 【注】测序的时候一般只会测到一部分的adapter，因此read和adaper对比的时候肯定是不需要要求百分百匹配率的，上述30%和10%其实是比较推荐的值。SLIDINGWINDOW，滑动窗口长度的参数，SLIDINGWINDOW:5:20代表窗口长度为5，窗口中的平均质量值至少为20，否则会开始切除； LEADING，规定read开头的碱基是否要被切除的质量阈值； TRAILING，规定read末尾的碱基是否要被切除的质量阈值； MINLEN，规定read被切除后至少需要保留的长度，如果低于该长度，会被丢掉。 此外，另一个值得注意的地方是，Trimmomatic的报错给出的提示信息都比较难以定位错误问题（如下图），但这往往都只是参数用没设置正确所致。12345Exception in thread "main" java.lang.RuntimeException: Unknown trimmer: u.trim.txt at org.usadellab.trimmomatic.trim.TrimmerFactory.makeTrimmer(TrimmerFactory.java:70) at org.usadellab.trimmomatic.Trimmomatic.createTrimmers(Trimmomatic.java:59) at org.usadellab.trimmomatic.TrimmomaticSE.run(TrimmomaticSE.java:303) at org.usadellab.trimmomatic.Trimmomatic.main(Trimmomatic.java:85) 小结数据质控的内容终于都讲完了，接下来第4节就是主流程的构建。 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>WGS</tag>
        <tag>数据质控</tag>
        <tag>fastq</tag>
        <tag>FastQC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始完整学习全基因组测序（WGS）数据分析：第2节 FASTA和FASTQ]]></title>
    <url>%2F2017%2F08%2F12%2F2017-08-12-Begining-WGS-Data-Analysis-Fasta-And-Fastq.html</url>
    <content type="text"><![CDATA[在WGS数据的分析过程中，我们会接触到许多生物信息学/基因组学领域所特有的数据文件和它们特殊的格式，在这一节中将要介绍的FASTA和FASTQ便是其中之一二。这是我们存储核苷酸序列信息（就是DNA序列）或者蛋白质序列信息最常使用的两种 文本文件，虽然看起来名字有些古怪，但它们完全是纯文本文件（如同.txt）！名字的发音分别是fast-A和fast-Q。这一篇文章内容虽然比较简单，但还是比较长，我在这里详细介绍了这两类文件的格式特点和一些在分析的时候需要考虑的地方。 FASTA我相信许多人（包括生物信息工程师们）一定不知道FASTA这个文件的来源，竟然是一款名叫“FASTA”的比对软件！名字中最后一个字母A，其实就是Alignment的意思！但这已经是上个世纪的事情了，最初是由William. R. Pearson 和 David. J. Lipman在1988年所编写，目的是用于生物序列数据的处理。 自那之后，生物学家和遗传学家们也没做过多的考虑，就草率地决定（其实类似的‘草率’行为在组学领域经常碰到）把FASTA作为这种存储 有顺序的序列数据的文件后缀【注】，这包括我们常用的参考基因组序列、蛋白质序列、编码DNA序列（coding DNA sequence，简称CDS）、转录本序列等文件都是如此，文件后缀除了.fasta之外，也常用.fa或者.fa.gz（gz压缩）。 【注】这里的序列、序列数据，指的其实就是表示DNA或者蛋白质的一条字符串。 这里再特别强调三个字：有！顺！序！说的是从1开始一个个按顺序往下排列的意思——这不也正是序列这个词的含义！ 因此，我们可以通过数个数，就知道某个DNA碱基在某个基因组上的准确位置，这个位置会用所在序列的名字和所在位置来表达，比如基因数据比对的结果（下一篇会介绍），方便后续数据分析。 FASTA文件主要由两个部分构成：序列头信息（有时包括一些其它的描述信息）和具体的序列数据。头信息独占一行，以大于号（&gt;）开头作为识别标记，其中除了记录该条序列的名字之外，有时候还会接上其它的信息。紧接的下一行是具体的序列内容，直到另一行碰到另一个大于号（&gt;）开头的新序列或者文件末尾。下面给出一个FASTA文件的例子，这是我们人类一个名为EGFR基因的部分序列。 12345678910&gt;ENSMUSG00000020122|ENSMUST00000138518CCCTCCTATCATGCTGTCAGTGTATCTCTAAATAGCACTCTCAACCCCCGTGAACTTGGTTATTAAAAACATGCCCAAAGTCTGGGAGCCAGGGCTGCAGGGAAATACCACAGCCTCAGTTCATCAAAACAGTTCATTGCCCAAAATGTTCTCAGCTGCAGCTTTCATGAGGTAACTCCAGGGCCCACCTGTTCTCTGGT&gt;ENSMUSG00000020122|ENSMUST00000125984GAGTCAGGTTGAAGCTGCCCTGAACACTACAGAGAAGAGAGGCCTTGGTGTCCTGTTGTCTCCAGAACCCCAATATGTCTTGTGAAGGGCACACAACCCCTCAAAGGGGTGTCACTTCTTCTGATCACTTTTGTTACTGTTTACTAACTGATCCTATGAATCACTGTGTCTTCTCAGAGGCCGTGAACCACGTCTGCAAT 可以看到，FASTA其实很简单，但它往往都很大，比如人类基因组有30亿个碱基，就是30亿个字符存储在这样的一个文本文件中，就算是压缩也要占用约1GB的存储空间。 另外，有两个地方，我觉得有必要提及： 第一，除了序列内容之外，FASTA的头信息并没有被严格地限制。这个特点有时会带来很多麻烦的事情，比如有时我们会看到相同的序列被不同的人处理之后、甚至是在不同的网站上或者数据库中它们的头信息都不尽相同，比如以下的几种情况都是可能存在的。 12345&gt;ENSMUSG00000020122|ENSMUST00000125984&gt; ENSMUSG00000020122|ENSMUST00000125984&gt;ENSMUSG00000020122|ENSMUST00000125984|epidermal growth factor receptor&gt;ENSMUSG00000020122|ENSMUST00000125984|Egfr&gt;ENSMUSG00000020122|ENSMUST00000125984|11|ENSFM00410000138465 这对于程序处理来说，凌乱的格式显然是不合适的。因此后来在业内也慢慢地有一些不成文的规则被大家所使用，那就是，用一个空格把头信息分为两个部分：第一部分是序列名字，它和大于号（&gt;）紧接在一起；第二部分是注释信息，这个可以没有，就看具体需要，比如下面这个序列例子，除了前面gene_00284728这个名字之外，注释信息（length=231;type=dna）给出这段序列的长度和它所属的序列类型。 12345&gt;gene_00284728 length=231;type=dnaGAGAACTGATTCTGTTACCGCAGGGCATTCGGATGTGCTAAGGTAGTAATCCATTATAAGTAACATGCGCGGAATATCCGGGAGGTCATAGTCGTAATGCATAATTATTCCCTCCCTCAGAAGGACTCCCTTGCGAGACGCCAATACCAAAGACTTTCGTAAGCTGGAACGATTGGACGGCCCAACCGGGGGGAGTCGGCTATACGTCTGATTGCTACGCCTGGACTTCTCTT 这对于程序处理来说，凌乱的格式显然是不合适的。因此后来在业内也慢慢地有一些不成文的规则被大家所使用，那就是，用一个空格把头信息分为两个部分：第一部分是序列名字，它和大于号（&gt;）紧接在一起；第二部分是注释信息，这个可以没有，就看具体需要，比如下面这个序列例子，除了前面gene_00284728这个名字之外，注释信息（length=231;type=dna）给出这段序列的长度和它所属的序列类型。 12345&gt;gene_00284728 length=231;type=dnaGAGAACTGATTCTGTTACCGCAGGGCATTCGGATGTGCTAAGGTAGTAATCCATTATAAGTAACATGCGCGGAATATCCGGGAGGTCATAGTCGTAATGCATAATTATTCCCTCCCTCAGAAGGACTCCCTTGCGAGACGCCAATACCAAAGACTTTCGTAAGCTGGAACGATTGGACGGCCCAACCGGGGGGAGTCGGCTATACGTCTGATTGCTACGCCTGGACTTCTCTT 虽然这样的格式还不算是真正的标准，但却非常有助于我们的数据分析和处理，很多生信软件（如：BWA，samtools，bcftools，bedtools等）都是将第一个空格前面的内容认定为序列名字来进行操作的。 第二，FASTA由于是文本文件，它里面的内容是否有重复是无法自检的，在使用之前需要我们进行额外的检查。这个检查倒不用很复杂，只需检查序列名字是否有重复即可。但对于那些已经成为标准使用的参考序列来说，都有专门的团队进行维护，因此不会出现这种内容重复的情况，可以直接使用，但对于其它的一些序列来说，谨慎起见，最好进行检查。 FASTQ这是目前存储测序数据最普遍、最公认的一个数据格式，另一个是uBam格式，但这篇文章中不打算对其进行介绍。上面所讲的FASTA文件，它所存的都是已经排列好的序列（如参考序列），FASTQ存的则是产生自测序仪的原始测序数据，它由测序的图像数据转换过来，也是文本文件，文件大小依照不同的测序量（或测序深度）而有很大差异，小的可能只有几M，大的则常常有几十G上百G，文件后缀通常都是.fastq，.fq或者.fq.gz（gz压缩），以下是它的一个例子： 12345678@DJB775P1:248:D0MDGACXX:7:1202:12362:49613TGCTTACTCTGCGTTGATACCACTGCTTAGATCGGAAGAGCACACGTCTGAA+JJJJJIIJJJJJJHIHHHGHFFFFFFCEEEEEDBD?DDDDDDBDDDABDDCA@DJB775P1:248:D0MDGACXX:7:1202:12782:49716CTCTGCGTTGATACCACTGCTTACTCTGCGTTGATACCACTGCTTAGATCGG+IIIIIIIIIIIIIIIHHHHHHFFFFFFEECCCCBCECCCCCCCCCCCCCCCC 你可以看到它有着自己独特的格式：每四行成为一个独立的单元，我们称之为read。具体的格式描述如下： 第一行：以‘@’开头，是这一条read的名字，这个字符串是根据测序时的状态信息转换过来的，中间不会有空格，它是 每一条read的唯一标识符，同一份FASTQ文件中不会重复出现，甚至不同的FASTQ文件里也不会有重复； 第二行：测序read的序列，由A，C，G，T和N这五种字母构成，这也是我们真正关心的DNA序列，N代表的是测序时那些无法被识别出来的碱基； 第三行：以‘+’开头，在旧版的FASTQ文件中会直接重复第一行的信息，但现在一般什么也不加（节省存储空间）； 第四行：测序read的质量值，这个和第二行的碱基信息一样重要，它描述的是每个测序碱基的可靠程度，用ASCII码表示。 那么，重点说一下什么是质量值？顾名思义，碱基质量值就是能够用来定量描述碱基 好坏程度的一个数值。它该如何才能恰当地描述这个结果呢？我们试想一下，如果测序测得越准确，这个碱基的质量就应该越高；反之，测得越不准确，质量值就应该越低。也就是说可以利用碱基被测错的概率来描述它的质量值，错误率越低，质量值就越高！如下图，红线代表错误率，蓝线代表质量值，这便是我们希望达到的效果： 这里我们假定碱基的测序错误率为: $${P}_{error}$$质量值为Q，它们之间的关系如下： $$Q=-10log_{10}{P}_{error}$$ 即，质量值是测序错误率的对数（10为底数）乘以-10（并取整）。这个公式也是目前测序质量值的计算公式，它非常简单，p_error的值和测序时的多个因素有关，体现为测序图像数据点的清晰程度，并由测序过程中的base calling 算法计算出来；公式右边的Q我们称之为Phred quality score，就是用它来描述测序碱基的靠谱程度。比如，如果该碱基的测序错误率是0.01，那么质量值就是20（俗称Q20），如果是0.001，那么质量值就是30（俗称Q30）。Q20和Q30的比例常常被我们用来评价某次测序结果的好坏，比例越高就越好。下面我也详细给出一个表，更进一步地解释质量值高低的含义： 测序平台 ASCII码范围 下限 质量值类型 质量值范围 备注 Sanger, Illumina(版本1.8及以上) 33-126 33 Phred quality score 0-93 现在沿用 Solexa, Illumina早期版本(&lt;1.3版本) 59-126 64 Solexa quality score 5-62 除了已测序数据之外，不再使用 Illumina(版本1.3-1.7) 64-126 64 Phred quality score 0-62 除了已测序数据之外，不再使用 现在回过头来说说为什么要用ASCII码来代表，直接用数字不行吗？行！但很难看，而且数字不能直接连起来，还得在中间加一个分隔符，长度也对不齐，还占空间，又不符合美学设计，真！麻！烦！ 因此，也是为了格式存储以及处理时的方便，这个数字被直接转换成了ASCII码，并与第二行的read序列构成一一对应的关系——每一个ASCII码都和它正上面的碱基对应，这就很完美。 不过，值得一提的是，ASCII码虽然能够从小到大表示0-127的整数，但是并非所有的ASCII码都是 可见的字符，比如所有小于33的ASCII码值所表示的都是不可见字符，比如空格，换行符等，因此 为了能够让碱基的质量值表达出来，必须避开所有这些不可见字符。最简单的做法就是加上一个固定的整数！也的确是这么干的。 但一开始对于要加哪一个整数，并没有什么指导标准，这就导致了在刚开始的时候，不同的测序平台加的整数也不同，总的来说有以下3种质量体系，演变到现在也基本只剩下第一种了，如下表： Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999% 从表中可以看到下限有33和64两个值，我们把加33的的质量值体系称之为Phred33，加64的称之为Phred64（Solexa的除外，它叫Solexa64）。不过，现在一般都是使用Phred33这个体系，而且33也恰好是ASCII的第一个可见字符（’!’），完美+2。 如果你在实际做项目的过程不知道所用的质量体系（经验丰富者是可以直接看出来的），那么可以用我下面这一段代码，简单地做个检查: 123456789less $1 | head -n 1000 | awk '&#123;if(NR%4==0) printf("%s",$0);&#125;' \| od -A n -t u1 -v \| awk 'BEGIN&#123;min=100;max=0;&#125; \ &#123;for(i=1;i&lt;=NF;i++) &#123;if($i&gt;max) max=$i; if($i&lt;min) min=$i;&#125;&#125;END \ &#123;if(max&lt;=126 &amp;&amp; min&lt;59) print "Phred33"; \ else if(max&gt;73 &amp;&amp; min&gt;=64) print "Phred64"; \ else if(min&gt;=59 &amp;&amp; min&lt;64 &amp;&amp; max&gt;73) print "Solexa64"; \ else print "Unknown score encoding"; \ print "( " min ", " max, ")";&#125;' 将上面这段代码复制到任意一份shell文件中（比如：fq_qual_type.sh），就可以用它来进行质量值类型的检查了。代码的思路其实比较简单，就是截取FASTQ文件的前1000行数据，并抽取出质量值所在的行，分别计算出其中最小和最大的ASCII值，再比较一下就判断出来了。下面给出一个例子，这是我们在本文中用到的FASTQ文件，它是Phred33的： 123$ sh fq_qual_type.sh untreated.fqPhred33( 34, 67 ) 另外，在查看碱基质量值的过程中，如果你心中存有ASCII码表当然可以直接“看”出各个碱基的质量值，但在实际的场景中都是通过程序直接进行转换处理。下面我就用Python的ord()函数举个转换的例子： 123456In [1]: qual='JJJJJIIJJJJJJHIHHHGHFFFFFFCEEEEEDBD'In [2]: [ord(q)-33 for q in qual]Out[2]:[35, 20, 17, 18, 24, 34, 35, 35, 35, 34, 35, 34, 29, 29, 32, 32, 34, 34, 33, 29, 33, 33, 32, 35, 35, 35, 34, 34, 34, 34, 35, 35, 34, 35, 34, 35, 34, 35, 34, 34, 34, 35, 35, 35, 35, 34, 33, 33, 30, 33, 24, 27] 这里的ord()函数会将字符转换为ASCII对应的数字，减掉33后就得到了该碱基最后的质量值（即，Phred quality score）。 另外，根据上面phred quality score的计算公式，我们可以很方便地获得每个测序碱基的错误率，这个错误率在我们的比对和变异检测中都十分重要，后续文章中我将会讲述该部分的具体内容，以下先给出一个转换的例子，还是以上述qual为例子： 12345678910In [1]: qual='JJJJJIIJJJJJJHIHHHGHFFFFFFCEEEEEDBD'In [2]: phred_score = [ord(q)-33 for q in qual]In [3]: [10**(-q/10.0) for q in phred_score]Out[3]:[3e-04, 1e-02, 2e-02, 2e-02, 4e-03, 4e-04, 3e-04, 3e-04, 3e-04, 4e-04, 3e-04, 4e-04, 1e-03, 1e-03, 6e-04, 6e-04, 4e-04, 4e-04, 5e-04, 1e-03, 5e-04, 5e-04, 6e-04, 3e-04, 3e-04, 3e-04, 4e-04, 4e-04, 4e-04, 4e-04, 3e-04, 3e-04, 4e-04, 3e-04, 4e-04, 3e-04, 4e-04, 3e-04, 4e-04, 4e-04, 4e-04, 3e-04, 3e-04, 3e-04, 3e-04, 4e-04, 5e-04, 5e-04, 1e-03, 5e-04, 4e-03, 2e-03] 这其实就是根据phred quality score的定义进行简单的指数运算。 小结到这里就说完了，虽然一开始只不过是想介绍两个普通的文件格式，但写着写着就变得很长，可见，越是看似简单的东西，其实越不容易说明白。关于FASTQ还有很多需要说的内容，我打算将其留到该系列的第四篇文章里，到时我会讲述该如何构造流程对其进行有效的数据质控等，这都是构造WGS分析流程之前非常重要的内容。 我一直觉得，生物信息学（或者说基因组学）中的许多数据文件，它们的格式都有着比较特殊的一面，为了能够真正有效地进行数据分析，多花些时间搞清楚它们的细节和来龙去脉是非常重要的。不然，你有可能在后续的数据分析过程掉入意想不到的陷阱，从而浪费大量宝贵的时间去寻找可能出错的地方。 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>WGS</tag>
        <tag>数据格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始完整学习全基因组测序（WGS）数据分析：第1节 测序技术]]></title>
    <url>%2F2017%2F08%2F04%2F2017-08-04-Begining-WGS-Data-Analysis-Sequecing-Tech.html</url>
    <content type="text"><![CDATA[前言 基因测序已是时下热门，目前除了华大基因之外，其他分布于全中国的大型测序平台（HiSeq X 10）还有约10个，每个每年大概能完成1.8万人的高深度全基因组测序，加起来就是18万人，如果加上华大，可能需要翻倍！而且随着新技术的快速发展和成本的下降，WGS正变得越来越普遍！再加上国家十三五规划已经提出了构建大规模中国人群遗传队列图谱的要求，全基因组测序技术正在逐渐替代其它测序手段，这也是我打算写这一个系列的原因。 HiSeq X 10分布（来源：转化医学网） 首先，全基因组测序的英文是Whole Genome Sequencing，简称WGS，目前默认指的是人类的全基因组测序。所谓全（Whole），指的就是 把物种细胞里面完整的基因组序列从第1个DNA开始一直到最后一个DNA，完完整整地检测出来，并排列好，因此这个技术几乎能够鉴定出基因组上任何类型的突变。对于人类来说，全基因组测序的价值是极大的，它的信息包含了所有基因和生命特征之间的内在关联性，当然也意味着更大的数据解读和更高的技术挑战。但，没关系，在这个系列中，我将从测序技术、常用文件解析，数据质控和流程构建等各个方面结合实际的例子，详细阐述什么是全基因组测序以及 该如何构造流程 分析全基因组测序（WGS）数据。 这是这一组学入门技术系列的第一篇（这篇文章修改自我以前的一篇博客，该文也已被各种形式转载），我首先将介绍当前的基因组测序原理及其发展历程。 第一节 NGS测序技术 在真正开始数据分析之前先知道我们是如何将那些原本存在于细胞中的DNA信息获取出来的——也就是测序的原理，总是有益的。 测序，简单来说就是将DNA化学信号转变为计算机可处理的数字信号。 它从1977年的第一代Sanger技术发展至今，已经足有40年时间。在这个技术发展的更迭历程中，测序读长从长到短，再从短到长。虽然就当前形势看第二代短读长测序技术在全球范围内上占有着绝对的垄断位置，但第三测序技术也已在这几年快速地发展着。测序技术的每一次变革和突破，都对基因组学研究，疾病医疗研究，药物研发，育种等领域产生巨大的推动作用。所以在这个系列的第一篇里我将对当前最主流的测序技术以及它们的测序原理做一个全面的介绍。 图1. 测序技术发展历程 第一代测序技术 第一代DNA测序技术用的是1975年由桑格（Sanger）和考尔森（Coulson）开创的链终止法或者是1976-1977年由马克西姆（Maxam）和吉尔伯特（Gilbert）发明的化学法（链降解）. 并在1977年，由桑格老人家测定了第一个基因组序列——噬菌体phiX-174，全长只有5,375个碱基。虽然与今日的技术比起来根本不算什么，但自此之后，人类获得了窥探生命本质的能力，并以此为开端真正步入了基因组学时代。 研究人员在Sanger法的多年实践之中不断对其进行改进。在2001年，完成的首个人类基因组图谱就是以改进了的Sanger法为基础进行测序的。Sanger法的核心原理是：由于ddNTP（4种带有荧光标记的A,C,G,T碱基）的2’和3’都不含羟基，其在DNA的合成过程中不能形成磷酸二酯键，因此可以用来中断DNA的合成反应，在4个DNA合成反应体系中分别加入一定比例带有放射性同位素标记的ddNTP（分别为：ddATP,ddCTP,ddGTP和ddTTP），然后利用凝胶电泳和放射自显影后可以根据电泳带的位置确定待测分子的DNA序列（图2）。这个网址为Sanger测序法制作了一个小短片，形象而生动。值得注意的是，在测序技术起步发展的这一时期中，除了Sanger法之外还出现了一些其他的测序技术，如焦磷酸测序法、连接酶法等。其中，焦磷酸测序法是后来Roche公司454技术所使用的测序方法，而连接酶测序法是后来ABI公司SOLID使用的测序方法，但他们的核心手段都是利用了Sanger中可中断DNA合成反应的dNTP。 图2. Sanger测序发原理 第二代测序技术 总的来说，第一代测序技术的主要特点是测序读长可达1,000bp，准确性高达99.999%，但其测序成本高，通量低等方面的缺点，严重影响了其真正大规模的应用。因而第一代测序技术并不是理想的测序方法。经过不断的技术开发和改进，以Roche公司的454技术、illumina公司的Solexa/HiSeq技术和ABI公司的SOLID技术为标记的第二代测序技术诞生了。第二代测序技术在大幅提高了测序速度的同时，还大大地降低了测序成本，并且保持了高准确性，以前完成一个人类基因组的测序需要3年时间，而使用二代测序技术则仅仅需要1周，但其序列读长方面比起第一代测序技术则要短很多，大多只有100bp-150bp。图3. 是第一代和第二代测序技术测序成本作了一个简单的比较，可以看出自第二代测序技术发展出来之后，历史开始发生根本性的改变，测序的成本开始快速实现断崖式下降，也就是业内经常提到的 超摩尔定律 现象。 图3. 测序成本比较（来源：NIH网站） 接下来我以illumina（目前最大、最成功的NGS测序仪公司）的技术为基础简要单介绍第二代测序测序技术的原理和特点。 目前illumina的测序仪占全球75%以上，以HiSeq系列为主。它的机器采用的都是边合成边测序的方法，主要分为以下4个步骤： 图4. illumina测序原理（来源：illumina官网） 1）构建DNA测序文库，图4-1 简单来说就是把一堆乱糟糟的DNA分子用超声波打断成一定长度范围的小片段。目前除了一些特殊的需求之外，基本都是打断为300bp-800bp长的序列片段，并在这些小片段的两端添加上不同的接头【注】，构建出单链DNA文库，以备测序之用； 【注】接头在illumina中一般分为P5和P7接头，其中一个带有和flowcell上的探针反向互补的序列，以完成待测序列和探针结合的作用，另外一个接头带有barcord序列以区分不同的样本。 2）测序流动槽（flowcell），图4-2 flowcell是用于吸附流动DNA片段的槽道，也是核心的测序反应容器——所有的测序过程就发生在这里。当文库建好后，这些文库中的DNA在通过flowcell的时候会随机附着在flowcell表面的槽道（称为lane）上。每个flowcell有8个lane（图5），每个lane的表面都附有很多接头，这些接头能和建库过程中加在DNA片段两端的接头相互配对，这就是为什么flowcell能吸附建库后的DNA的原因，并能支持DNA在其表面进行桥式PCR的扩增，理论上这些lane之间是不会相互影响的。 图5. flowcell（实物 VS 示意图） 3）桥式PCR扩增与变性 图6. 桥式PCR扩增（来源：illumina官网） 这是NGS技术的一个核心特点。桥式PCR以flowcell表面所固定的序列为模板，进行桥形扩增，如图6所示。经过不断的扩增和变性循环，最终每个DNA片段都将在各自的位置上集中成束，每一个束都含有单个DNA模板的很多分拷贝，这一过程的目的在于实现将单一碱基的信号强度进行放大，以达到测序所需的信号要求。 4）测序，如图4-4和图7所示 图7. 边合成边测序（来源：illumina官网） 测序方法采用边合成边测序的方法。向反应体系中同时添加DNA聚合酶、接头引物和带有碱基特异荧光标记的4中dNTP（如同Sanger测序法）。这些dNTP的3’-OH被化学方法所保护，因而每次只能添加一个dNTP，这就确保了在测序过程中，一次只会被添加一个碱基。同时在dNTP被添加到合成链上后，所有未使用的游离dNTP和DNA聚合酶会被洗脱掉。接着，再加入激发荧光所需的缓冲液，用激光激发荧光信号（图7），并有光学设备完成荧光信号的记录，最后利用计算机分析将光学信号转化为测序碱基。这样荧光信号记录完成后，再加入化学试剂淬灭荧光信号并去除dNTP 3’-OH保护基团，以便能进行下一轮的测序反应。 Illumina的这种每次只添加一个dNTP的技术特点能够很好的地解决同聚物长度的准确测量问题，它的主要测序错误来源是碱基的替换，目前它的测序错误率在1%-1.5%左右。测序周期以人类基因组重测序为例，30x-50x测序深度对于Hisq系列需要3-5天时间，而对于2017年初最新推出的NovaSeq系列则只需要40个小时！ 表1. 测序量比较（双流动槽为例，如为单流动槽则测序量减少为下表的一半，时间不变）一次测序的数据总产量的单位Gb，不是计算机字节，而是测序碱基的数目（Giga base） 图8. NovaSeq与其他测序仪测序通量的比较（来源：illumina官网） 上面表1和图8是NovaSeq和其他测序系列的比较，数据相当好。按照这个数据量估算，一台NovaSeq 6000（S4）在跑满的情况下，一年就可以测序6400多人！而且按照以往的经验，illumina的官方公布的数据都是偏于保守的，我们在实际的使用过程中发现 高质量（Q30）的read其实占到了总数据的90%以上，远高于官方公布的75%，数据的总产量也同样更高。 第三代测序技术 这是一个新的里程碑。以PacBio公司的SMRT和Oxford Nanopore Technologies的纳米孔单分子测序技术为标志，被称之为第三代测序技术。与前两代相比，最大的特点就是 单分子测序，测序过程无需进行PCR扩增，超长读长，以下图9是PacBio SMRT技术的测序读长分布情况，平均达到10Kb-15Kb，是二代测序技术的100倍以上，值得注意的是在测序过程中这些序列的读长也不再是相等的，下文有解析！ 图9. PacBio SMRT 测序read读长分布（来源：PacBio官网） PacBio SMRT PacBio SMRT技术其实也应用了边合成边测序的思想，并以SMRT芯片为测序载体（如同flowcell）。基本原理是： DNA聚合酶和模板结合，用4色荧光标记A,C,G,T这4种碱基（即是dNTP）。在碱基的配对阶段，不同的碱基加入，会发出不同的光，根据光的波长与峰值可判断进入的碱基类型。 图10. PacBio SMRT 测序原理 这个DNA聚合酶是实现超长读长的关键之一，读长主要跟酶的活性保持有关，它主要受激光对其造成的损伤所影响。PacBio SMRT技术的一个关键点是在于如何将反应信号与周围游离碱基的强大荧光背景区别出来。他们利用的是ZMW（零模波导孔）原理：如同微波炉壁上可看到的很多密集小孔。这些小孔的直径是有严格要求的，如果直径大于微波波长，能量就会在衍射效应的作用下穿透面板从而泄露出来（光波的衍射效应），从而与周围小孔相互干扰（光波的干涉）。如果孔径能够小于波长，那么能量就不会辐射到周围，而是保持直线状态，从而可起到保护的作用。同理，在一个反应管(SMRTCell:单分子实时反应孔)中有许多这样的圆形纳米小孔,，即 ZMW(零模波导孔)，外径100多纳米，比检测激光波长小(数百纳米)，激光从底部打上去后不会穿透小孔进入上方的溶液区，能量会被限制在一个小范围(体积20X 10-21 L)里（图10-A），正好足够覆盖需要检测的部分，使得信号仅仅只是来自于这个小反应区域，孔外过多的游离核苷酸单体依然留在黑暗中，从而实现将背景噪音降到最低的目的。 PacBio SMRT技术除了能够检测普通的碱基之外，还可以通过检测相邻两个碱基之间的测序时间，来检测碱基的表观修饰情况，如甲基化。因为假设某个碱基存在表观修饰，则通过聚合酶时的速度会减慢，那么相邻两峰之间的距离会增大，我们可以通过这个时间上的差异来检测表观甲基化修饰等信息（图11）。 图11. PacBio SMRT 检测甲基化修饰（来源：PacBio官网） SMRT技术的测序速度很快，每秒约10个dNTP。但这么快的测序速度也带来了一些明显的缺点——测序错误率比较高（这几乎是目前单分子测序技术的通病），可以达到10%-15%，而且以缺失序列和错位居多，但好在它的出错是随机的，并不会像第二代测序技术那样存在一定的碱基偏向，因此可以通过多次测序来进行有效纠错。 Oxford Nanopore Oxford Nanopore 的MinION是另一个比较受关注的第三代测序仪，俗称U盘测序仪，它真的很小，我亲手拿过，并拆过，图12（左）！这家公司开发的纳米单分子测序技术与以往的测序技术相比都不一样，它是基于电信号而不是光信号的测序技术！ 图12. Oxford Nanopore MinION 这个技术的关键点在于他们所设计的一种特殊纳米孔，孔内共价结合分子接头。当DNA分子通过纳米孔时，它们使电荷发生变化，从而短暂地影响流过纳米孔的电流强度（每种碱基所影响的电流变化幅度是不同的），最后高灵敏度的电子设备检测到这些变化从而鉴定所通过的碱基（图13）。 图13. MinION 测序原理 纳米孔测序以及其他第三代测序技术，有可能会彻底地解决目前第二代测序平台的诸多不足。另外，MinION的主要特点是：读长很长，而且比PacBio的都长得多，基本都是在几十kb上百kb以上，最新的数据显示可以达到900 kb！错误率是5%-15%，也是随机错误，MinION最大的特点除了极小的体积之外，就是数据将是可实时读取的，并且起始DNA在测序过程中不被破坏！这真是个可以上天的能力。然鹅，遗憾地多说几句，目前还没真正公布，细节也不知，自从2012开过一次发布会之后，就没什么声响了。 这种纳米孔单分子测序仪还有另一大特点，它能够 直接 读取出甲基化的胞嘧啶，而不必像二代测序方法那样需要事先对基因组进行bisulfite处理。这对于在基因组水平直接研究表观遗传相关现象有极大的帮助。下面是对PacBio和Oxford Nanopore这两家第三代测序技术公司的测序仪做的一个简单比较，可以看出其实成本还是蛮高的，质量也只是还行，期待他们的下一次进化吧。 总结 以上，便是对各代测序技术的原理做了简要的阐述。在这个比较的过程中，可以看到测序成本，读长和通量是该测序技术先进与否的三个重要指标。其实第一代和第二代测序技术除了通量和成本上的差异之外，测序的核心原理都来自于边合成边测序的思想。第二代测序技术的优点是通量大大提升，成本大大减低，使得昔日王榭堂前燕，可以飞入寻常百姓家。总之，只有变成白菜价，才能真正对大众有意义；但它的缺点是所引入PCR过程会在一定程度上增加测序的错误率，并且具有系统偏向性，同时读长也比较短。第三代测序技术是为了解决第二代所存在的缺点而开发的，它的根本特点是单分子测序，不需要任何PCR的过程，这是为了能有效避免因PCR偏向性而导致的系统错误，同时提高读长，但这个技术还不是很成熟，需要再进化，成本也偏高。 图14. 全球测序仪数量分布 参考文献 Sanger, F. &amp; Nicklen, S. DNA sequencing with chain-terminating. 74, 5463–5467 (1977). Mardis, E. R. Next-generation DNA sequencing methods. Annual review of genomics and human genetics 9, 387–402 (2008). Shendure, J. &amp; Ji, H. Next-generation DNA sequencing. Nature biotechnology 26, 1135–45 (2008). Metzker, M. L. Sequencing technologies - the next generation. Nature reviews. Genetics 11, 31–46 (2010). Niedringhaus, T. P., Milanova, D., Kerby, M. B., Snyder, M. P. &amp; Barron, A. E. Landscape of Next-Generation Sequencing Technologies. 4327–4341 (2011). Rothberg, J. M. et al. An integrated semiconductor device enabling non-optical genome sequencing. Nature 475, 348–52 (2011). 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>测序技术</tag>
        <tag>WGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于深度学习的序列模型预测非编码区变异的功能效应]]></title>
    <url>%2F2017%2F05%2F30%2FPredicting-effects-of-noncoding-variants-with-deep-learning-based-sequence-model.html</url>
    <content type="text"><![CDATA[Deep Learning，现在几乎到处都能看到它的应用。看！紧随DeepBind，在基因组学应用中又来了一个DeepSEA——这是一个适用于表观遗传研究和应用的工具，它只从DNA序列出发，并没用其他有关于表观研究的实验或者测序技术，通过直接输入fasta sequence，vcf或者bed文件，就可以预测转录因子结合位点(Transcription factors binding site), DNase I超敏感位点（DNase I hypersensitive sites）和组蛋白靶点（histone marks），这么多年来，这样的做法还是头一回。下面这张示意图展示的是各个主要的表观修饰在染色体中的位置和相关实验测定技术。 为什么要有这么个东西呢？ 众所周知，人类基因组上绝大部分的序列都是非编码序列——不直接编码蛋白质的序列，这些序列在很长的一段时间里都被误解为所谓的“垃圾DNA”！但其实它们各自都有着独特的作用——调控着机体的正常运作，只是要想正确地理解它们确实不是一个容易的事情。DeepSEA想要干的就是尝试从序列的基础功能预测着手去解决这么一个难题。 它先通过学习大量已知的染色质修饰数据——主要来自于ENCODE和Roadmap Epigenomics等大型项目，经过不断的训练，学习到了许多种在非编码区域中序列调控的序列模式或者说是序列特征（注意是序列模式，不是功能模式），之后，便可以通过这些模式和特征去预测序列上单碱基的突变会如何影响染色质的修饰功能。从发表的文章来看，其精确程度是目前所有方案中最高也是在同等数据下最有效的了。 DeepSEA 在Nature Method的原文http://www.nature.com/nmeth/journal/v12/n10/full/nmeth.3547.html更赞的是它的代码和相关训练数据都一起公开在网站上：http://deepsea.princeton.edu/ 可以尝试玩起来了。 【注】本文首发于泛基因和公众号。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>非编码区</tag>
        <tag>深度学习</tag>
        <tag>功能预测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GATK中如何计算Inbreeding coefficient（近交系数）]]></title>
    <url>%2F2017%2F05%2F28%2Fhow-to-calculate-inbreading-coefficient-in-GATK.html</url>
    <content type="text"><![CDATA[关于近交系数是什么的定义，除了英文资料，中文上也给出了清晰的定义，这里引用一下： 近交系数（inbreeding coefficient）是指根据近亲交配的世代数，将基因的纯化程度用百分数来表示即为近交系数，也指个体由于近交而造成异质基因减少时，同质基因或纯合子所占的百分比也叫近交系数，普遍以F或f来表示。 GATK近交系数的计算程序在github上可以找到：AS_InbreedingCoeff.java 代码不短，但计算很简单，我主要说展示一下这个计算的核心部分并在代码中做些注释，如下： 1234567891011121314151617181920212223242526protected double calculateIC(final VariantContext vc, final Allele altAllele) &#123; final int AN = vc.getCalledChrCount(); final double altAF; final double hetCount = heterozygosityUtils.getHetCount(vc, altAllele); final double F; //shortcut to get a value closer to the non-alleleSpecific value for bialleleics if (vc.isBiallelic()) &#123; double refAC = heterozygosityUtils.getAlleleCount(vc, vc.getReference()); double altAC = heterozygosityUtils.getAlleleCount(vc, altAllele); double refAF = refAC/(altAC+refAC); altAF = 1 - refAF; F = 1.0 - (hetCount / (2.0 * refAF * altAF * (double) heterozygosityUtils.getSampleCount())); // inbreeding coefficient &#125; else &#123; //compare number of hets for this allele (and any other second allele) with the expectation based on AFs //derive the altAF from the likelihoods to account for any accumulation of fractional counts from non-primary likelihoods, //e.g. for a GQ10 variant, the probability of the call will be ~0.9 and the second best call will be ~0.1 so adding up those 0.1s for het counts can dramatically change the AF compared with integer counts altAF = heterozygosityUtils.getAlleleCount(vc, altAllele)/ (double) AN; // 计算inbreeding coefficient F = 1.0 - (hetCount / (2.0 * (1 - altAF) * altAF * (double) heterozygosityUtils.getSampleCount())); // heterozygosityUtils.getSampleCount() 获取总样本数 &#125; return F;&#125; 总的来说，是利用哈迪温伯格定律来计算的。 1.0 - (hetCount / (2.0 (1 - altAF) altAF(double)N ，N是人数。这个值给出的是期望的杂合变异的个数。所以参数F说的就是”实际的hetCount”除以”期望的hetCount”再与1.0取差。当F值越接近0，就意味着实际的hetCount与理论的hetCount越接近。]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>GATK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[癌症基因组学研究全程回顾]]></title>
    <url>%2F2017%2F01%2F02%2F2017-01-02-cancer-genomics-review.html</url>
    <content type="text"><![CDATA[癌症，一种慢性的基因遗传病。 简介在癌症研究中，每个癌症样品呈现在研究人员眼前的已经是一个发生了改变的基因组，其中包含着独特且难以预测的诸多点突变、序列的插入缺失、易位、融合以及其他畸变。并且，这些发生的变异中，许多往往都是之前所未观察到的（Novel mutations），它们也不会只存在于基因组的编码区域中，因而为了能够真正做到全面研究癌症基因组本身所发生的所有突变事件，全基因组测序已被视为肿瘤基因变异研究中 唯一 严谨的方法。 然而，在所有这些变异中，却只有少数的几个主导着癌症这一疾病的发展和演变。要有效揭示这一演变和发展的过程，就需要监控基因表达水平上的变化，那么RNA-Seq便是用以确定这些遗传改变是否会影响疾病发展有用技术。但遗传改变有可能影响所有的细胞过程，包括染色质结构、DNA甲基化、RNA剪接异构体、RNA编辑和microRNA（miRNA）等。这就意味着，只有对所有这些独立的过程都进行检测和综合分析，才能在癌症研究中取得真正的突破。这些内容我们都将在下文一一展开。 当前基因组测序技术的一大特点是在于能够在很短的时间内并行测得数十亿（甚至数百亿）的独立序列片段——read，而每个read来源于单个的DNA分子。由此产生的数据我们可将其视为是对DNA分子的随机抽样，这反过来代表肿瘤样品中每个细胞的基因组的情况。这是我们解开癌症的原因和机制的一种强大工具。 肿瘤异质性癌症基因组的突变是复杂的。 每个人都携带一套独特的来自父母遗传的胚系突变（germline mutations）信息。但随着癌症的发展，体细胞突变（Somatic mutations）和基因组重排（genomics rearrangements）会逐渐增加。这些改变往往会引发耐药性以及转移。越来越多的研究证据表明，这些过程竟然可以是 有意的！——它们其实可以认为是癌细胞面临药物刺激的过程中不断进化的结果。我们想要全面地理解这一复发和耐药性的原因，就有必要进行纵向实验，按照癌症的发展过程，分阶段采集样品来进行研究。 如上图，这是处于正常组织背景下的多克隆肿瘤（polyclone tumor）。大多数的肿瘤样品都会同时包含肿瘤细胞和正常细胞，如基质细胞、血管和免疫细胞。并且肿瘤本身也常常包含几种不同的克隆亚型（clone types），每一种都有着不同的治疗反应和复发可能。 根据传统病理学的估计，大部分研究的结果其实都只集中在那些肿瘤细胞比例 &gt;60% 的区域中。并且为了确定哪些突变是肿瘤特异的，通常都要包含来自同一个体的正常组织样本作为参考。 然而，肿瘤本身也往往是异质性的。在癌症发展的过程中，个别细胞会发生新的突变，包含这些新突变的细胞会继续增殖，形成克隆亚型。因此，对于晚期癌症我们通常检测到的都是一个多克隆肿瘤，其中每一个克隆有一套独特的突变信息、独特的病理学和药物反应机制。这其实也正是癌症难以完全治疗的原因，而它的这种 异质性其实正是肿瘤基因组复杂性导致的一种表型性质。目前的深度测序可以检测样本中含量低至1%的克隆。 肿瘤内的异质性。体细胞突变的逐步累积产生了一个异质的多克隆肿瘤，其中不同克隆可能对治疗的反应不同。 而且，异质性一般还可以分两种情况讨论，第一种情况指的是：同一个病人的肿瘤细胞具有异质性。处于肿瘤发生的不同时期的肿瘤细胞的基因突变情况不同，造就了每一个肿瘤细胞群体内还有许多亚群（subclones），肿瘤细胞在通过转移时，就会有属于不同亚群的肿瘤细胞去侵入新的地方，形成新的肿瘤；第二种情况指的是：除了同一病人的不同肿瘤细胞会造成肿瘤的异质性外，肿瘤的异质性还体现在不同病人可能得了相同的肿瘤，但是那个『相同』未必真是相同——仅仅是表型相同，不代表着基因型也相同。 在某些基因中，突变频繁发生在同一个位置，这应该有特定的机制在起作用，这些有规律性的情况都会稍微容易对付。然而遗憾的是，对于大多数的基因，突变显然是随机出现在整个基因中的，这其实说明了DNA的复制和修复机制失灵了。 图中为两个假定的基因，他们有着两种不同的突变模式。深灰色框代表外显子组，而红色柱子代表存在突变的位置。A: 特定位置的频发突变可能表示有产生突变的生物学机制的参与。B: 散发突变存在于整个基因中，如P53，可能是由于复制和修复机制的失效。我们可以通过测序检测这两种情况下的突变。 参考文献 2012年Ding Li等人发表在Natrue上的一项研究弄清了急性髓细胞白血病（AML）的复发原因。他们发现了两个一般机理：（1）原发肿瘤中的始发克隆（founding clone）获得突变，进化成复发克隆；（2）始发克隆的一个亚克隆挺过了初次治疗，获得了更多突变并在复发时增殖。在一个病例中，原发肿瘤里原本仅占5.1%的亚克隆在复发后却成长为主要的克隆。而且对于所有的病例，化疗并不能够根除这些始发克隆。这项研究直接表明了在诊断后和初次治疗后检测并根除掉那些原本不起眼的小细胞群体有多么的重要！Ding L., Ley T. J., Larson D. E., Miller C. A., Koboldt D. C., et al. (2012) Clonal evolution in relapsed acute myeloid leukaemia revealed by whole-genome sequencing. Nature 481: 506-510 同样是2012年，同样是Ding Li这波人，同样是研究AML，他们这次发现大约三分之一的骨髓增生异常综合征患者会发展成继发性急性髓细胞白血病(AML)。这个研究的目的是为了确定骨髓增生异常综合征中的突变，它们有可能预测AML的进展。他们对七名继发性AML患者的七组皮肤和骨髓样品以及先前的骨髓增生异常综合征的配对骨髓样品一并做了全基因组测序。结果发现，在所有病例中，占主导地位的继发性AML克隆都是来自一个骨髓增生异常综合征的始发克隆。这其实就是说，骨髓增生异常综合征样品包含了对预后很重要的突变，针对这些突变的治疗方案是可能改善预后的。Walter M. J., Shen D., Ding L., Shao J., Koboldt D. C., et al. (2012) Clonal architecture of secondary acute myeloid leukemia. N Engl J Med 366: 1090-1098 继续肿瘤治疗和预后的话题，Gerlinger M这一波人发起了一项研究，利用全外显子组测序来研究多个样品，这些样品不仅来自两名患者体内的原发肾癌，还包括了患者相关转移部位的空间分隔区域。最后，他们在原发肿瘤中看到了广泛存在的异质性现象！而且还特别指出了在每个肿瘤区域内有63%-69%的体细胞突变是无法检测到的。同时，还检测了同一肿瘤不同区域内预后良好和不良的基因表达特征。这其实还是突出了在突变积累之前早期诊断的重要性，以及对较大肿瘤需要进行多个部位的活检。利用同一名患者的多个样本得到的信息，他们能够重建疾病的发展进程！这是一种极为强大的方法，不仅能检测引发事件，还能检测表现出平行进化的基因。平行进化通常是基因在进化压力下的一种表现，其实也表示那些基因可能成为有效的治疗靶点。Gerlinger M., Rowan A. J., Horswell S., Larkin J., Endesfelder D., et al. (2012) Intratumor heterogeneity and branched evolution revealed by multiregion sequencing. N Engl J Med 366: 883- 892。 转移肿瘤的转移是一个复杂的过程，其中癌细胞脱离原发肿瘤，通过血液或者淋巴系统循环到身体的其他部位。在新部位，细胞继续繁殖，最终形成更多肿瘤，这些肿瘤包含了反映其组织来源的细胞。肿瘤（胰腺癌和葡萄膜肿瘤）转移的能力大大增加了它们的致死性。关于转移肿瘤的克隆结构、转移酶之间的系统发育关系、转移和原发部位的平行进化规模，肿瘤如何扩散，以及肿瘤微环境在转移部位决定中的作用如何等，许多这些基本问题目前仍然没有很好地解决。 转移瘤可能来源于原发肿瘤中一个主要克隆（如上图：Metastasis1），也可能来源于次要克隆（Metastasis2）。转移瘤也会经历克隆进化（如Metastasis1所示） 参考文献 Hsieh A. C., Liu Y., Edlind M. P., Ingolia N. T., Janes M. R., et al. (2012) The translational landscape of mTOR signalling steers cancer initiation and metastasis. Nature 485: 55-61 这篇文章证明了前列腺癌基因组经由致癌mTOR通路的特殊翻译机制，这其中产生了一个特定的基因群，它们参与了细胞增殖、代谢和侵袭。随后作者对一类翻译控制前侵袭信使RNA进行了功能鉴定，并指出了这些mRNA主导了癌症侵袭和转移。 基因组突变所有的肿瘤在其发展的过程中都会不断积累体细胞突变（somatic mutations）。大多数常见的肿瘤与不同的癌基因相关联，这些癌基因以低频率发生突变。从大型癌症数据库中观察到的一个最令人惊讶的现象是癌症间甚至各个癌症类型内的显著遗传异质性。然而，似乎只有有限的细胞通路对肿瘤的细胞生物学很重要。目前很多人正在编辑收录各种癌症类型的体细胞突变综合列表，这对于更好地了解这种疾病背后的机制将有很大的指引作用。 研究参考 Nik-Zainal S., Alexandrov L. B., Wedge D. C., Van Loo P., Greenman C. D., et al. Mutational processes molding the genomes of 21 breast cancers. Cell 149: 979-993这篇文章中研究了21个乳腺癌基因组，并给出了它的一个体细胞突变列表。发现带BRCA1或者BRCA2突变的癌症会有一种特别的替换突变特征和与众不同的缺失图谱。文章中还描述了一种局部的超突变现象，这称为『kataegis』(kataegis，希腊语中『雷雨』的意思，文中指的是在一个小区域中出现大量突变的机制，如下图)。并且这些区域中的碱基替换几乎都发生在TpC二核苷酸的胞嘧啶上！ 这是Kataegis图像。纵轴是突变间距（对数刻度）。这个图中基因组内大部分的突变都有着$$~10^6bp$$至$$~10^6bp$$的突变距离。其中超突变区即是表现为突变间距较低的簇。 Govindan R., Ding L., Griffith M., Subramanian J., Dees N. D., et al. Genomic landscape of non-small cell lung cancer in smokers and never-smokers. Cell 150: 1121-1134这是另一篇文章，主要是对17个非小细胞肺癌（NSCLC）患者的肺癌及癌旁正常组织样本进行了全基因组和转录组测序。值得注意的是吸烟者中所观察到的突变频率比不吸烟者高10倍！这是通过深度测序揭示出的这些群体间所不同的克隆模式。而且其中所有经过验证的EGFR和KRAS突变都存在于原始克隆中，这其实也就表明了它们在癌症启动中可能发挥非常重要的作用。 镶嵌性对于这个现象我们也同样通过实际的研究来说明。AML（急性髓细胞白血病）基因组中发现的大多数突变实际上是随机事件，在造血干细胞/祖细胞（HSPC）获得原始突变之前就存在了；但是随着克隆的扩增，细胞的突变历史被『捕获』了。如何理解这里的『捕获』？其实说的就是，原本那些突变都没什么鸟用，就摆在那无所事事，但是，在许多情况下，偏偏只需要再来一个或者两个额外的突变来协助就能共同作用，最后产生恶性的原始肿瘤克隆！ 原发肿瘤和转移的镶嵌性。非同义点突变和插入缺失（绿色块）的区域分布的假设热图。行代表来自七个原发肿瘤区域和六个转移区域的样品。 研究参考 Abyzov A., Mariani J., Palejev D., Zhang Y., Haney M. S., et al. (2012) Somatic copy number mosaicism in human skin revealed by induced pluripotent stem cells. Nature 492: 438-442这里作者发现了一个现象：平均而言，iPSC细胞系表现出2个拷贝数变异（CNV），而这些CNV在iPSC来源的成纤维细胞中不明显。他们发现，至少50%的CNV以低频体细胞变异存在于亲本的成纤维细胞中。根据这一观察，他们估计大约30%的成纤维细胞的基因组中携带体细胞CNV，这表明体细胞镶嵌性广泛存在于人体中。 基因融合基因融合是非常普遍的，也是癌症的一个重要特征。现在的研究发现，一个强启动子与一个下游功能基因（比如：原癌基因）的融合在某些癌症中很普遍。据估计，半数的前列腺癌含有TMPRSS2和ETS转录因子家族成员之间的融合。基因融合是由两个原本分开的基因或位点融合形成的。他们可能形成一种基因产物，很多时候表现出来的功能都是全新的，与两个融合的基因个体都不同。这种阴差阳错的情况可能引起致癌机制的激活，就像费城染色体阳性急性淋巴细胞白血病一样。这种基因融合导致BCR-ABL酪氨酸激酶表达，从而激活细胞增殖。有几种机制会导致基因融合的发生，这个现象是一些癌症类型的特点。胰腺癌的特点便是染色体重排的频繁断裂-融合-桥循环。目前有几种方法可以通过测序研究融合事件，如对肿瘤的全基因组测序和mRNA-Seq。 mRNA-Seq与全基因组测序组合的方法对于发现基因融合及其机制特别高效。原因就是mRNA-Seq可以提供直接的证据，来支持观察到的融合是否发生，并同时为融合基因是否表达提供了证据。而全基因组测序可以发现那些mRNA-Seq所发现不了的区域的信息，如基因间区和UTR。 由折回倒位所引起的融合事件可捕获基因组中遥远区域的片段，如着丝粒重复或参与体细胞重排的区域。在这个例子中，6号染色体上的片段被插入到19号染色体上的重复区域之间。注意19号染色体的第二个拷贝是倒置的，这是折回倒位的特点。 MED1（红色）与几个伙伴基因（蓝色）：ACSF2，USP32 和 STXBP4形成基因融合。 实验上的设计参考 以Pair-end进行全基因组测序是目前检测基因融合最准确、最全面的工具，这些融合包括重复、倒位、通读和单碱基插入缺失。可以说Pair-end是检测融合基因成功与否的一个关键因素。另外就是高深度测序结合更长的读长可以分辨融合连接中微同源的单碱基。而且这种能力是测序独有的。 研究参考 Robinson D. R., Wu Y. M., Kalyana-Sundaram S., Cao X., Lonigro R. J., et al. Identification of recurrent NAB2-STAT6 gene fusions in solitary fibrous tumor by integrative sequencing. Nat Genet 45: 180-185文章主要是利用了全外显子组和转录组测序发现了转录抑制因子NAB2与转录激活因子STAT6的基因融合现象。其中27个独立性纤维性肿瘤（SFT）的转录组测序发现所有肿瘤中存在NAB2-STAT6基因融合。NAB2-STAT6基因融合的过表达诱导了培养细胞的增殖，并激活了EGR应答基因的表达，最后导致了肿瘤。 Seshagiri S., Stawiski E. W., Durinck S., Modrusan Z., Storm E. E., et al. Recurrent R- spondin fusions in colon cancer. Nature 488: 660-664这一篇文章则主要分析了70个原发性人结肠癌的外显子组、转录组和拷贝数变异。拷贝数和RNA-Seq的数据分析确定了在一部分结直肠癌中存在IGF2的扩增和相应过表达。他们还利用RNA-Seq，在10%的结直肠癌中发现了与R-脊椎蛋白家族成员（RSPO2和RSPO3）相关的基因融合。这项研究表明了综合多项技术去了解复杂的癌症基因组很重要。 Thompson-Wicking K., Francis R. W., Stirnweiss A., Ferrari E., Welch M. D., et al. (2012) Novel BRD4-NUT fusion isoforms increase the pathogenic complexity in NUT midline carcinoma. Oncogene这篇文章则提到了PER-624中一种新的BRD4-NUT融合竟然编码了一种功能蛋白，并且它对这些细胞的致癌机制很关键。BRD4-NUT融合转录本是通过易位后的RNA剪接而产生的，这似乎是这些癌症的一个共同特征。这种有助于融合基因的替代异构体表达的机制是第一次报道。 Wen H., Li Y., Malek S. N., Kim Y. C., Xu J., et al. (2012) New fusion transcripts identified in normal karyotype acute myeloid leukemia. PLoS ONE 7: e51203在这项研究中，作者运用双端RNA-Seq来发现染色体核型中的融合，它们经传统的细胞遗传学分析未检测到异常。他们发现了临近基因间的融合转录本以及7个只存在于正常核型中的融合本。 染色体碎裂这是一个不希望发生的现象，染色体碎裂是一个一次性的细胞危机，在单次事件中发生数十次至数百次基因组重排。这种灾难性事件的后果是复杂的局部重排和拷贝数变异，其中染色体上2个（偶尔3个）拷贝的有限范围可被检测。这种单次灾难性事件的模式不同于癌症发展的逐步积累突变的典型模式。在突变积累的癌症发展模式中，拷贝数无上限，因此通常有一个较大的范围。据估计，在所有癌症及其不同亚型之间，染色体碎裂的发生概率约2-3%，而在骨癌中发生概率则大约25%。 染色体碎裂的图示。 研究参考 Rausch T., Jones D. T., Zapatka M., Stutz A. M., Zichner T., et al. (2012) Genome Sequencing of Pediatric Medulloblastoma Links Catastrophic DNA Rearrangements with TP53 Mutations. Cell 148: 59-71 文章提到一名Sonic-Hedgehong髓母细胞瘤（SHH-MB）患者的大量、复杂的染色体重排，此患者带有生殖细胞系TP53突变（Li-Fraumeni综合征）。同时将规模扩大到11名Li-Fraumeni综合征患者的筛查，发现有36%的肿瘤表现出与染色体碎裂一致的重排。这比一般肿瘤群体所观察到的2%染色体碎裂发生率要高得多。P53的生殖细胞系突变与一些肿瘤中凋亡中止导致染色体碎裂的假说是一致的。 拷贝数变异（CNV）结构性变异影响基因量——可转录基因的功能拷贝数。肿瘤发展、药物反应及耐药性的发生通常是由基本的基因扩增和删除来驱动的。这些基因组上的改变可分成大的畸变和小的畸变。大的畸变包括整个染色体或部分染色体的丢失或重复，这被称为非整倍体。小的畸变可能只包含一个碱基，比如点突变和小片段的插入缺失。与健康的基因组不同，这些基因表达的改变会受到转录因子的严格调控，癌症基因组则通过基因的重复和删除来适应和逃避这种调控。癌症耐药性的发生正是此反应的速度和效率的绝佳证明。 基因表达基因表达分析测定基因转录、RNA加工和表观遗传控制的产物。因此，基因表达分析不仅可以看出这些过程的『健康』程度，也可以深入研究细胞里面的分子机制。基于芯片的mRNA分析曾在癌症的基因变得研究中广泛使用，但基于测序的mRNA分析（mRNA-Seq）的出现代表我们测定和解析基因表达产物能力的又一次飞跃。mRNA-Seq可检测修饰过的RNA和表达水平极低的RNA的能力让它特别适合癌症研究。基于mRNA-Seq的方法也可检测非常快的转录变化、剪接异构体、融合基因以及可变聚腺苷酸化位点。 Feng H., Qin Z. and Zhang X. (2012) Opportunities and methods for studying alternative splicing in cancer with RNA-Seq. Cancer Lett 这篇综述关注了RNA-Seq在研究癌症相关的可变剪切中的应用。文中包含一个生物信息学工具列表，以及有关估计可变剪切异构体的表达水平的详尽讨论。 利用RNA-Seq研究癌症中基因表达和选择性剪接的典型生物信息学流程。首先，将短read定位到参考基因组或转录组。在定位之后，估算注释基因和转录本的表达与剪接。 van Delft J., Gaj S., Lienhard M., Albrecht M. W., Kirpiy A., et al. (2012) RNA-Seq provides new insights in the transcriptome responses induced by the carcinogen benzo[a]pyrene. Toxicol Sci 130: 427-439 作者发现，RNA-Seq所检测到的基因比芯片技术多约20%，而表达差异明显的基因更是接近三倍之多。因此，他们检测到的受影响的通路和生物学机制达2-5倍。作者还在许多基因中发现了可变异构体的表达，包括细胞死亡和DNA修复的调控因子，如TP53、BCL2和XPA，它们与基因毒性反应相关。他们还发现了功能未知的新亚型，如已知转录本的片段、带有额外外显子的转录本、内含子保留或外显子跳跃事件。 Kaur H., Mao S., Li Q., Sameni M., Krawetz S. A., et al. (2012) RNA-Seq of human breast ductal carcinoma in situ models reveals aldehyde dehydrogenase isoform 5A1 as a novel potential target. PLoS ONE 7: e50249作者将三个DCIS模型（MCF10.DCIS、SUM102和SUM225）的表达与三维（3D）覆盖培养的非致癌乳腺上皮细胞的MCF10A模型进行了比较，确定了DCIS模型共用的表达变化。他们发现，差异表达的基因编码了与多个信号通路相关的蛋白。 Meyer J. A., Wang J., Hogan L. E., Yang J. J., Dandekar S., et al. (2013) Relapse-specific mutations in NT5C2 in childhood acute lymphoblastic leukemia. Nat Genet 45: 290-294 作者利用RNA测序，报道了诊断和复发相配对的骨髓标本的转录本图谱，这些标本来自十名患有小儿B淋巴细胞白血病的个体。转录组测序鉴定出20个新获得的突变，它们不存在于最初的诊断中，而2名个体带有复发特异的突变。带有NT52C2突变的所有个体都在初步诊断后36个月内复发。 实验设计上的注意事项 RNA-Seq已成为一种研究肿瘤分子变化的常规应用，大部分研究人员采用生产商的试验流程。rRNA的去除可提高信噪比，实现低表达转录本的检测。 癌症中的体细胞突变基本上是 de novo。测序不需要关于突变的先验知识，即可准确定位突变以及得到转录本丰度。 肿瘤通常包含各种细胞。mRNA-Seq的可延伸检测范围和准确性对检测微小的表达变化非常宝贵。只要肿瘤转录本包含了独特的体细胞突变或剪接变异体，那么就可将它与正常的细胞区分开来。 二代双端对读测序检测基因融合的灵敏度取决于许多因素，包括表达水平、转录本长度、所使用的样品制备方法以及cDNA文库的片段长度。 大部分实验方案采用poly（A）富集的RNA制备方法来测定mRNA水平。然而，非编码RNA，如miRNA，也在细胞的生物学中发挥重要作用，并常常介导对肿瘤生长和存活很关键的过程。非编码RNA可通过现有的poly(A)-(rRNA去除)实验方案轻松分析。 RNA表达是组织和细胞类型特异的。在选择肿瘤-正常对照中的对照时，应考虑这一点。 选择性剪接癌症的生物起源、发展、转移与转录组中的许多变异相关联。癌症特异的选择性剪接是个普遍存在的现象，也是个主要的转录后调控机制，涉及到许多癌症类型。 Seo J. S., Ju Y. S., Lee W. C., Shin J. Y., Lee J. K., et al. (2012) The transcriptional landscape and mutational profile of lung adenocarcinoma. Genome Res 22: 2109-2119作者分析了韩国200个肺腺癌。他们在LMTK2、ARID1A、NOTCH2和SMARCA4中发现了新的驱动突变。他们还发现了45个融合基因，其中8个是嵌合的络氨酸激酶。在17个反复发生的选择性剪接事件中，原癌基因MET中的第14号外显子跳过可能是癌症驱动因素。这项研究表明了这种癌症的复杂性以及运用几种技术的价值。 Liu J., Lee W., Jiang Z., Chen Z., Jhunjhunwala S., et al. (2012) Genome and transcriptome sequencing of lung cancers reveal diverse mutational and splicing events. Genome Res 22: 2315- 2327作者对19个肺癌细胞系和3组肺部肿瘤/正常样本配对开展了全基因组测序和转录组测序。他们鉴定出106个与癌症特异性的异常剪接相关的剪接位点突变，包括一些已知的癌症相关基因中的突变。RAC1b是RAC1 GTP酶的一个异构体，含有一个额外的外显子，被认为在肺癌中优先上调，并对MAP2K（MEK）抑制剂PD-0325901敏感。 Thompson-Wicking K., Francis R. W., Stirnweiss A., Ferrari E., Welch M. D., et al. (2012) Novel BRD4-NUT fusion isoforms increase the pathogenic complexity in NUT midline carcinoma. Oncogene这篇文章发现了PER-624中一种新的BRD4-NUT基因融合编码了一种功能蛋白，它对这些细胞的致癌机制很关键。BRD4-NUT融合转录本是通过易位后的RNA剪接而产生的，这似乎是这些癌症的一个共同特征。这种现象以及促进融合基因的可变异构体表达的机制，过去一直未被发现。 RNA编辑在我们人体中，DNA和RNA序列之间的差异也被称之为 RNA编辑，这是一种广泛存在的现象。最频繁的RNA编辑类型是通过腺苷脱氨酶作用于RNA(ADAR)从而实现由腺苷到肌苷的转换。然后紧接着，剪接和翻译机制会将肌识别为鸟苷。在一些肿瘤基因组比正常基因组有着更更比例的RNA-DNA差异。 Jiang Q., Crews L. A., Barrett C. L., Chun H. J., Court A. C., et al. (2013) ADAR1 promotes malignant progenitor reprogramming in chronic myeloid leukemia. Proc Natl Acad Sci U S A 110: 1041-1046作者发现，慢性髓细胞白血病(CML)急变期的祖细胞有着更高的IFN-r通路基因表达以及BCR-ABL扩增。在CML发展期间，他们还发现IFN应答的ADAR1 p150亚型的表达增强，并且腺苷-肌苷的RNA编辑增加。 MicroRNA和非编码RNAMicroRNA(miRNA)的长度很短，大小集中在17bp-25bp之间，属于非编码RNA（ncRNA）家族的成员。它们调控多种不同的生物学功能，包括发育、细胞增殖、细胞分化、信号转导、凋亡、代谢和细胞寿命。 通过RNA-诱导沉默复合物（RISC）与转录本的3-UTR或者与编码区中的识别位点相互作用。miRNA的一个主要作用是抑制基因的转录后表达。在多种癌症中，许多miRNA位于存在序列缺失删除或扩增的基因组区域，这表明它们很可能在癌症的发展过程中扮演很重要的角色。miRNA的编辑位点已经在近年来的研究中被发现了，表明RNA编辑和miRNA介导的调控之间可能存在着关联。同时由于miRNA的测定简单、相对稳定，并且在大量mRNA的控制上起作用，这就让miRNA成为癌症诊断以及治疗期间的检测和分期过程中极具吸引力的标志物。 Law P. T., Qin H., Ching A. K., Lai K. P., Co N. N., et al. (2013) Deep sequencing of small RNA transcriptome reveals novel non-coding RNAs in hepatocellular carcinoma. J Hepatol这篇文章描述了一种新的PIWI-互作RNA(piRNA)piR-Hep1，它参与了肝脏肿瘤发展。与周围正常肝脏相比，46.6%的肝癌细胞(HCC)存在piR-Hep1表达上调。piR-Hep1的沉默抑制了细胞活力、运动和侵袭。作者还发现miR-1323在HCC中的大量表达，以及它与肝硬化背景下所产生的肿瘤的独特关联。 实验设计上的注意事项 测序深度与检测灵敏度是直接相关的。在典型的实验中，如果测序流动槽（Flowcell）的一条通道（lane）只上一个样本，那么测序深度会非常高，这能实现极其灵敏的检测。基于此原因，miRNA的测序深度很少成为考虑因素。在筛查应用中，或不需要如此高深度检测的研究中，样品可加上测序Index标签，从而能够在同一个测序lane中上样多个不同的样本。需要注意的是，在设计miRNA的测序深度时，要时刻记住miRNA控制基因表达，因而miRNA水平的小变化可能影响许多编码蛋白。 新发现的miRNA应当通过功能分析(如Ago2结合或敲除实验)来确认。 实验应当包含足够的样品，以便结论具有真正的统计意义和高的可信度。一般来说，建立一个可能的假设相对来说是比较容易的，只要能证明miRNA存在于患者的肿瘤中，即便样本数量不多也是足够的。但是要真正验证假设就没那么容易了，通常需要大量的患者来检验，并建立统计学可信度。目前，关于如何在测序研究中建立统计学可信度和多重检验纠正，还没有特别公认的方法。当前基于测序的miRNA分析并没有实现miRNA表达的绝对定量，而仅是不同miRNA(如肿瘤-正常配对)的相对量。 样品分层是癌症样品的一个问题。一个特定的癌症表型可能代表几种不同的病因和机理。为了要进行严格的分析，应当有足够多的样品量，从而能够充分代表每个肿瘤亚型。而miRNA的表达会随着肿瘤的发展而变化，因此在实验设计时应建立肿瘤分期和分级。 RNA-蛋白结合（CLIP-Seq）在人类细胞中，大多数mRNA(或前体mRNA)与核不均一性核糖蛋白(hnRNP)相结合，形成大的hnRNP-RNA复合物。hnRNA蛋白在RNA加工的所有关键环节中都发挥重要作用，包括前体mRNA剪接以及mRNA出核、定位、翻译和稳定性。几十种RNA结合蛋白(RBP)和基因的hnRNP蛋白与癌症相关联。 RNA-蛋白的相互作用可通过交联免疫沉淀测序(CLIP-Seq)来测定。在CLIP-Seq中，细胞经过紫外线处理，让RBP与RNA复合物共价交联。细胞随后被裂解，RBP-RNA复合物被免疫共沉淀，从而测序相应的RNA。 Wilbert M. L., Huelga S. C., Kapeli K., Stark T. J., Liang T. Y., et al. LIN28 binds messenger RNAs at GGAGA motifs and regulates splicing factor abundance. Mol Cell 48: 195-206LIN28是个保守的RNA结合蛋白，它与多能性、重编程和肿瘤的形成相关。在各种不同的癌细胞和原发肿瘤组织中都发现LIN28的异常上调。在这篇文章中，作者利用CLIP-Seq鉴定了大约四分之一分布于人类转录本中LIN28的结合位点。从这些结合位点中他们发现，LIN28与mRNA中富含环结构的GGAGA序列结合。同时还发现了，LIN28的表达能够导致可变剪切中的下游变化。 表观遗传和甲基化癌症发展过程中的表观遗传改变与异常的基因表达相关联。近期的研究证据表明，表观遗传上的改变可能在癌症 起始中 发挥作用。表观遗传的控制是通过多个不同过程进行介导的，包括DNA修饰（甲基化或乙酰化）、组蛋白修饰和核小体重塑。发现控制表观基因组的基因发生突变，在人类癌症中是一个相当普遍的现象。NGS测序技术提供了一整套工具，可定位突变并测定它们对癌症发展的影响。 癌症中表观遗传修饰物的基因突变。在不同类型的癌症中经常观察到三类表观遗传修饰物发生突变，这突出了遗传学和表观遗传学之间的串扰。表观遗传修饰物的突变有可能引起癌症的全基因组表观遗传改变。了解遗传和表观遗传变化的关系将为癌症治疗提供新的见解。 DNA修饰DNA修饰目前可以很容易地通过多种技术来进行测定。不同技术的选择取决于所需的通量和分辨率。 Bert S. A., Robinson M. D., Strbenac D., Statham A. L., Song J. Z., et al. Regional activation of the cancer genome by long-range epigenetic remodeling. Cancer Cell 23: 9-22文章作者通过协同的长距离表观遗传激活（LREA）技术，鉴定出一种结构域基因去调控的机制。这些区域通常会跨越1Mb的基因组长度，包括关键癌基因、microRNA和癌症的生物标志物基因。LREA结构域中基因启动子的特点是活性染色体标记的的获得和抑制标记的丢失。 Brastianos P. K., Horowitz P. M., Santagata S., Jones R. T., McKenna A., et al. Genomic sequencing of meningiomas identifies oncogenic SMO and AKT1 mutations. Nat Genet 45: 285-289为了鉴定和验证脑膜瘤中的体细胞遗传改变，作者对17个脑膜瘤进行了全基因组或外显子组测序，并对另外48个肿瘤进行了靶向测序。他们所观察到的突变谱是分布广泛，但他们证实了43%的肿瘤中存在病灶NF2失活，并在另外8%的肿瘤中发现表观遗传修饰物的改变。 Duncan C. G., Barwick B. G., Jin G., Rago C., Kapoor-Vazirani P., et al. A heterozygous IDH1R132H/WT mutation induces genome-wide alterations in DNA methylation. Genome Res 22: 2339-2355脑胶质瘤、急性骨髓性白血病和软骨瘤中经常发生NADP+依赖的异柠檬酸脱氢酶IDH1和IDH2的单等位点基因点突变。作者表明，IDH1R132H等位基因的杂合表达足以诱导这些肿瘤特有的以DNA甲基化为特征的全基因组改变。这说明了IDH1R132H/WT突变体是推动人类癌细胞的表观遗传不稳定性的原因。 Zhang J., Benavente C. A., McEvoy J., Flores-Otero J., Ding L., et al. A novel retinoblastoma therapy from genomic and epigenetic analyses. Nature 481: 329-334视网膜母细胞瘤是一种发生于视网膜的侵袭性儿童癌症，由RB1失活所引发，但潜在机理仍然未知。在此类高度侵袭性的癌症中，许多基因都参与其中，但RB1是唯一的已知发生突变的癌基因。与有限的体细胞突变不同，相对正常的成视网膜细胞，肿瘤的甲基化图谱表现出巨大的改变。最惊人的结果之一是人视网膜母细胞瘤中原癌基因脾络氨酸激酶（SYK）的诱导表达。SYK是肿瘤细胞生存所必需的。研究人员接着表明，小分子抑制剂对SYK的抑制导致培养和体内的视网膜母细胞瘤的细胞死亡。 实验设计上的注意事项 每种组织和细胞类型都有着独特的甲基化模式；因此必须获得感兴趣的组织以便分析。癌症研究中，肿瘤组织-癌旁正常组织的配对可简化分析。 通过重亚硝酸氢盐测序所产生的超大量CpG标志物很难解释，且可靠的统计学分析目前仍然困难重重。不过，下面一些实际的方法可简化分析： RRBS-Seq通过限制覆盖度而简化分析； 综合分析大大改善了结果的可解释性。例如，将表达分析与甲基化分析相结合，让我们可专注于表达水平改变的基因； 将分析限制在某个感兴趣的基因或区域中。这种方法对GWAS的后续研究很有效，也适合已有实验证据说明目的区域存在基因调控或染色质重塑的研究。与降低代表性的方法不同。这种方法实现了更多区域的分析，因此可获得更多信息。 组织培养物应当谨慎使用。随着时间的推移和组织增殖，培养物的甲基化水平可能已经改变，不大能代表原先的组织样本。 组蛋白修饰组蛋白修饰通常指的是甲基化和乙酰化。组蛋白H3K9、H3K27和H4K20的甲基化与基因转录的抑制相关，而H3K4和H3K36的三甲基化与活性转录的染色质相关。组蛋白乙酰化几乎总是与染色质可接近性和转录活性水平的增高相关。通过操控染色质状态和DNA可接近性，表观遗传修饰在各个发育阶段、组织类型和疾病中都对基因表达的控制起着关键作用。 Wilkinson A. C., Ballabio E., Geng H., North P., Tapia M., et al. (2013) RUNX1 is a key target in t(4;11) leukemias that contributes to gene activation through an AF4-MLL complex interaction. Cell Rep 3: 116-127这篇文章报道了一种转化机制，其中两个致癌融合蛋白合作激活目的基因，然后调节其下游产物的功能。 实验设计上的注意事项 每种组织和细胞类型都有着独特的甲基化模式；因此必须获得目的组织以便分析。 组蛋白修饰可以通过各种ChIP-Seq方法进行检测，原理是通过抗体与目标甲基化组蛋白进行特异结合。 同样，组织培养物应当谨慎使用。随着时间的推移和组织增殖，培养物的甲基化水平可能已经改变，不大能代表原先的组织样本。 染色质结构与重排染色体重排需要DNA双链断裂的形成和连接。这些事件的发生，会破坏基因组的完整性，并经常在白血病、淋巴瘤和肉瘤中观察到。并且特定基因间的反复的基因融合在不同的个体中均观察到，这表明这些基因一定在细胞周期中的某个阶段他们之间的物理位置非常接近。 这是一个设想的三维、具有转录活性的复合物，它包含了致密的环化位置。这个示意图是基于检测到的成环事件，并假设所有成环事件都能发生在单个细胞内。在这个模型中，所有小环汇集到一个共同的核心（蓝色球）。环降低了转录活性复合物的物理大小，从而推动转录因子接近待定的基因组位点。 检测染色质相互作用。在三维空间中，相同或不同染色体上远端的基因组区域相互作用，而这种相互作用是由一个或多个DNA结合蛋白介导的。a) ChIP-Seq 利用染色质免疫共沉淀来鉴定DNA和蛋白的相互作用。人们采用各种DNA-片段化方法和核酸外切酶，来缩小片段的大小分布。b)染色质构像捕获实验利用一个连接步骤将互作的染色质片段相连接。这种方法可鉴定与遥远序列相结合的蛋白。c)配对末端标签测序分析染色质相互作用（ChIA-PET）同样也利用连接步骤来检测染色质相互作用，将不相邻的互作区域配对。然而，ChIA-PET利用染色质免疫共沉淀（ChIP）步骤，只能鉴定特定蛋白的相互作用，如RNA聚合酶II。 参考文献 Papantonis A., Kohro T., Baboo S., Larkin J. D., Deng B., et al. TNFalpha signals through specialized factories where responsive coding and miRNA genes are transcribed. EMBO J 31: 4404- 4414作者利用测序以及染色体构像捕获（3C）和ChIA-PET表明，TNF_alpha诱导响应基因聚集在分散的『NF-B工厂』。一些『工厂』还专门转录编码miRNA的响应基因，这些miRNA靶定下调的mRNA。 Rocha P. P., Micsinai M., Kim J. R., Hewitt S. L., Souza P. P., et al. Close proximity to Igh is a contributing factor to AID-mediated translocations. Mol Cell 47: 873-885细胞核组织可决定『脱靶』活性以及融合伴侣的选择。这项研究表明，绝大多数已知的活化诱导胞嘧啶核苷脱氨酶（AID）介导的Igh转位伴侣在类型转换过程中与此位点接触的染色体结构中被发现。此外，这些相互作用结构域可用来鉴定被AID靶定的其他基因。 Theodoratou E., Montazeri Z., Hawken S., Allum G. C., Gong J., et al. Systematic meta- analyses and field synopsis of genetic association studies in colorectal cancer. J Natl Cancer Inst 104: 1433-1457这篇研究文章表明，T细胞特异的转录因子GATA3在介导增强子接近调控区域上扮演了重要角色，这些调控区域参与了雌激素受体（ESR1）介导的转录。GATA3沉默导致在雌激素刺激之前辅助因子和活性组蛋白标记的整体重新分配。 Hakim O., Resch W., Yamane A., Klein I., Kieffer-Kwon K. R., et al. (2012) DNA damage defines sites of recurrent chromosomal translocations in B lymphocytes. Nature 484: 69-74作者发现，在培养的小鼠B淋巴细胞中，缺乏经常性的DNA损伤时，Igh或Myc与其他所有基因之间的易位与它们的接触频率直接相关。反过来，与经常性位点指向的DNA损伤相关的易位与DNA断裂形成的速率成正比。他们认为，非定向重排反映了细胞核结构，而DNA断裂形成决定了包括驱动B细胞恶性肿瘤在内的经常性易位的位置和频率。 综合分析（多组学分析）所有的生物过程都是相互关联的，而在癌细胞发生过程的任何一个变化都会影响其他所有过程。突变可能影响所表达的活性，继而又影响DNA甲基化，再就影响其他许多基因的表达等等一连串的反应。每个个体都有着大量的特有突变，再加上这一连串的事件，能够让人们深入研究各种用于区分癌症的疾病表型。综合分析可以使揭示癌症生物学的真正复杂性向前迈进了一步。研究人员如今能够检测大部分的单个过程，但认识和治疗癌症的真正进步将来自于对所有这些过程的综合分析，也就是常说的多组学分析。 参考文献 Weischenfeldt J., Simon R., Feuerbach L., Schlangen K., Weichenhan D., et al. (2013) Integrative genomic analyses reveal an androgen-driven somatic alteration landscape in early-onset prostate cancer. Cancer Cell 23: 159-170作者发现，早发性前列腺癌的形成涉及到雄激素驱动的结构重排。相比之下，老年发病的前列腺癌积累了非雄激素相关的结构重排，表明一种不同的肿瘤形成机制。 Cowper-Sal lari R., Zhang X., Wright J. B., Bailey S. D., Cole M. D., et al. (2012) Breast cancer risk- associated SNPs modulate the affinity of chromatin for FOXA1 and alter gene expression. Nat Genet 44: 1191-1198作者表明，与乳腺癌风险相关的SNP集中在FOXA1和ESR1的顺反组（cistrome），以及组蛋白H3赖氨酸4单甲基化（H3K4me1）的表观基因组。大多数的风险相关SNP调控FOXA1在远端调控元件的染色质亲和力，这导致等位基因特异的基因表达。 Peifer M., Fernandez-Cuesta L., Sos M. L., George J., Seidel D., et al. (2012) Integrative genome analyses identify key somatic driver mutations of small-cell lung cancer. Nat Genet 44: 1104-1110作者发现了TP53和RB1失活的证据，并在编码组蛋白修饰物的基因中发现了反复的突变。此外，他们还在PTEN、SLIT2和EPHA7中观察到突变，以及FGFR1络氨酸激酶基因的病灶扩增。这种综合分析表明，组蛋白修饰可能与小细胞肺癌（SCLC）有关。 技术参考一个良好的实验设计可以提高技术性能，从而产生最易解释和可靠的结果。这里重点强调研究人员在设计实验时必须牢记的生物学和技术的特性。 癌症中的实验设计面临一些独特的挑战。典型的肿瘤样本包含两个基因组：遗传自父母的生殖细胞系（germline）和在疾病发展过程中积累的体细胞突变（somatic mutations）。肿瘤细胞在样本中的比例可能在10%-100%之间。肿瘤基因组也是动态的，会快速积累 de novo 突变。因此，每一个肿瘤中又可能同时包含几个克隆亚型。 目前大部分已发表研究的样本量都非常小，可被视为仅是提出了相关的假说。而随着越来越多的测序信息被我们所获得，大部分癌症类型可根据其分子表型，被分成多个亚群。这严重降低了实验的能力，并增加了严格分析所需的样本数量。部分解决这一难题的方案是在探索阶段利用全基因组测序来寻找新的突变。在第二阶段，利用全外显子组或靶向测序来确认新发现的这些突变，并确定他们在大型队列中的丰度。然而，在未来，统计学上严谨的全基因组测序实验有可能是非常大的，需要数千个样本。 使用NGS测序技术进行深度测序是指多次生成定位到同一区域的序列片段，有时达上百次。但由于每条序列片段是从单个DNA分子中产生的，故提高深度测序能够实现原始样品中低至1%的克隆的检测。比较同一个体的肿瘤和癌旁正常组织的序列，我们可以很容易识别浸润组织的序列片段。最佳的读取深度将取决于癌症类型和所需的灵敏度，但一般建议为正常基因组最低为40倍的覆盖深度，而癌症基因组需要80倍以上的覆盖深度。在肿瘤高度异质时，可能需要肿瘤不同部位的多次活检，才能包含所有的细胞类型。 在这个假定的例子中，肿瘤含有两个癌症克隆和邻近组织。肿瘤样本中正常细胞所产生的序列（图中：比对中的前两个序列）可通过与邻近正常组织所产生的序列比较后确定。肿瘤样本中的剩余序列可分为两组，分别代表主要和次要的肿瘤克隆。次要克隆，若不及时治疗，可能在复发时成为肿瘤的主要组分。在实际分析中，肿瘤样本至少要有40倍的覆盖深度，并覆盖靶定的基因集合、全外显子组或全基因组。 检测癌症基因组中的体细胞突变通常有三种方法：全基因组测序、全外显子组测序和靶向基因测序。下表简要介绍了每种方法的有点和缺点。在比较多发性骨髓瘤的全基因组和外显子组测序时，半数的蛋白编码突变通过染色体畸变（如易位）而存在，其中大部分不能单独被外显子组测序而发现。靶向重测序是一种有用的技术，可收录超大队列中已知癌症相关基因的突变。从长远来看，随着我们对基因组的了解日益加深，并且我们处理和解释大型数据集的能力的提高，全基因组测序无疑是最好的肿瘤分子鉴定方法。而在短期内，靶向基因测序可为患者匹配出市场上已有的药物，让他们立刻受益。 参考来源 Illumina cancer research]]></content>
      <categories>
        <category>癌症基因组</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>癌症</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用Shapeit2对人类基因组数据进行phasing]]></title>
    <url>%2F2016%2F03%2F15%2F2016-03-15-how-to-phase-the-human-genome-by-shapeit2.html</url>
    <content type="text"><![CDATA[SHAPEIT(2.0)是专门用于对推断基因组单体型的软件，有牛津大学的团队所开发，并且一直应用与千人基因组计划中。 以下，我将记录如何通过shapeit2对人群的变异数据集（VCF 格式）进行phasing，并构造出reference panel的过程。 首先，准备文件。整个过程只需要变异数据集（VCF 格式），样本信息文件(sample.fam)，genetic_map文件和参考序列（fasta格式）。对于genetic_map文件需要单独做些说明，这个记录的是基因组中各个位点的重组率和物理距离之间的关系，这是phasing过程非常重要的一个文件。它来自于人类单体型计划-Hapmap计划，可以下载,最新版是b37或者说hg19，如果你的reference版本高于hg19，则需要liftover，liftover之后那些顺序发生交叉的位点，是liftover的错误导致的，要去掉。但是要注意的是，genetic-map中两个位点之间的重组率（recombination rate）是不变的，这其实也很好理解，reference之所以需要升级，是因为它的组装结果并非是百分百符合真实情形的，随着技术的进步，我们会不断去升级逼近这个真实情况，但重组率是根据群体的重组情况来计算的，它是由真实情况反映出来的，因此即便reference版本改变了，它的值也不需要改变。不过对于两个位点之间的物理距离（physical distance）就不同了，leftover之后，这个距离是会发生变化的，通过和原点（一般是重组率为0的点或者就是各个染色体的第一个位点）的距离比例来调节。 至于样本信息文件，格式如下： 12345671009 1009-01 0 0 1 341009 1009-02 0 0 2 331009 1009-06 1009-01 1009-02 2 671030 1030-01 0 0 1 431030 1030-02 0 0 2 441030 1030-06 1030-01 1030-02 1 71 其他的两份文件就不必多说了。 准备好以上文件之后接下来就是主要的步骤了： 第一步，将vcf转化为bed/bim/fambed/bim/fam这三个文件是phasing的常用谱系文件格式。做这一步转换的工具有很多，我们这里直接借助GATK的VariantsToBinaryPed模块进行转换： 123456789time java -Xmx8g -jar GenomeAnalysisTK.jar -T VariantsToBinaryPed \ -R hg20.fa \ -V chr22.vcf \ --metaData sample.fam \ -mgq 0 \ -bed chr22.bed \ -bim chr22.bim \ -fam chr22.fam \ -log gatk.log &amp;&amp; echo "** done **" &amp;&amp; sed 's/^chr//g' chr22.bim &gt; t.bim &amp;&amp; mv -f t.bim chr22.bim 这个执行命令的最后多加了一小步：将原来输出的.bim文件中第一列的chr22换成了22。之所以要费这个小周折，是因为如果不做这个小操作，接下来的plink步骤中，会直接报ERROR: Problem reading BIM file, line 1退出，原因就是它不允许chr的开头，至于具体的原因我也没去细查。 第二步，过滤genotype高missing rate和孟德尔错误的位点12plink=/com/extra/testing/bin/plinktime $plink --noweb --bfile chr22 --keep-allele-order --me 1 1 --set-me-missing --make-bed --out chr22.nomendel &amp;&amp; echo "** nomendel done **" &amp;&amp; time $plink --noweb --bfile chr22.nomendel --keep-allele-order --geno 0.05 --make-bed --out chr22.nomendel.filter &amp;&amp; echo "** fileter done **" 第三步，phasing这是phasing的最后一步了，这里分成两小步，phasing和输出格式转换：1234567891011121314# phasingtime shapeit2 \ --duohmm \ -W 5 \ --input-bed chr22.nomendel.filter.bed chr22.nomendel.filter.bim chr22.nomendel.filter.fam \ --input-map genetic_map.chr22.txt \ -O hapData \ --thread 1 &amp;&amp; echo "** panel done **"# 格式转换time shapeit2 -convert \ --input-haps hapData \ --output-vcf chr22.haps.vcf \ --output-ref chr22.phased.hap chr22.phased.leg chr22.phased.sam &amp;&amp; echo "** all done **" 以上输出结果中，chr22.haps.vcf便是进行phasing之后的结果，而chr22.phased.hap和chr22.phased.leg这两个文件是从chr22.haps.vcf中得到的，它们便是Imputation分析中的reference panel。 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>phasing</category>
      </categories>
      <tags>
        <tag>shapeit2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用Python绘制GWAS分析中的曼哈顿图和QQ图]]></title>
    <url>%2F2016%2F02%2F02%2F2016-02-02-How-to-create-manhattan-and-qq-plot-for-GWAS-study-in-Python.html</url>
    <content type="text"><![CDATA[【前言】这篇文章使用geneview完成这两类图的作法，它是一个Python高级库，建立在matplotlib的基础之上，专门用于基因组数据的可视化，目的是为了使创建高大上（精致）的基因组数据图表变得简单。目前该发布的Python包中已经内置多个优美的调色板和风格（默认情况下就能创建赏心悦目的图形），同时已经集成了曼哈顿图和Q-Q图的绘制函数。作为该Python包的主要开发者，只是如此是远远不够的，在未来的日子里，我希望它能在功能不断完善的同时也变得更加易用。 曼哈顿图和QQ图是两个在全基因组关联（GWAS）分析里面最常出现的图形，基本上已经是GWAS的标配，几乎在每篇GWAS的文章都会见到，它们的作用和所要传达出来的信息我也在上一篇关于GWAS的博文中做了些说明，在这里我们就只集中在如何用Python和geneview将其有效地展现出来。 首先，准备一些数据来作为例子。 我这里用来展现的数据是2011年丹麦人所做过的一个关于年轻人过度肥胖的GWAS研究——GOYA，数据也是从他们所发表的结果中获得，总共有5,373个样本，其中超重的个体（case）有2,633个，正常的个体（control）是2,740个，从样本量上看还算可以。为了方便使用，我对其做了相关的处理，包括从PED和MAP文件到GEN文件的生成，并重复了一次case-control的关联性分析，计算出了芯片上所研究的各个SNP位点与肥胖相关的显著性程度（即p-value），最后又将结果数据抽取出来做成数据集——放在这里供下载（15.6Mb，csv格式）。 【注】以上内容虽提及到了一些领域内术语和相关文件格式，但若不懂也请不必纠结，因为后续处理都是基于这个最终的数据集来完成的。 接着，需要将geneview软件包加入到你的Python中，有多种不同的方式，但推荐直接使用pip，以下是安装比较稳定的发布版，直接在终端命令行下(Linux or Mac)输入： 1pip install geneview 或者，也可以直接从github上安装正在开发的版本： 1pip install git+git://github.com/ShujiaHuang/geneview.git#egg=geneview 第三种办法就是直接下载源码，然后自行编译，虽然不推荐这种做法（因为还有依赖包必须自行下载安装，过程会比较麻烦低效），但对于某些不能连接外网的集群也只能如此，这三种方式都是可行的。 曼哈顿图将示例数据下载下来： 1wget https://raw.githubusercontent.com/ShujiaHuang/geneview-data/master/GOYA.csv ./ 先简单地查看一下数据的格式: 12345678910chrID,rsID,position,pvalue1,rs3094315,742429,0.1445861,rs3115860,743268,0.2300221,rs12562034,758311,0.6443661,rs12124819,766409,0.1462691,rs4475691,836671,0.4581971,rs28705211,890368,0.3627311,rs13303118,908247,0.229121,rs9777703,918699,0.379481,rs3121567,933331,0.440824 一共是4列（逗号分隔），分别为：[1]染色体编号，[2]SNP rs 编号，[3] 位点在染色体上的位置，[4]显著性差异程度（pvalue）。在本例曼哈顿图中我们只需要使用第1,3和4列；而QQ图则只需要第4列——pvalue。 下面我们先从绘制曼哈顿图开始。我们先将需要的数据读取到一个列表中，可以这样做： 12345678import csvdata = []with open("GOYA.csv") as f: f_csv = csv.reader(f) headers = next(f_csv) data = [[row[0], int(row[2]), float(row[3])] for row in f_csv] 现在GOYA.csv中的数据就都存放在data列表中了，由于Python在读取文件中数据时，都是以string类型存放，因此对于第3和第4列的数据有必要事先把做点类型转换。 接下来，调用geneview中的曼哈顿图函数。 123456import matplotlib.pyplot as pltfrom geneview.gwas import manhattanplotax = manhattanplot(data, xlabel="Chromosome", ylabel="-Log10(P-value)") # 这就是Manhattan plot的函数plt.show() 只需这样的一句代码就能创建一个漂亮的曼哈顿图，有必要再次指出的是，geneview是以matplotlib为基础开发出来的，所创建的图形对象实际上仍属于matplotlib，geneview内部自定义了很多图形风格，同时封装了大量只属于基因组数据的图表类型，但图形的输出格式以及界面显示都仍和matplotlib一样，因此在这里我们使用matplotlib.pyplot的show()函数(上例中：plt.show())将所绘制出来的曼哈顿图显示出来。如果要将图形保存下来，则只需执行plt.savefig(&quot;man.png&quot;)，这样就会在该目录下生成一个名为『man.png』png格式的曼哈顿图，若是要存为pdf格式，则只需将所要保存的文件名后缀改成『.pdf』（plt.savefig(“man.pdf”)）就可以了。下面这些格式：emf, eps, pdf, png, jpg, ps, raw, rgba, svg, svgz等都是支持的，至于最新的还有多少种，还请参照matplotlib文档中说明。 此外，geneview中的每个画图函数都有着足够的灵活性，我们也可以根据自己的需要做一些调整，比如： 123456789xtick = ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','16','18', '20','22']manhattanplot(data, xlabel="Chromosome", # 设置x轴名字 ylabel="-Log10(P-value)", # 设置y轴名字 xtick_label_set = set(xtick), # 限定横坐标轴上的刻度显示 s=40, # 设置图中散点的大小 alpha=0.5, # 调整散点透明度 color="#f28b1e,#9a0dea,#ea0dcc,#63b8ff", # 设置新的颜色组合 ) 实现新的颜色组合、限定x轴上的刻度显示和散点大小的调节。甚至还可以将散点改为线： 12345678manhattanplot(data， xlabel="Chromosome", # 设置x轴名字 ylabel="-Log10(P-value)", # 设置y轴名字 xtick_label_set = set(xtick), # 限定横坐标轴上的刻度显示 alpha=0.5, # 调整散点透明度 color="#f28b1e,#9a0dea,#ea0dcc,#63b8ff", # 设置新的颜色组合 kind="line" ) 其它方面的调整请查看geneview文档中的相关说明。 Q-Q图qq图只需用到上例中的pvalue那一列：123456789101112import csvimport matplotlib.pyplot as pltfrom geneview.gwas import qqplotpvalue=[]with open("GOYA.csv") as f: f_csv = csv.reader(f) headers = next(f_csv) pvalue = [float(row[3]) for row in f_csv]ax = qqplot(pvalue, color="#00bb33", xlabel="Expected p-value(-log10)", ylabel="Observed p-value(-log10)") # Q-Q 图plt.show() 同样，也可以根据自己的需要对改图进行相关的调整。 以上，便是如何使用Python来制作Manhattan图和QQ图的方法，geneview的集成函数简化了这样的一个过程。 另外，如果你也看过丹麦人的这个GOYA研究，就会发现实际以上的两个图和其文章中的基本是一致的，当然我自己做了些数据清洗的操作，结果上仍然会有些许的不同。虽然此刻下结论还有点为时尚早，但总的来讲，我应该也可以通过这个数据集比较顺利的将其结果重复出来了。 最后，附上利用geneview画曼哈顿图和QQ图的代码： （1）曼哈顿图： 123456789101112131415import sysimport csvimport matplotlib.pyplot as pltfrom geneview.gwas import manhattanplotwith open("data/GOYA.csv") as f: f_csv = csv.reader(f) headers = next(f_csv) data = [[row[0], int(row[2]), float(row[3])] for row in f_csv]ax = manhattanplot(data, xlabel="Chromosome", ylabel="-Log10(P-value)")plt.show() （2）QQ图：12345678910111213import csvimport matplotlib.pyplot as pltfrom geneview.gwas import qqplotpvalue=[]with open("data/GOYA.csv") as f: f_csv = csv.reader(f) headers = next(f_csv) pvalue = [float(row[3]) for row in f_csv]ax = qqplot(pvalue, color="#00bb33", xlabel="Expected p-value(-log10)", ylabel="Observed p-value(-log10)")plt.show()]]></content>
      <categories>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>GWAS</tag>
        <tag>manhattan plot</tag>
        <tag>Q-Q plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Flask开发基因组数据web服务的RESTful API（1）]]></title>
    <url>%2F2015%2F12%2F20%2F2015-12-20-Develop-a-genome-data-RESTful-API-server-by-flask-1.html</url>
    <content type="text"><![CDATA[这真的是一个系统工程！ 首先，我要开发一个在线的基因组数据库，目的是能够以符合RESTful API设计准则来进行访问的数据Web服务。虽然，此前我没接触过任何Web开发，想想也是困难重重，但这并不能阻止我——兴趣所在，而且我自己也更清楚需求是什么。 那么说干就要干。经过一番有限的比较之后（主要是看各种技术论坛和博客），我发现Rails、Django和Flask都适合用来干这个事情，他们的文档都很好，是目前Web开发的主流，社区活跃，用的人多了碰到问题也容易找到解决方案，但是觉得Rails和Django太大，太系统，以至于冗余，我希望的是微框架，因为初步搭成只是第一步，后面一定会有很多自定的优化和开发，而不是一次就完成，太冗余不灵活的话反而会极大影响自己的后续步骤，灵活的可拓展性对我而言反而更重要。这么考虑之后，我就选了Flask，其他的组件需要的时候加入即可，还可以随时换掉，或者自己重写，而且项目结构是完全自定的，这就很适合我的口味了，框架功能不需多，只要解决了Web开发中最重要的问题，作为一个最小可行集就OK——就像人体肠道最小宏基因组一样，浓缩即是精华，其他的枝叶要能够被灵活地删增。 那么选好之后，接下来我是怎么做的呢？第一，学会了Flask，读了很多技术博客，操练了一些如何例子，比如使用 Python 和 Flask 设计 RESTful API，The Flask Mega-Tutorial等，同时读了《Flask Web开发：基于Python的Web应用开发实战》，这对于我这种从未接触过Web应用开发的小白来说，真是一本好书，读了之后真是有一种相见恨晚的感觉，内容很好读[但我并不是说它读起来容易。虽然它确实是从基础的讲起，然而对我这类没有任何基础的人来说并不十分容易，好在]从开发到测试到部署每一步都十分清晰，很多内容讲得比博客清楚得多多了，而且整本书本身就是项目驱动的，就像我也是项目驱动要去用Flask一样。第二，要学会数据库。其实这一步和前面是分不开了，基本都是同时进行。但在选择使用那种数据库这一步中我也花了不少心思，最后按照我的数据情况，我选择了非关系型数据库MongoDB。 在这个一周多的学习时间中（工作之外的时候），我学到了很多，不但实现了一个完整的Web应用，还进一步加深加强了对如何更有效应用python的理解和认识。对我提升最大的还不是会了如何用flask去做一个简单的web，而是它们的设计理念，程序/项目如何布局，如何分离，如何做到低耦合，测试和性能分析应该怎么做才合理有效，这些理念是我们平时做基因组数据分析所缺少（马虎）的，因为本来许多生物信息工程师并不懂web开发，甚至从未有过任何IT软件设计的训练，平时也难有时间专门去学习，很多时候都是，任务来了要赶紧写个程序解决一下，然后再来一个任务，又再写个程序处理一下，如此反复，时间久了这些零散的程序根本难以复用，最后慢慢地也就成了垃圾程序。就算是比较大型的生信软件的开发，过程和布局也欠缺规范，关于这一点我深有体会，也想抽时间去系统学习，但实际操作起来并没严格注意，更多的是直接参看github上一些项目的布局和设计来依样画葫芦而已。 接下来就难了，要做符合RESTful API准则的数据web服务，那么就得开始设计API了。怎么做？不同版本如何管理，如何语义化，如何条理化管理新旧版本的内容和功能？如何把基因组数据资源转换为JSON格式的序列化字典，大数据资源如何分页等等诸多的细节都需要一一考虑，我参考了Solvebio，23andme，豆瓣等，最后得到了我自认为比较合理的设计方式。 （未完待续）]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>RESTful API</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么说FPKM/RPKM是错的]]></title>
    <url>%2F2015%2F08%2F25%2Fwhy-fpkm-and-rpkm-are-wrong.html</url>
    <content type="text"><![CDATA[两周前，我接触了一个RNA-seq的项目，做完之后，我重新思考了FPKM和RPKM的计算，觉得它们很可能是不对的，后来查阅了一些文献终于验证了我的想法。现在我重新将这个过程记录了下来： FPKM和RPKM分别是什么RPKM是 Reads Per Kilobase Per Million的缩写，它的计算方程非常简单： $$RPKM = \frac{10 ^ 6 \times n_r}{L × N}$$ 其中，$n_r$ 是比对至某一个基因的read数量；$L$是该基因的外显子长度之和除以1000，因此，要注意这里的$L$单位是kb，不是bp；$N$ 是 有效比对至基因组的总read数量。FPKM是 Fregments Per Kilobase Per Million的缩写，它的计算与RPKM极为类似，如下: $$FPKM = \frac{10 ^ 6 \times n_f}{L \times N}$$ 其中，$n_f$是比对至目标基因的fregment数量。FPKM与RPKM唯一的区别是：F代表fragments，R代表reads。如果是Pair-end测序，每个fragments会由这两个成对的reads构成，因此FPKM只计算两个reads能比对到同一个转录本的fragments数量；而RPKM计算的是可以比对到转录本的reads数量（不管Pair-end的两个reads是否能比对到同一个转录本上）。如果是single-end测序，那么FPKM和RPKM计算的结果将是一致的。 以上是这两个量的计算方式。这样计算的目是为了解决在计算RNA-seq转录本丰度时的两个bias： （1）相同表达丰度的转录本，往往会由于其基因长度上的差异，导致测序获得的Read（Fregment）数不同。总的来说，越长的转录本，测得的Read（Fregment）数越多，但这并不代表表达量就真的多。 （2）由测序文库的不同大小而引来的差异。即同一个转录本，其测序深度越深，通过测序获得的Read（Fregment）数就越多。 FPKM和RPKM通过同时除以L（转录本长度）和除以N（有效比对的Read（Fregment）总数）的办法，最终将不同样本（或者同个样本在不同条件下）的转录本丰度归一化到一个能够进行量化比较的标准上。 上面的式子看起来似乎合情合理，但是它们却都做错了。 为什么FPKM/RPKM是错的要回答这个问题，我们需要先撇开所有形式上的计算，重新思考这个问题——到底什么是RNA转录本的表达丰度？事实上，对于任何一个取得的样本，它上面任何一个基因的表达量（或者说丰度），都将已是一个客观存在的值，这个值是不管你改变了多少测序环境都不会变的。而且细胞中此刻总共有多少个基因在表达，实际上也已经是客观定好了的。一旦我们开始以这样一种“先知”的形式来理解的时候，有趣的事情就开始出现了。 此时，我们可以假定，对于样本X，其中有一个基因G被转录了g次，同时样本X中所有基因的转录总次数假定是total，那么正确描述基因G转录丰度的值应该是： $$r_{g}=\frac{mRNAg}{mRNA{total}}$$ 没毛病！而且与此同时，样本X中其他基因转录丰度的计算也和以上式子类似，除了要把分子换为其他基因对应的转录次数之外，分母都一样。于是这个有趣的事情就是，所有基因转录本丰度的均值$r_{mean}$将是一个恒定不变的数，由以上定义这个数就是： $$r{mean} = \frac{1}{g{total}}\sum_{g}^{G}{rg} = \frac{1}{g{total}}\frac{\sum_{g}^{G}{mRNAg}}{mRNA{total}}$$ 而 $$\sum_{g}^{G}{mRNAg} = mRNA{total}$$ 所以 $$r{mean} = \frac{1}{g{total}}$$ 这个值是由基因的总数决定的，也就是说，对于同一个物种，不管它的样本是哪种组织（正常的或病变的等），也不管有多少个不同的样本，只要它们都拥有相同数量的基因，那么它们的$r_{mean}$都将是一致的。这是一个在进行比较分析的时候，非常有意义的恒等关系。 但在实际的操作中，我们是难以直接计算这些r值的。好在只要能够保证模型的自洽性，我们是能通过自建一些统计量来对r值进行间接描述的，比如FPKM和RPKM。本质上它们的目的就是为了描述r。虽然如此，但我们也要注意，所有这些要用来描述转录本丰度的统计量，都应该能等价描述这一恒等关系。也就是说，不管我们使用了什么统计量，它所描述出来的转录本丰度应该且必须是真实丰度$rg$的m倍（m必须是一个根据模型定出的不变值），它的均值也将是$r{mean}$的m倍，至少这样才是得到有意义结果的前提！ （那么）现在，我们回过头来看看FPKM和RPKM的计算式，就会发现它们根本做不到。 举个例子来说明（以FPKM的计算为例），我们假定有两个来自同一个个体不同组织的样本X和Y，这个个体只有5个基因，分别为A、B、C、D和E，它们的长度分别如下： 由此，我们可以得到，样本X和Y的转录本的不变量，$r{mean}$值都是$r{mean} = \frac{1}{5} = 0.2$。如果FPKM或RPKM是一个合适的统计量的话，那么至少，样本X和Y的平均FPKM（或RPKM）值应该相等。 我们以FPKM的计算的为例子，以下这个表格列出的分别是样本X和Y在这5个基因中比对上的fregment数和各自总的fregment数量： 于是，按照以上公式我们可以得到样本X和Y在这5个基因上的FPKM值分别为： 接下来就可以计算FPKM的均值了。我们得到，样本X在这5个基因上的FPKM均值$FPKM{mean} = 5,680$；而样本Y的FPKM均值却是$FPKM{mean} = 161,840$!! 它们根本不同，而且差距相当大，那么究竟为什么会有如此之大的差异？难道这是我故意构造出来的例子所造成的吗？当然不是，这是由其数学计算上的缺陷所导致的。 首先，我们可以把FPKM的计算式拆分成两个部分：（1）等价（其实严格来讲也没那么等价）描述某个基因转录本数量的统计量（$\frac{n_f}{L}$） 和（2）测序获得的总有效Fregment数量的百万分之一（$\frac{N}{10 ^ 6}$）；看，FPKM便是这两部分的商！分开来看它们貌似都有点道理，但是合起来的时候其实很没逻辑，尤其是第二部分$\frac{N}{10 ^ 6}$，本来式子的第一部分 （$\frac{n_f}{L}$）就已经是描述某个基因的转录本数量，那么正常来讲，第二部分就应该是描述样本总体的转录本数量（或至少是其等价描述）才能说得通，而且可以看得出FPKM(RPMK)是有此意的，因为这本身就是这一统计量的目的。然并卵，它失败了！$\frac{N}{10 ^ 6}$的大小其实是由RNA-seq的测序深度所决定的，并且是一个和总转录本数量无直接线性关系的统计量——N与总转录本数量之间的关系还受转录本的长度分布所决定，而这个分布往往在不同样本中是有差异的！比如，有些基因，虽然有效比对到它们身上的Fregment数目是相等的，但很明显，长度越长的基因，其被转录的次数就越少。也就是说，N必须将各个被转录的基因的长度考虑进去才能正确描述总体的转录本数！而FPKM（RPKM）显然没有做到这一点，这便是FPKM（RPKM）出错的内在原因。 那么应该是用什么样统计量才合适其实，通过以上分析，我们已经可以确定一个更加合理的统计量来描述RNA转录本的丰度了。我意外地发现，这个统计量其实在2012年所发表的一篇关于讨论RPKM的文章（RPKM measure is inconsistent among samples. Wagner GP, Kin K, Lynch VJ. Theory Biosci. 2012.）中就已被提到过了，它被称之为TPM —— Transcripts Per Million，它的计算是： $$TPM = \frac{\frac{n_r \times read_l}{ g_l} \times {10}^{6}} {T} = \frac{n_r \times read_l \times {10} ^ {6} } {g_l \times T}$$ $$T =\sum_{g=i}^{G}{ (\frac{n_r \times read_l}{g_l})_i }$$ 其中，$read_l$是比对至基因G的平均read长度，$g_l$ 是基因G的外显子长度之和（这里无需将其除以1000了）。在不考虑比对剪切的情况下，$read_l$这个值往往都是一个固定值（如100bp或者150bp等），因此我们也可以将$read_l$统一约掉，那么分子就会蜕变成RPKM计算式的第一部分，但把$read_l$留着会更合理。这样，整个统计量就很好理解了，分子是基因G的转录本数（等价描述），分母则为样本中总转录本的数量，两者的比值TPM——便是正确描述基因G的转录本丰度！并且，简单计算我们就可以知道TPM的均值是一个独立于样本之外的恒定值： $$TPM_{mean} = \frac{10^6}{N}$$ 这个值也刚好是$r_{mean}$的$10^6$倍，满足上述等价描述的关系。我们仍然通过上面的例子来进作说明，为简单起见我们只把fregment换为read，其他数字都一样，并且统一假设$read_l$都是一样的： 接着，我们可以分别计算样本X和Y的TPM_mean,并且很明显它们都是$200000 = 10^6 / 5$. 而且，经过这样的标准化之后，X和Y就处于同样的一个标准上了，此刻，彼此之间的比较分析才是真正有意义的。 既然FPKM/RPKM是错的，那为什么大家直到现在都还在用，而且还真找到了（能被实验所验证）有价值的结果呢？关于对于这个问题，我也思考过。而且我们都知道2008那篇关于RPKM的文章更是用实验结果证明了，RPKM是一个合适的统计量，符合qPCR的验证结果。但归根到底，我觉得眼见未必为实，很多实验其实是表象的，我们更应该从其本质意义和原理上去考虑。FPKM/RPKM之所以看起来会是一个合适的值，我想主要原因有二： 其一，它们和TPM之间存在一定的正比关系。这可通过它们各自的数学计算方程式看出来（以RPKM的计算为例）： $$RPKM = \frac{T \times 10^3}{ N \times read_l } \times TPM$$ 而且在同一个样本内部由于T，N和$read_l$实际上都是定值，因此同个样本内的RPKM和TPM是可以恒等转换的。然而在样本与样本之间就不行，因为不同样本T和N是不同的（假定测序长度$read_l$都一样），这就导致它们之间的转换因子大小不一样！ 如以上例子，对于样本X，TPM转换到RPKM的转换因子为：0.0284，但在样本Y中，它的转换因子却是：0.8092。而由于这个基础标准的改变，导致其原本所要描述的“转录本丰度”变得不可比较。然，这其实不是最根本的原因，更本质的原因是，这个转换会对本来已经正确标准化了的结果——TPM，再次做了一次无意义的不等变换，最终导致了结果不可解释。如何理解呢，后文会有补充，这里先简单说一下：这个数学转换式子仅是告诉了我们这样子来计算是可行的，但是在RNA-seq的实际应用场景中，它其实是无生物（或物理）意义的； 其二，实验验证的精度是有限的，常用的qPCR也只能给出定性的比较结果，而且实验验证也未必总能成功。 总结现在回过头来总结一下。事实上，FPKM/RPKM最大的问题就在于其无意义性。我们所要表达出来的任何统计量，它的变化都应该要能对应到物理或生物过程中的变化，如果做不到这一点，那么这个统计量往往都是无意义的，用它得到的结果就算看起来符合预期也只不过是数值上的巧合，本质上是不可解释的。FPKM/RPKM的分母($N/10^6$)并不具有任何形式的生物意义，它所能表达出来的这个量，只能代表测序深度的变化，而无法作为表达生物过程的量，比如无法代表（等价代表）样本中转录本的总量。 一个统计量该如何计算，说到底都只是一个“术”的问题，而我们应该尽可能在接近其本质意义的地方去确定。 FPKM/RPKM和TPM存在一定的正比关系，因此我们在使用FPKM/RPK时，有些时候确实也能获得可以被实验所验证的“好”结果，但其实它是一个橡皮筋，它的单位刻度是会随着样本的不同而改变的。到头来，样本之间的差异比较实际上也只是在不同的标准下进行的，这样的比较就算得到了所谓的“好”结果，那又有什么意义，根本就是个错误的东东。想想就是由于这种统计量，我们一定已经获得了许多的假阳性结果，同时也肯定错过了许多本来真正有意义的差异，真是弯路走尽也不知，而且还浪费了大堆的心情和时间。 这篇文章：A comprehensive evaluation of normalization methods for Illumina high-throughput RNA sequencing data analysis. Briefings in Bioinformatics.10.1093/bib/bbs046. 对7种主要的RNA-seq标准化方法（但不包含本文提到的TPM）做了一个详细的比较，它用实际结果进行比较（不同于本文所用的数学方式）也得出了RPKM/FPKM这些统计量应该被摒弃的结论，因为它所描述出来的结果是最不合理的，其实所有类似于RPKM/FPKM的统计量在描述转录本丰度的时候都应该被摒弃。 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>RNA</tag>
        <tag>RPKM</tag>
        <tag>FPKM</tag>
        <tag>TPM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目前最好最完整的SOAPdenovo使用说明]]></title>
    <url>%2F2015%2F07%2F09%2FThe-best-manual-for-soapdenovo2.html</url>
    <content type="text"><![CDATA[由于丹麦人国家基因组项目的原因，近期我整理了一份关于SOAPdenovo2的使用说明，内容包括了程序使用、参数的详细说明、参数如何调整、各个主要输出文件的格式说明等，而且我敢说这是目前最好最全的！ 简介SOAPdenovo（目前最新版是SOAPdenovo2）是一种应用de Bruijn graph组装短read的方法，它以kerm为节点单位，利用de Bruijn图的方法实现全基因组的组装，与其他短序列组装软件相比，它可以进行大型基因组，比如人类基因组的组装，组装结果更加准确可靠，可以通过组装的结果非常准确地鉴别出基因组上的序列结构性变异，为构建全基因组参考序列和以低测序成本对未知基因组实施精确分析创造了可能。 下载地址：http://soap.genomics.org.cn/soapdenovo.html 安装： 下载SOAPdenovo的压缩包 解压缩 将得到可执行文件SOAPdenovo和一个配置文件的模板example.contig 使用程序及参数SOAPdenovo可以一步跑完，也可以分成四步单独跑，一步跑完的脚本: 1./SOAPdenovo all -s lib.cfg -K 29 -D 1 -o ant &gt;&gt;ass.log 四步单独跑的脚本:1234./SOAPdenovo pregraph -s lib.cfg -d 1 -K 29 -o ant &gt;pregraph.log./SOAPdenovo contig -g ant -D 1 -M 3 &gt;contig.log./SOAPdenovo map -s lib23.cfg -g ant &gt;map.log./SOAPdenovo scaff -g ant -F &gt;scaff.log 参数说明1用法：/PathToProgram/SOAPdenovo all -s configFile [-K kmer -d KmerFreqCutOff -D EdgeCovCutoff -M mergeLevel -R -u -G gapLenDiff -L minContigLen -p n_cpu] -o Output 12345678910111213-s STR 配置文件-o STR 输出文件的文件名前缀-g STR 输入文件的文件名前缀-K INT 输入的K-mer值大小，默认值23，取值范围 13-63-p INT 程序运行时设定的线程数，默认值8-R 利用read鉴别短的重复序列，默认值不进行此操作-d INT 去除频数不大于该值的k-mer，默认值为0-D INT 去除频数不大于该值的由k-mer连接的边，默认值为1，即该边上每个点的频数都小于等于1时才去除-M INT 连接contig时合并相似序列的等级，默认值为1，最大值3。-F 利用read对scaffold中的gap进行填补，默认不执行-u 构建scaffold前不屏蔽高覆盖度的contig，这里高频率覆盖度指平均contig覆盖深度的2倍。默认屏蔽-G INT 估计gap的大小和实际补gap的大小的差异，默认值为50bp。-L 用于构建scaffold的contig的最短长度，默认为：Kmer参数值 ×2 使用方法及示例（1）示例1SOAPdenovo all -s HCB.lib -K 25 -d -o test （2） 输入文件configFile，配置文件内容如下，非程序生成，需要软件使用者自己配置。各个说明参考如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# 以“#”开头的行是注释内容# maximal read length （read的最大长度）# 该值一般设置的比实际read读长稍微短一些，截去测序最后的部分，具体长度看测序质量max_rd_len=50 [LIB] # 文库信息以此开头# 文库平均插入长度，一般取插入片段分布图中给出的文库大小avg_ins=200#序列是否需要被反转，目前的测序技术，插入片段大于等于2k的采用了环化，所以对于插入长度大于等于2k文库，序列需要反转，reverse_seq＝1，小片段设为0reverse_seq=0# 该文库中的read序列在组装的哪些过程（contig/scaff/fill）中用到# 设为1：只用于构建contig；# 设为2：只用于构建scaffold；# 设为3：同时用于构建contig和scaffold；# 设为4：只用于补洞# [注意]短插入片段(&lt;2K)的设为3，同时用于构建contig和scaffold，长插入片段(&gt;=2k)设为2，不用于构建contig，只用于构建scaffold，454single 长reads只用于补洞。asm_flags=3# rank该值取整数，决定了reads用于构建scaffold的次序，值越低，数据越优先用于构建scaffold。# 设置了同样rank的文库数据会同时用于组装scaffold。# 一般将短插入片段设为1；2k设为2；5k设为3；10k设为4；# 当某个档的数据量较大时，也可以将其分为多个档，同样，当某档数据量不足够时，可以将多个档的数据合在一起构建scaffold。# 这里说的数据量够与不够是从该档的测序覆盖度和物理覆盖度两个方面来考虑的。rank=1# 可选参数，pair_num_cutoff该参数规定了连接两个contig 或者是pre-scaffold 的可信连接的阈值，即，当连接数大于该值，连接才算有效。短插入片段(&lt;2k)默认值为3，长插入长度序列默认值为5pair_num_cutoff=3# map_len该参数规定了在map过程中 reads和contig的比对长度必须达到该值（比对不容mismacth和gap），该比对才能作为一个可信的比对。可选参数，短插入片段(&lt;2k)一般设置为32，长插入片段设置为35，默认值是K＋2.map_len=32# read 1的fastq格式的序列文件，“/path/**LIBNAMEA**/fastq_read_1.fq”为read的存储路径q1=/path/**LIBNAMEA**/fastq_read_1.fq# read 2的fastq格式的序列文件，与read1对应的read2文件紧接在read1之后）q2=/path/**LIBNAMEA**/fastq_read_2.fq#read 1的fasta格式的序列文件f1=/path/**LIBNAMEA**/fasta_read_1.fa# read 2的fasta格式的序列文件f2=/path/**LIBNAMEA**/fasta_read_2.fa# 单向测序得到的fastq格式的序列文件q=/path/**LIBNAMEA**/fastq_read_single.fq# 单向测序得到的fasta格式的序列文件f=/path/**LIBNAMEA**/fasta_read_single.fa# 双向测序得到的一个fasta格式的序列文件p=/path/**LIBNAMEA**/pairs_in_one_file.fa 输出文件及说明SOAPdenovo 分四部分别对应的输出文件：12341. pregraph 生成7个文件 *.kmerFreq *.edge *.preArc *.markOnEdge *.path *.vertex *.preGraphBasic2. contig 生成4个文件 *.contig *.ContigIndex *.updated.edge *.Arc3. map 生成3个文件 *.readOnContig *.peGrads *.readInGap4. scaff 生成6个文件 *.newContigIndex *.links *.scaf *.scaf_gap *.scafSeq *.gapSeq *.contig：contig序列文件，fasta格式； *.scafSeq：fasta格式的scaffold序列文件，contig之间的gap用N填充； 对于得到的*.scafSeq文件还需要用GapCloser去合并其中的gap，最后的contig文件则是对补洞之后的scaffold文件通过打断N区的方法得到。 以上两个文件是组装结果中最主要的输出。 *.scaf：包括scaffold中contig的详细信息；在scaffold行中包括scaffold名字、contig长度和该scaffold长度。在contig行包括contig名字、contig在scaffold上的起始位置、正反链、长度和contig间的链接信息; *.links：contig间的pair-end连接信息; *.readOnContig：reads在contig上的位置; *.peGrads： 主要可以通过调整本文件中的参数来显示构建scaffold所用到的插入片段库的个数，总共要到的read数，最长的read的长度，每个库对应的哪些reads，rank设置，pair_num_cutoff设置。例如： 1234567891011grads&amp;num: 10 522083934 70323 104577616 1 3334 180770522 1 3345 226070520 1 3486 361955834 2 32200 392088076 3 52290 422272580 3 52400 445522690 3 54870 475666064 4 59000 511030930 5 89110 522083934 5 5 该文件中共分成4列。组装的配置文件中有n个文库，该文件则有n+1行，且按照文库大小顺序排列。第1行中，第二三四列分别是 所用文库，reads总数和组装中用到的最长的reads长度。第2行中，四列分别是文库大小，文库中的reads数目，该文库reads用到的rank等级和该文库中reads用到的pair_num_cutoff。第3～n+1行，四列分别是文库大小，文库中的reads数目加上前面的文库中的reads总数，该文库reads用到的rank等级和该文库中reads用到的pair_num_cutoff。如果配置文件中没有设置pair_num_cutoff，即使用默认参数，则最后一列显示为0。 对于SOAPdenovo的每个步骤都有日志文件输出，要保存好日志文件，日志文件中包含有很多有用的信息。 SOAPdenovo日志输出说明1）pregraph.log: 其中有很多的统计信息，包括构建debruijn-graph时用到多少reads数，构图中生成了多少uniq的kmer以及设置-d参数后去除了多少kmer。在pregraph中，可选参数有 –R –K –d 结果如： 125467781332 nodes allocated, 70662750348 kmer in reads, 70662750348 kmer processed3283081670 kmer removed 其中Kmer 数是取决于所设k值大小以及数据量，nodes数即特异性的kmer数目，当nodes数目过高（一般和基因组大小差不多大小），可能是数据的错误率比较高，也可能是存在杂合。若nodes数目偏小，并且kmer数目很多，则基因组本身可能存在一定的重复度。对于k值的选取，当数据量充足时（&gt;=40X），植物基因组一般采用大kmer会有比较好的效果，而对于动物基因组，k值一般多取27和29则足够。kmer removed表示的 –d 参数所去除的低频的kmer。 2）contig.log: contig 中，可选参数 –R –D –M，注，-R 参数的选定，必须pregraph和contig中同时选择才有效。结果例子： 116430183 pairs found, 2334584 pairs of paths compared, 1674493 pairs merged 从merged的数量可以作为估计杂合以及测序错误的程度。 12sum up 1932549703bp, with average length 1170the longest is 36165bp, contig N50 is 2871 bp,contig N90 is 553 bp 3）map.log:12Output 415219610 out of 1956217742 (21.2)% reads in gaps1661094582 out of 1956217742 (84.9)% reads mapped to contigs 一般情况下，reads in gap的比例和map to contig 的比例总和大于1。可能是因为reads map到多个地方都被算在其中的原因。当map to contig的比例很高（80%左右时），但是组装效果并不很好，可能是重复序列比较多。reads in gap比例较高（大于40%），是因为基因组组装的较碎，gap区域较多。map_len 默认值=K+5，当默认值大于设置的map_len时，以默认值为准，当默认值小于map_len值时，设置的map_len为准。 4）scaff.log:1average contig coverage is 23, 5832270 contig masked 构建scaffold是对高频覆盖的contig进行屏蔽（即频率高于average contig coverage的两倍的contig不用于构建scaffold），从这里可以看出组装的基因组一定的重复情况。 12estimated PE size 162, by 40034765 pairson contigs longer than 173, 38257479 pairs found,SD=8, insert_size estimated: 163 173 是配置文件中该文库的insertsize，163 是根据reads map到contig上的距离的估计值，8是这个分布的标准偏差。一般考虑 比对上去的pair数目和SD值。若pair对数很多且SD值很小（小片段文库数据不超过三位数，大片段文库数据部超过500），那我们一般可以将配置文件中的文库插入片段的值改对短插入片段文库（=2K），因为是把reads map到contig上，若最长contig较短时，可能找不到成pair比对上去的reads，这时，无法估计文库大小，需要自己将大片段一级一级的map到前一级的组装结果上，然后再分析大片段文库的插入片段大小。注，需要调整insertsize信息时，只需要修改 .peGrads文件中的第一列，然后删除.links文件，重新跑scaff这一步即可。即构建scaffold时，主要是根据*.links文件的信息进行连接。 12Cutoff for number of pairs to make a reliable connection: 31124104 weak connects removed (there were 4773564 active cnnects)) Cutoff for number是在配置文件中设的pair_num_cutoff值，weak connects是低于这个值被认定为无效的连接数，active connects是满足cutoff的连接数，根据这个数值可对pair_num_cutoff做调整 1Picked 25241 subgraphs,4 have conflicting connections conflicting connections 是表示构建scaffold时的矛盾数，矛盾数比较高（&gt;100）时，可根据前面的有效连接数，适当提高pair_num_cutoff值，即提高scaffold连接要求的最少关系数 12182483 scaffolds&amp;singleton sum up 1990259817bp, with average length 10906the longest is 6561520bp,scaffold N50 is 836795 bp, scaffold N90 is 157667 bp scaffold 统计信息，将是根据rank分梯度的统计:1Done with 13301 scaffolds, 2161915 gaps finished, 2527441 gaps overall -F 参数补洞的统计信息。 参数调整一般组装时需要调整的参数，主要分两种： 一种是针对脚本中的参数改动：如调整 -K -R -d -D -M-K 值一般与基因组的特性和数据量相关，目前用到的SOAPdenovo软件主要有两个版本，grape1123和grape63mer，其中grape1123是最新版的组装软件，K值范围13-31，grape63mer是可以使用大kmer的组装版本，K值范围13-63。 【经验】：植物基因组的组装采用大kmer效果会比较好（要求短片段reads长度75bp），动物基因组很少有用到大kmer后有明显改进效果的，且动物基因组的组装K值一般设置为27和29较多。 -R参数，对于动物基因组，R参数一般不设置，植物基因组由于较多的repeat区，则设置R参数后，效果更好。注意，设置-R时，一般使用-M 的默认值。（熊猫基因组组装时得出的结论） -M 参数，0-3,默认值1。一般杂合率为千分之几就设为几。熊猫基因组组装时-M 2 。 -d 参数，对于没有纠错，没有处理的质量又较差的原始数据，kmer的频数为1的很多的数据的组装，一般设置为-d 1 则足够。对于处理过，或者是测序质量较好的数据，可以不用设置。数据量很多时，也可以以-d 参数去除部分质量稍差的数据。 -D 参数，默认为1，一般不用另行设置。 第二种，从map这一过程去调节参数。可以调整配置文件的map_len的值和调整文件*.peGrads。 当文库插入片段分布图中文库大小与实验给出的文库大小差异很大时，调整*.peGrads文件中的插入片段大小。 根据每一档数据的数据量去调整文库的rank等级。当该文库的数据量很多或者是在构建scaffold的过程中的冲突数很多时，可是适当的调大第四列 的pair_num_cutoff，把条件设置的更严一些。 内存估计SOAPdenovo的四个步骤消耗的内存是不一样的，其中第一步消耗的内存最多，使用没有纠错的的reads，(K&lt;=31)第一步消耗的内存在基因组大小的80－100倍左右，纠过错则在40－50倍左右，第二步相对消耗的内存会少很多，第三步消耗的内存是仅次于第一步的，在第一步的一半左右，第四步消耗的内存也会比较少。对于CPU的使用，默认是8个，如果申请内存时申请一个计算节点的所有内存测将CPU就设置为该计算节点的CPU个数充分利用计算资源，如果仅申请一个节点的部分内存则根据实际情况考虑。对于大kemr(K&gt;31)其内存使用是(k&lt;=31)的1.5倍左右，有时甚至更多，要充分估计内存的使用，在第一次运行的时候考虑不能太保守。 常见错误1）配置文件中read存储路径错误 只输出日志文件。pregraph.log中的错误信息：“Cannot open /path/LIBNAMEA/fastq_read_1.fq. Now exit to system…” 2）-g 后所跟参数与pregraph（第一步） -o 后所跟参数名不一致 contig map scaff 这三个步骤都只是输出日志文件。 contig.log中的错误信息：“Cannot open *.preGraphBasic. Now exit to system…” map.log中的错误信息：“Cannot open *.contig. Now exit to system…” scaff.log中的错误信息：“Cannot open *.preGraphBasic. Now exit to system…” 3）从map开始重新跑时，需要删除*.links文件，否则会生成core文件，程序退出。 【注】仅同时发布于泛基因fungenomics]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>组装</tag>
        <tag>SOAPdenovo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用de Bruijn graph组装基因组的时候，Kmer为什么必须是奇数？]]></title>
    <url>%2F2015%2F05%2F22%2Fwhy-Kmer-count-should-be-odd-number-in-assembly-algorithm.html</url>
    <content type="text"><![CDATA[根本原因就是为了避免导致 正反链混淆。 一开始，我并没弄明白，后来仔细想想也终于懂了。 如果kmer是偶数，我们会发现基因组上有些序列（如，CGCGCGCG，kmer=4）的Kmer在反向互补后得到的序列仍然是它自身！这是不能允许发生的。因为这将导致你无法区分某段序列的kmer到底是属于它自身还是说只是来自于它的互补链！！这会给解de Bruijn graph带来极大的混淆和困难！ 或许你会觉得 “为什么我需要纠结于序列是不是来自互补链呢？毕竟双链DNA的正反链是严格反向互补的啊，基因组组装技术不也是把它们合并装在一起的吗？！”。你若是这样来理解其实是非常难得的，但前提却是基因组必须能够被一次性完整地（至少是非常接近完整）测出来，这时的测序深度甚至只需是1就可以了。但是你回头想想，既然都已经把基因组完整测序出来了，那还要组装干嘛呢？ 并且，目前的NGS测序技术也做不到通测基因组。一般来说都是测出上百万千万亿万个小小的片段（read，长度一般是100bp-300bp）。而且，为了确保准确性，基因组都会被反复测很多层。组装时构建的kmer单位，实际上是对这些read进行的。具体的操作就是按照kmer的长度把这些read切割成更小的、存在重叠关系的片段。那么，此刻当我们构建de Bruijn graph时，如何能够保证正确地把 同属于一条read上的Kmer连接起来，就显得极为重要了！我们不能一会儿把A kmer正确地连到它自己所在的read，一会儿又连到它互补链的read上去！ 这就是为何Kmer不能是偶数的原因了，因为只有奇数，才能保证每个kmer序列的反向互补kmer与自身也是不同的，而这个不同的真正意义就是为了避免正反链混淆。比如 ：5-mer的 CGCGC，反向互补后是 GCGCG， 它们是不同的；这就不会像 4-mer，CGCG发现它反向互补后仍然是CGCG，这个时候就就会在后续连接kmer的过程中发生正负链混淆，装出一个嵌合体基因组！ 最后，放一张发表在Genome Research有关组装的图，大家可以大致感受一下这一段重复序列的组装过程。 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>组装</tag>
        <tag>de Bruijn graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Python中调用C++模块]]></title>
    <url>%2F2015%2F04%2F03%2F2015-04-03-How-Does-Python-call-Cpp-module.html</url>
    <content type="text"><![CDATA[在Python中成功实现了对原来C++代码模块的复用！这个好处多多，Python写得快，C++跑得快，那就是既快又快了！方法很简单，以至于我能够用一张截图记录下整个过程（点击图片看大图）！ 其实，注意到，必须在原来的C++代码后面添加extern “C”来辅助（C则不需要，这也是与复用C代码时最大的不同点），不然Python在调用这个构建后的动态链接库时是找不到原来的方法或者函数的，说到底还都是因为当前Python的设定中只能调用C函数，而不能直接调用C++的方法，因此extern “C”封装的函数必须是C风格的，也即就是说，函数中的所有参数（输入参数和返回参数）都不能包含任何非C定义的类型（包括不能包含只属于C++的类型）。]]></content>
      <categories>
        <category>信息图</category>
        <category>编程技术</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[何不争当数据科学家？]]></title>
    <url>%2F2015%2F03%2F22%2F2015-03-22-how-to-become-a-data-scientist.html</url>
    <content type="text"><![CDATA[当今世上大数据横行，于是duang许多人都想成为数据科学家。 分享两张路线图： （1）成为数据科学家的8个简单步骤！ （2）数据科学家之路 OK，步骤就是这样，图样说简单，过程其实不简单，但却也都是可操作的！这种东西看多了，只觉得什么都是虚的，这些也都是术，关键还是在人，既然有抱负，地图就在那了，接下来，就看你是否有耐心，是否愿意花时间走下去。奋斗吧，Fighting！ http://blog.datacamp.com/how-to-become-a-data-scientist]]></content>
      <categories>
        <category>信息图</category>
      </categories>
      <tags>
        <tag>数据科学家</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二代测序：碱基平衡性与barcode选择]]></title>
    <url>%2F2015%2F02%2F01%2F2015-02-01-base-balance-and-barcode-selection.html</url>
    <content type="text"><![CDATA[这是转载过来的一篇文章，虽然基础，但却觉得是很重要的知识,所以便记录了下来，原始出处来自微信公众号“基因测序资讯AGCT”。 碱基平衡性 碱基复杂度与碱基多样性是一个意思；复杂度高，碱基即平衡。低多样性(low diversity)即碱基不平衡，指碱基的组成太单纯了，种类少。碱基复杂度本来无关紧要，从前除了设计PCR的时候考虑高GC(GC-rich)以外，基本没人思考这个问题，没人觉得这是一个问题。随着Illumina的二代测序技术风靡全球，独占鳌头，这个不起眼的概念意外地变得重要起来。 一、概念 对于一个基因来说，它所包含的碱基种类越多，则碱基复杂度越高；如果各种碱基的百分含量越接近一致，则碱基组成越平衡。 假设一个DNA片段，它的全部碱基都是A，AAAAAAAAAAAAAAAAAAAAAAAA，显然其碱基组成是极度不平衡的。 DNA碱基有4种：AGCT。所以碱基最平衡的情况就是：%A=%G=%C=%T=25%，比如这样的DNA片段：AGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCT。 以上是从纵的方面讲的。对于二代测序，更重要的是横的方面。假设12个基因整整齐齐站成一排，第一个位置的12个碱基如果都是A，复杂度太低，严重不平衡；如果A和G各有6个，虽然平衡了，但是复杂度还是不够；如果AGCT各有3个，最复杂，也最平衡；如果A3个G4个C4个，T1个，虽然复杂，但是严重不平衡。 二、影响 4张滤色片，在4个波长处收集信号，然后合成，进行cluster定位及其他运算。如果缺少一种碱基，该波长的照片就是全黑的，没有信号，无法完成图片合并以及cluster定位，导致数据浪费。 需要特别注意碱基复杂度的二代测序应用:PCR产物测序，特别是用于鉴定细菌、真菌以及其他物种的16S rRNAPCR产物测序；小RNA测序；甲基化测序。 三、增加碱基复杂度的方法 文库： 把不同的文库混合在一起。 如果没有其他文库，那么掺入人基因组DNA文库、人外显子组文库或者PhiX标准品。这些都是已知碱基平衡的。 引物： 对于PCR产物来说，只要引物长度不同，就能自然错开，增加碱基复杂度。 采用多对序列不同的引物来完成扩增，然后将产物混合在一起。 Barcodes： 仔细挑选barcode组合，确保每个位置都有3-4种碱基且碱基分布均匀。 ##barcode 选择很多情况下，我们需要把多个样本混合在一起，在同一个通道(lane)里完成测序。像转录组测序、miRNA测序、lncRNA测序、ChIP测序等等，通常每个样本所需要的数据量都比较少，远少于HiSeq一个通道的产出能力，混合样本是非常常见。以转录组测序为例，一个样本测序60 M reads (8G PF data) ，就能够满足绝大部分研究所需。而HiSeq2500-PE125的一条通道，使用V4试剂，数据产出&gt;480 M reads。为了充分利用测序仪产能，节约成本，需要把7~8个RNA样本混合起来。 为了能够把测序数据按样本分离（de-multiplexing），在构建文库(library)的时候，需要用不同的标签序列(index, 也叫barcode)对文库进行标记。只有文库作了记号，数据才能区分。 Barcode的选择是一门技术活。如果barcode组合不佳，标签序列测序质量下降，部分或者全部标签碱基识别不正确，将导致部分数据无法归属到任何一个样本，成为undetermined数据，造成浪费。 一、如何判断barcode组合好坏 碱基平衡。好的barcode组合必须是“4种碱基达到平衡”的，或者说碱基复杂度高。具体就是：a. 在一组barcode的每一个位置，同时存在A、G、C、T四种碱基，不缺少任何一种碱基；b. 这4种碱基的比例接近，最好各1/4，分别为25%左右，没有任何一种碱基特别多或者特别少。 激光平衡。 受客观条件限制 a.试剂盒提供的barcode种类有限b.有些barcode已经被其他样本占用，导致可选的余地受限制，这就导致barcode组合经常无法达到理想的碱基平衡要求。退而求其次，要力保“红绿激光达到平衡”。 在所有型号的Illumina测序仪中，A和C两种碱基共用一种激光，由波长660 nm的红激光激发；G和T共用一种激光，由波长532 nm的绿激光激发。对于一组barcode的每一个位置，如果A＋C的总数与G＋T的总数相接近，可以在一定程度上弥补碱基不平衡的负面作用。 3、激光平衡是次优选择，不得已而为之。它虽然可以在一定程度上提高barcode测序质量，减少de-multiplexing出问题的可能性，但是并不是说，只要激光平衡了，测序数据的分离就一定不受影响。4、如果barcode组合碱基也不平衡，激光也不平衡，则de-multiplexing风险非常高。 二、Barcode组合举例 好的组合。 Illumina推荐的12个样本barcode组合如下。 编号 序列 01 ATC ACG 02 CGA TGT 03 TTA GGC 04 TGA CCA 05 ACA GTG 06 GCC AAT 07 CAG ATC 08 ACT TGA 09 GAT CAG 10 TAG CTT 11 GGC TAC 12 CTT GTA 以第一个位置（纵列）为例，A:G:C:T=3:3:3:3=1:1:1:1。实际上，该barcode组合每个位置的碱基比例都接近1:1，碱基平衡度近乎完美。 不好的组合 下面的组合有缺陷。比如说，第1个位置只有A和C两种碱基，A、C都属于红激光，导致绿激光没有信号，碱基和激光都不平衡。 AGT TCCACT GATACG AGCACT CCTCAA AAGCAA CCACAC CAG 三、Barcode碱基不平衡的后果 如果barcode组合的碱基组成不平衡，会导致测序进行到这些碱基时，软件对测序信号的处理出现障碍，不能准确地识别这些碱基(base-calling)，表现为QV值降低，%Q30曲线波动。 在这种情况下，运用生物信息软件对测序数据进行数据分离（de-multiplexing）出现困难，部分数据不能准确分离，成为undetermined 数据的一部分，造成undetermined数据增多，可分离的数据减少。 如果测序数据的总量很多，远大于全部样本数据量期望值的总和，则问题有可能不那么严重，全部或者大部分样本仍然可能分离到足够的数据量。 万一样本性质特殊，反应效率低；或者混合样本之间竞争和抑制严重，导致测序数据总量在期望值附近，余量很少；或者其中个别样本数据量特别少，这时如果undetermined数据比例过高，就会导致部分或者全部样本的数据量不够用。 混合样本补数据是一个非常麻烦的问题，成本极高。如果一组样本中只有个别样本需要补数据，由于文库是混合在一起的，其他样本也不得不跟着重测一次。这是困难之一。困难之二，如果数据缺口比较小，本来可以与其他样本混合，搭个便车，可是，进行第二次混合的时候，经常会遇到barcode冲突或者碱基不平衡，拼lane非常困难，往往要等很长时间，才有合适的机会。 四、实验证明de-multiplexing成功，该barcode组合今后是否一定好用 如果barcode组合碱基平衡，则无论样本怎么变，该组合一定好用。 如果barcode组合的碱基组成不理想，即使以前的实验证明好用，不等于今后一定好用。下一次测序效果可能好，也可能不好。 这是由于不同的项目样本不同，有可能导致两种后果： a.数据总量在期望值附近，余地不够多，de-multiplexing后部分样本数据量不够；b.如果新的样本本身也碱基不平衡，read 1测序质量很差，会影响到barcode和read2的测序质量。当然，情况b责任不在barcode，即使barcode很好，数据还是不够。 五、补救措施 如果满足以下两个条件： a. 混合样本的数据总量足够，只是由于barcode质量不好，导致de-multiplexing后部分或全部样本数据量不够；b. 排除QV值低的barcode碱基后，其余质量好的barcode碱基仍然足够用来区分全部样本； 那么，可以通过改变de-multiplexing算法来为每个样本获得尽量多的数据。比如去掉信号识别模糊的碱基，或者增加mismatch碱基的数目，重新运行de-multiplexing程序。 六、样本少于4种，不可能碱基平衡，怎么办 如果样本数少于4种，每一个位置的碱基最多只有3种，不可能碱基平衡，怎么办呢？这时一定要保证激光平衡。Illumina推荐了3种low-level pooling的barcode组合： 2个样本： :—:|:—-: #6|GCC AAT #12|CTT GTA 3个样本： :—-:|:—-: #4|TGACCA #6|GCCAAT #12|CTTGTA 6个样本： :—:|:—: #2|CGATGT #4|TGACCA #5|ACAGTG #6|GCCAAT #7|CAGATC #12|CTTGTA 这3种组合包含一个共同内核：6号和12号。6号和12号组合是百分百激光平衡的，每一个位置的碱基（纵列，即GC、CT、CT、AG、AT和TA）都分别属于不同的激光。只要barcode组合中包含6号和12号，就能满足最基本的要求，不至于颗粒无收。6号和12号是barcode组合的核心，不可或缺。]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>测序技术</tag>
        <tag>碱基平衡性</tag>
        <tag>barcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何选择入门编程语言]]></title>
    <url>%2F2015%2F01%2F22%2F2015-01-22-which-programming-language-should-I-learn-first.html</url>
    <content type="text"><![CDATA[这是一个很有意思的编程语言入门学习图谱，以不同人的目的为导向推荐不同程度的入门语言，还有一个最有意思的地方就是为每一个主流编程语言配上一个魔戒人物用以形象描述它的独特之处,非常有特点！点击图片看大图。]]></content>
      <categories>
        <category>编程技术</category>
        <category>信息图</category>
      </categories>
      <tags>
        <tag>入门学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何布局并管理好项目目录（2/2）]]></title>
    <url>%2F2015%2F01%2F14%2F2015-01-14-how-to-manage-the-project-data-2.html</url>
    <content type="text"><![CDATA[…书接上回… 不同类型的项目会有适合其特点的目录布局，很难说会有一个真正的标准，我这里特指的是基因组（或相关的）项目目录布局问题。 基因组项目目录结构的科学管理是一个非常重要的问题。如果不科学，东西一多更是成了灾难，很多时候你想找的东西并不是丢了，而是根本找不到了！有时更糟的是就算找到了，也会忘了是怎么来的，最后连自己都会怀疑它是不是真的就是正确的那一份！只得一轮又一轮地冥思苦想寻找着若隐若现的线索，甚至有时即使打算重做都不知道该怎么下手，效率真是极低，简直就是在浪费生命！这样下去根本没得玩，直到昨天我才意识到实际上要用程序设计的思想来管理每个项目的目录！还是A workflow for R这篇博文给我的灵感，它提出了一些规则： 透明（Transparency）：目录布局清晰明朗，逻辑清楚，整个结构一看就能明白。 易维护（Maintainability）：容易实行项目的修改和相关（如文件名、目录变换）调整。 模块化（Modularity）：任务之间都应该尽可能保持其独立性，每一个就只干好自己的事，不干涉其他任务，避免牵一发而动全身的情况发生，要有一一对应的关系，这样即便是需要修改也将会非常方便。 可移植（Portability）：项目可以很容易地移植到其他的平台或者系统中。 易复现（Reproducibility）：不论经过多久，都要能轻易重现原来的结果。 秒懂 (Efficiency)：无需多想就能明白项目执行过程中的相关细节，比如所要解决的每个问题是如何处理的和所用的工具是什么等等。 这几点完全说出了项目布局和目录管理所应达到的目的和状态，值得铭记于心多拿来参考参考。只是仅有理论，还是太过不着边际了点，有必要弄个例子来看看具体应该怎么做。 有那么一段时间以来，我的数据目录布局是这样的：虽然可以保持每次被调用的文件名字都是“sample.mat”弊病其实也是明显的，时间一久，真的很难想得起到底是哪个对哪个，每次都需要重新回忆。后来我看到了这篇文章，顿时觉得， 所以分享出来，不过我不会严格地按原文去翻译。]]></content>
      <categories>
        <category>规范和标准</category>
      </categories>
      <tags>
        <tag>项目管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何布局并管理好项目目录（1/2）]]></title>
    <url>%2F2015%2F01%2F13%2F2015-01-13-how-to-manage-the-project-data-1.html</url>
    <content type="text"><![CDATA[不同类型的项目会有适合其特点的目录布局，很难说会有一个真正的标准，我这里特指的是基因组（或相关的）项目目录布局问题。 基因组项目目录结构的科学管理是一个非常重要的问题。如果不科学，东西一多更是成了灾难，很多时候你想找的东西并不是丢了，而是根本找不到了！有时更糟的是就算找到了，也会忘了是怎么来的，最后连自己都会怀疑它是不是真的就是正确的那一份！只得一轮又一轮地冥思苦想寻找着若隐若现的线索，甚至有时即使打算重做都不知道该怎么下手，效率真是极低，简直就是在浪费生命！这样下去根本没得玩，直到昨天我才意识到实际上要用程序设计的思想来管理每个项目的目录！还是A workflow for R这篇博文给我的灵感，它提出了一些规则： 透明（Transparency）：目录布局清晰明朗，逻辑清楚，整个结构一看就能明白。 易维护（Maintainability）：容易实行项目的修改和相关（如文件名、目录变换）调整。 模块化（Modularity）：任务之间都应该尽可能保持其独立性，每一个就只干好自己的事，不干涉其他任务，避免牵一发而动全身的情况发生，要有一一对应的关系，这样即便是需要修改也将会非常方便。 可移植（Portability）：项目可以很容易地移植到其他的平台或者系统中。 易复现（Reproducibility）：不论经过多久，都要能轻易重现原来的结果。 秒懂 (Efficiency)：无需多想就能明白项目执行过程中的相关细节，比如所要解决的每个问题是如何处理的和所用的工具是什么等等。 这几点完全说出了项目布局和目录管理所应达到的目的和状态，值得铭记于心多拿来参考参考。只是仅有理论，还是太过不着边际了点，有必要弄个例子来看看具体应该怎么做。 …未完待续]]></content>
      <categories>
        <category>规范和标准</category>
      </categories>
      <tags>
        <tag>翻译</tag>
        <tag>项目管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[替换google字体，加快网站访问速度]]></title>
    <url>%2F2015%2F01%2F07%2F2015-01-07-Replace-google-fonts.html</url>
    <content type="text"><![CDATA[一段时间以来，这个博客的打开速度慢得出奇！本来只有很少的东西，ping的速度也在120ms左右，算是可以的了，怎么会这样！一开始我没有搞明白问题的根源。今天才突然醒悟到一定是google字体加载的问题！当时这个博客的主题是从yihui和Carl Chen那里抄来的（不用试了，这俩的页面是极难打开的了），因为懒，因为不认识网页语言，尤其是css，所以所有.css后缀什么的，我一概不去看！所以那几个css就成了我的一个暗区，现在问题很明显了，要修改的地方一定在那！目标锁定之后，那么着手处理吧！ 我页面的主要配置都在style.css和home.css，打开这两文件，搜索google，果然发现“fonts.googleapis.com”！那么剩下的就是要先找字体替换方案了，搜索一番后，发现原来360已经把整个的google字库下载下来放在了自己的服务器上了，这样就太好了！改起来也极其简单，只需要直接把所有的fonts.googleapis.com替换为fonts.useso.com,而且还不用做任何其他的修改，字体也还是原来的字体！ 完成后，发现这下页面打开的速度和以前相比根本就是两个网站！不禁感叹：“靠！怎么这么简单！” 参考 : http://www.cgtt.net/1323.html]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改二维码生成工具]]></title>
    <url>%2F2014%2F09%2F19%2F2014-09-19-Change-QRCode-Generator.html</url>
    <content type="text"><![CDATA[本来博客在改版后，短时间内，我是不想做修改了的，回国后，没办法，原来的用于生成二维码的代码必须得换！根本就是失效了。原因也是很明朗的，二维码生成代码用的API是google的，稳定，强大……但在墙面前显得跟土一样。有时候我总觉得，是不是因为有了墙，大家的技术反而都要强那么一点了——魔高一尺道高一丈！只是有些时候也不得不拜倒了。总之，为了速度，为了大家的良好体验，换了！google的跟踪代码，我也取消了，换成了百度，不然网页死活打不开的，这是题外话了。 总之货比三家，最后我做了个比较大众化的选择——联图网的服务接口！说是接口，听起来似乎很高大上的样子，其实也就是一小段代码罢了，无需紧张。下面就是主题了。 API接口调用代码如下： 123456&lt;!-- 网页自动生成二维码的代码 --&gt;&lt;script type="text/javascript"&gt; thisURL = document.URL; strwrite = "&lt;p align='center'&gt;&lt;img src='http://qr.liantu.com/api.php?w=120&amp;m=2&amp;text=" + thisURL + "' alt='QR Code'/&gt;（传送门）&lt;/p&gt;" document.write( strwrite ); &lt;/script&gt; 还和上次博客改版中提到的一样，把上面这段代码完全替换掉原来qrCodeGenerate.md中的内容就ok了。 为了方便大家看的明白，这里需要对上面这段代码中的一些重要参数做些说明： http://qr.liantu.com/api.php? 是联图网的QR code API接口地址； w：二维码图片宽度，根据需要自行修改，如w=120，表示生成一个120×120像素的正方形二维码图片； m：二维码静区（外边距），一般设为比较小的数，我这里设置为2； text：二维码数据信息， 我这里设置为对应页面的URL地址； alt：二维码图片描述， 这个就没什么讲究了。 总体的效果请看文章末尾的二维码图片。 参考资料： 免插件自动生成wordpress文章二维码图片 很感激这篇文章给我的启发。]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>网站插件</tag>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅述CNV——我怎么会是我]]></title>
    <url>%2F2014%2F08%2F16%2F2014-08-16-Slightly-Introduce-CNV.html</url>
    <content type="text"><![CDATA[什么是CNV CNV的全称是Copy Number Variantion ，但这里特指“人类基因组拷贝数变异”。我们知道人类基因组是由60亿个化学碱基（核苷酸）所构成的。这60亿个化学碱基一共组合成了23对（46条）染色体，在每一对染色体中都有两条姐妹染色单体，它们分别遗传自父亲和母亲。在这些染色体中包含着大约30000个编码基因。鉴于人类基因组是一个二倍体，那一般来说，我们会自然而然地认为每一个基因都应该恰好有2份拷贝，即姐妹染色单体上各有一份。但，实际的情况却并非如此。近期的研究也表明，基因组上有些长度较大的DNA序列片段——长度通常在1000bp至1Mbp，存在着反常的拷贝次数。这样的一种现象就称之为拷贝数变异，也就是英文的CNV。这种拷贝数变异会导致受其影响的基因表达失调。举个例子来说，比如我们一直以为在每个人中这些基因都只有2份拷贝，但是现在却会在有些人中看到它们只有1份，或是3份，甚至更多。更为罕见的情况是在有些人中甚至存在着两条染色单体中同时丢失某个基因的情况（见下图）。 大家从图中可以看到，大多数的基因确实只有2分拷贝，但也存在着反常的情形（红色高亮的部分）（译者：这个图只是示意而已，实际的情形可不一定是这样分布的）。 CNV为什么那么重要 明确了CNV是什么之后，接下来的话题便是要明白CNV为什么重要了。应该说正是由于DNA序列上的差异使得我们每个人在这个世界上都是独一无二的。当然了，这些独一无二的特性还包括了对不同疾病易感程度的差异和不平等性。以前的看法认为单核苷酸多态性（SNP）是DNA遗传差异的最主要和最重重要的来源。然而，近期的研究成果中我们却看到，人类基因组CNV变异的序列，在长度上至少已经是单核苷酸变异（SNP）总长的3倍（译者：原文虽然这么说，但其实并不能因此就简单的以为CNV在遗传差异上的贡献会是SNP的3倍，这还和它所能表现出来的效应相联系）。并且考虑到发生CNV的序列常常围绕在基因的周围，这很可能暗示了它们在人类疾病和药物反应中扮演着一个相当重要的角色。另外，能弄明白人类基因组CNV的发生机制也将有助于我们更好的了解自身基因组的进化历程，以便搞清楚“我为何不同，我之所以是我”的深层原因。 这里是原文。]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多Github账号的使用问题]]></title>
    <url>%2F2014%2F08%2F10%2F2014-08-10-Multiple-Github-In-One-Machine.html</url>
    <content type="text"><![CDATA[同一台电脑，或者同一个（大型机）服务器账号下要使用多个Github账号，该咋办？ 如果是单用户(single-user)，很方便，默认拿id_rsa与你的github服务器的公钥对比；如果是多用户（multi-user）如user1,user2,那么就不能用在user2的身上了，这个时候就要配置一下了： 1、新建user2的SSH Key 12345#新建SSH key(若有需求，注意不要误删其他的id_rsa key)：$ cd ~/.ssh # 切换到~/.sshssh-keygen -t rsa -C mywork@email.com # 新建SSH key# 设置名称为id_rsa_hshujia (设置别的名字，防止因名字冲突而误删其他的id_rsa)Enter filein which to save the key (~/.ssh/id_rsa): id_rsa_hshujia 2、新密钥添加到SSH agent中 因为默认只读取id_rsa，为了让SSH识别新的私钥，需将其添加到SSH agent中： 添加至SSH agent1ssh-add ~/.ssh/id_rsa_hshujia 如果出现Could not open a connection to your authentication agent的错误，就试着用以下命令：1ssh-agent bash &amp;&amp; ssh-add ~/.ssh/id_rsa_hshujia 3、修改config文件 在~/.ssh目录下找到config文件，vi打开进行编辑，如果没有就创建：12# 创建configtouch config 然后按如下形式修改： 12345678910111213# 该文件用于配置私钥对应的服务器# Default github user(first@mail.com)**Host github.com HostName github.com User "user1" IdentityFile ~/.ssh/id_rsa # second user(second@mail.com) # 建一个github别名，新建的帐号使用这个别名做克隆和更新Host hshujia.github.com HostName github.com User "user2" IdentityFile ~/.ssh/id_rsa_hshujia 其规则就是：从上至下读取config的内容，在每个Host下寻找对应的私钥。这里将GitHub SSH仓库地址中的git@github.com替换成新建的Host别名如：hshujia.github.com，那么原地址是：git@github.com:user/Mywork.git，替换后应该是：git@hshujia.github.com:user/Mywork.git. 以下是我在（大型机）服务器上config文件的具体内容： 4、打开新生成的~/.ssh/id_rsa_hshujia.pub文件，将里面的内容添加到GitHub后台，此处默认大家懂得如何添加，就不详述了。 完成之后，在终端命令行中测试： 12ssh -T git@hshujia.github.com # 检验key是否已被成功添加到Github后台Hi hshujia! You&apos;ve successfully authenticated, but GitHub does not provide shell access. # 若能看到类似于这样的一句话就说明已经成功添加。 5、若以前是Global配置，则取消Global配置。 因为git pull or git push 的时候识别的是邮箱，多个github账号，就有多个邮箱，我们自然不能使用global的user.email了。 1234567# 1.取消global git config --global --unset user.namegit config --global --unset user.email# 2.在各个对应repo的目录下，设置项目repo自己的user.email和user namegit config user.email "xxxx@xx.com"git config user.name "xxx" 6、应用（例子） 写个Hello world 测试一下。打开登陆自己的账号，并新建一个repo，我这里命名为hello-world，创建完成之后，我在命令行中的具体操作如下: 1234567git initgit add README.mdgit config user.name hshujiagit config user.email hshujia@qq.comgit commit -m "first commit"git remote add origin git@hshujia.github.com:hshujia/hello-world.git # 注意修改git@后面的名字 git push -u origin master 这里唯一需要注意的就是多了两个git config用于告知该repo是属于哪一个user的；还有就是git remote add origin后面的host名字需要做相应的修改，具体的情况也已经在上面的代码中指明了。 参考资料 git初体验（七）多账户的使用 Git的多账号如何处理]]></content>
      <categories>
        <category>编程技术</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人类不是从猴子进化而来]]></title>
    <url>%2F2014%2F07%2F19%2F2014-07-19-Human-did-not-evolved-from-monkey.html</url>
    <content type="text"><![CDATA[问：如果人类是从猴子进化而来，那为什么猴子依然存在？ 答：我们不是从猴子进化而来的。我们（人和猴子）是从一个共同的祖先进化而来。 原文出处：sufuns 译文出处：科学公园–RhettZhang]]></content>
      <categories>
        <category>物种进化与迁徙</category>
      </categories>
      <tags>
        <tag>自然选择</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客大改版：添加评论，二维码生成，数学公式的显示，添加分析代码等]]></title>
    <url>%2F2014%2F07%2F18%2F2014-07-18-Change-blog-template.html</url>
    <content type="text"><![CDATA[目录 &nbsp;&nbsp;&nbsp;&nbsp;概述&nbsp;&nbsp;&nbsp;&nbsp;评论系统由Disqus改成了多说&nbsp;&nbsp;&nbsp;&nbsp;添加google analytics&nbsp;&nbsp;&nbsp;&nbsp;如何搞定数学公式显示问题&nbsp;&nbsp;&nbsp;&nbsp;如何让每个页面自动生成二维码&nbsp;&nbsp;&nbsp;&nbsp;如何添加“返回顶部”按钮“&nbsp;&nbsp;&nbsp;&nbsp;其他&nbsp;&nbsp;&nbsp;&nbsp;最后 概述 前几天把博客改版了，在Jekyll wiki上爬主题来回爬了好几遍，先是爬了Linghua Zhang,而后又直接爬了yihui和Carl Chen，不过做我这个版面主题的原始作者是yihui，随便一提这位是牛人，统计之都和COS论坛，以及中国R会议（第一届开始）都是他弄的，年纪轻轻且最近已经出了两本和R相关的书在亚马逊上卖了，恐怕国内（+很多国外）用过R的基本都知道！感兴趣的可以去他的主页扒扒。。。扯远了，说回我自己，虽然我也想改得更加不同一些，但一方面暂时还没啥时间，其次，此前未碰过任何与网页制作相关之事，HTML勉强能看，css和js就停留在听说过这两词的地步；然，最重要的是，我也看上了这个版面！所以门面修改一事先缓一缓吧，先在这里谢过各位作者大人！但是话也说回来，以上只要有任意一位作者介意，我也只能作罢，重新扒过。 OK，既然现在外在的部分还无力去动，那接下来，我说一下自己所做的一些内在改变。 1. 评论系统由Disqus改成了多说 我倒不是排斥Disqus，一开始我用的就是它，Disqus，国际化，版面简洁，管理容易，主页也生动，cool，我很喜欢！一个账号说遍天下，当然这一点上多说也一样。换掉它根本的原因还是在于伟大的‘墙’，Disqus只具有分享到Facebook和Twitter的功能，而这两货正常途径咱是上不了的。也罢，在国内的话多说用起来确实要更友好些，所以这次改版就重新选择了和国内社交网络联系在一起的多说。 这里我说一下自己是如何添加的。 （1）如果还没有绑定多说，那第一步需要做的就是到多说上绑定。多说是不需要做任何注册的，只要有QQ，微博，百度等账号就行，这一点相当方便，不然我又得增加一个账号，我都已经记不得自己在网络上到底注册过多少个账号了，恐怕各型各色几百个都有了！ （2）尔后，登陆多说主页点击“我要安装”，按提示走就行了。在站点地址这一栏填入自己的域名，再填上其他信息，按”创建” （3）创建了之后，在侧边框找到选择工具，选择通用代码，会看到多说提供了一段相关的代码，我们要做的事情很简单，直接把它复制并贴到你自己网页的代码中，然后，就没有然后了。。。诶诶，慢着呀！那应该贴进哪一个的网页里头啊？我这简单说明一下，比如，_layout（生成网站页面的相关网页绝大数都放这，jekyll会自己来加载）目录下我们一般都会创建defualt.html, page.html, post.html等这么几个文件。我假设post.html就是你用于博客的，而你也只想博客的页面才加多说评论框，那么代码就应该贴进post.html。虽然你可以放在与之间的任何位置，但考虑到页面主要内容的加载速度，最好还是把它靠后放，评论模块要那么急着加载干嘛，最好就只是在之前，下面我给了个示意。 添加多说的代码结构。 12345678910&lt;html&gt;&lt;head&gt;&lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt;...&lt;!—多说代码--&gt;&#123; % include duoshuo.md % &#125; &lt;!-- 实际使用中 “&#123;” 和 “%”，以及“%”和”&#125;“ 之间的空格需要去掉--&gt;&lt;/body&gt;&lt;/html&gt; 如果除了post.html，你还打算将它添加到其他的页面中，也没问题，操作和上面类似。但整块整块的代码复制来复制去的，总觉得很麻烦，一旦想修改还得一个个去改，太吃力了！！我自己更喜欢的一种方式就是直接把多说的代码写到一个单独的文件中，比如就叫duoshuo.md，并放在_include目录下（这个目录按照jekyll的建议，就是让我们用来存放模块插件的），然后在需要多说的页面中直接调用{ % include duoshuo.md % }就行(本文为了显示的原因在{和%以及%和}之间加了一个空格，正式使用的时候注意去掉，下同)，这就犹如一个函数一般，修改也只需要改duoshuo.md这个文件，相当方便。 在设置&lt;div class=&quot;ds-thread&quot; data-thread-key=&quot;请将此处替换成文章在你的站点中的ID&quot; data-title=&quot;请替换成文章的标题&quot; data-url=&quot;请替换成文章的网址&quot;&gt;&lt;/div&gt;，这几个值的时候，我参考了这篇博文，它讲的很清楚，不过里面写成data-thread-key=”&lt;%= page.path %&gt;”，data-title=”&lt;%= page.title %&gt;”和data-url=”&lt;%= page.permalink %&gt;”的形式在我这并不能成功，不知道是否与我用markdown格式编辑有关。所以后来我改成&lt;div class=&quot;ds-thread&quot; data-thread-key=&quot;{ { page.path } }&quot; data-title=&quot;{ { page.title } }&quot; data-url=&quot;{ { page.permalink } }&quot;&gt;&lt;/div&gt;就成功了（本文为了显示的原因在{和{以及}和}之间加了一个空格，正式使用的时候注意去掉，下同）。 2. 添加google analytics 对于自己的网站，一个关心的事情就是网站的浏览量如何？是通过什么途径以什么方式流入流出的？别的先不多说，光是能时刻知道自己的网站在哪里、被用什么操作系统、多少人浏览、网页之间的流入流出是怎么样的等状态信息的本身就是一个很cool的事情，这跟玩游戏一样，相信没多少人会愿意把自己角色的血条和蓝条隐藏掉，然后在那瞎玩。所以我就开始琢磨着应该怎么搞定这样一个事情。后来知道了可以用google-analytics帮助记录和分析，同样需要登陆google账号，填入网站域名，获得对应该域名的google analytics网络跟踪代码，然后又是复制粘贴。改过一次评论系统之后，我就发现，这东西都是大同小异的，就那么几下板斧。当然了我还是把它单独写成一个模块放在_include下供调用。代码如下： 123456789101112&lt;!-- Google Analysis For website tracking --&gt;&lt;script&gt; (function(i,s,o,g,r,a,m)&#123;i['GoogleAnalyticsObject']=r;i[r]=i[r]||function()&#123; (i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) &#125;)(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-52659904-2', 'auto'); ga('require', 'displayfeatures'); ga('send', 'pageview');&lt;/script&gt; &lt;!-- Google Analysis end --&gt; 这一次我希望全站跟踪，default.html是我所有页面都会添加的网页，所以这个代码就放在default.html中了。关于GA代码应该放在文件中的哪个位置比较适合，我还是做了一下考虑的，参考了这篇文章,按照异步跟踪的方式添加： 1234567891011&lt;html&gt;&lt;head&gt;&lt;title&gt;&lt;/title&gt;...&lt;!—GA异步追踪代码--&gt;&#123; % include googleAnalysis.md % &#125; &lt;!-- 实际使用中 “&#123;” 和 “%”，以及“%”和”&#125;“ 之间的空格需要去掉--&gt;...&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 注意: 在国内使用google analytics 并不是一个明智的做法，它已被封，这会大大降低你的网页访问速度，体验极差，建议还是使用百度分析. 3. 如何搞定数学公式显示问题 makrdown是个好东西，jekyll+markdown做网站很容易！但麻烦的是，markdown不能支持LaTeX，我写个数学公式在那上面，它解析不了，只是原封不动的显示在那。。这一点真让人捉急！咋办？又是一番的google，后来还是找到了个好办法——MathJax，它是一个数学公式显示引擎，能够把LaTeX编辑的公式在网页上显示出来，不过它是在线解析的，公式如果比较多的话，显示速度会稍慢。要用MathJax，需要先把_config.yml中的makrdown解析器换成kramdown，不然用不了。我同样也是写成模块mathJax.md，然后由post.html调用(估计我也不会在除了博客之外的地方用到它)，为了加载速度，也放的靠后一些，刚好在多说模块上面，下面是我用的mathJax.md模块的代码，当然了，若有需要你也可以使用： 123456&lt;script type="text/x-mathjax-config"&gt; MathJax.Hub.Config(&#123; tex2jax: &#123;inlineMath: [['$','$'], ['\\(','\\)']]&#125; &#125;);&lt;/script&gt;&lt;script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt; 成功添加了之后只需在公式前后用\$\$围起来，就可方便的编写公式了。如： 123$$\rho_i=\sum_j\chi(d_&#123;ij&#125;-d_c)$$ 显示的结果就是：$$\rho_i=\sumj\chi(d{ij}-d_c)$$ 4. 如何让每个页面自动生成二维码 二维码的作用其实也不必做多解释，最重要的就是方便。我之前并不知道可以用Google API为网站自动生成二维码一事，直到我成功了之后才恍然一悟，真是隔行如隔山。一开始我都是暴力解决：找个能生成二维码的网站，把自己每一篇博客的网址贴进去，点击生成二维码，然后再把这个生成的二维码图片下载下来，接着将它上传至图床，最后再将图片的链接添加到每篇博客的最后！这实在是。。。这个过程就算只是这样说起来都觉得十分费事，可以想象操作起来该有多费劲，而且一旦网站发生调整，就得重来一遍，实在低效！所以我一直寻思着该如何做才能让网页自己去产生二维码，一定要将它自动化！后来注意到了这个QR生成器，并且受到它share按钮中内容的启发，我当时就在想为什么它这样的一段代码（如下）在插入到网页中之后就能出来一个二维码呢？（当然这个二维码是指向它自己网站的） 12&lt;!--QR生成器的share代码--&gt;&lt;a href="https://www.the-qrcode-generator.com/"&gt;&lt;img src="http://chart.apis.google.com/chart?chs=200x200&amp;amp;cht=qr&amp;amp;chld=|1&amp;amp;chl=" alt="QR Code" /&gt;&lt;/a&gt; 在仔细观察后发现代码中url地址中后面的chs=200x200&amp;amp;cht=qr&amp;amp;chld=|1&amp;amp;chl=看起来很像某些参数，貌似是可以自己设置的，若真如此的话，又该怎么做呢？这次得感谢DIVCSS5这篇博文中的那一段代码结构，它完全让我明白了自己该如何改装上面的QR生成器代码让它能为我所用，给每一个网页都自动根据自己的网址产生正确的二维码，改了的代码如下： 123456&lt;!-- 网页自动生成二维码的代码 --&gt;&lt;script type="text/javascript"&gt; thisURL = document.URL; strwrite = "&lt;p align='center'&gt;&lt;img src='http://chart.apis.google.com/chart?chs=120x120&amp;amp;cht=qr&amp;amp;chld=|1&amp;amp;chl=" + thisURL + "' width='120' height='120' alt='QR Code'/&gt;（传送门）&lt;/p&gt;"; document.write( strwrite ); &lt;/script&gt; 这一段代码本身是独立的，不依赖于任何特定的网站拥有者，而且光看代码我们也能想象得到，这里头调用的就是Google API，所以如果有需要，也欢迎大家直接复制粘贴到自己的页面代码中。当然了，我还是把它独立写在了一个文件——qrCodeGenerate.md中并放在了_include目录下，然后再调用，就跟上面添加多说和google跟踪代码一个思路，不过这个倒没什么位置限制，就看各自喜欢放哪就放哪，只要是在与之间就行，但我还是建议放在博文内容加载模块之后，这样可以确保博文内容先加载。 注意: 后来在国内发现上面这个基于google的二维码，不但没用还大大影响了页面的加载速度！原因大家也都知道，具体的修改请参看这篇修改二维码生成工具博文。 5. 如何添加“返回顶部”按钮 “返回顶部”按钮（本博客右下角）的添加完全参照这个教程，作者写的相当详细！按照文章的信息我把’二’中的代码写入backtop.js，’三’中的信息写入backtop.css。然后将它们放在defualt.html中的与之间调用，示意如下： 12345678910&lt;html&gt;&lt;head&gt;...&lt;link rel="stylesheet" href="/media/css/backtop.css"&gt; &lt;!-- Back Top --&gt;&lt;script type="text/javascript" src="/media/js/backtop.js"&gt;&lt;/script&gt; &lt;!-- Back Top --&gt;...&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 6. 其他 （1）关于主页上的“FORUM”，本来有打算做成类似于论坛形式可以让大家就某个话题来进行讨论，但是发现要实现这个功能难度很大工作量不小！限于我自己能力有限，这个也得慢慢来，暂时将就变成了贴“鸡汤文”的板子了，o(╯□╰)o！ （2）google网站站长工具，上传了一份google提供的验证网页（它仅是验证之用），成功之后虽然可以将它删掉，但google官网的意思是建议留下。 （3）图床的选择。图床对于独立博客来说是一个重要又头疼的事情，目前我直接用点点博客的图片功能，使用外链。它的好处是免费，稳定，访问速度也快，要是真有一天点点不允许这样用的话，那我也不怕，实在不行就用七牛(注: 已改用七牛 )，或者想办法用Github作为图床。 7. 最后 OK！唠叨了不少总算把这篇文章写完了。接下来与博客搭建相关的内容先暂放一边了，我要回归主业的更新了。]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>网站插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客搬家]]></title>
    <url>%2F2014%2F07%2F09%2F2014-07-09-Blog-Migration.html</url>
    <content type="text"><![CDATA[以前在cnblog上还遗留了几篇文章。当时还准备就在那上面安家的呢。不过，发了几篇博文之后，大伙多评论说，“哥们是不是发错地方了”之类的云云，还有一次惊讶了我一下的是，博文直接被管理员丢出了cnblog的首页，o(╯□╰)o。也罢了，毕竟博客园面向的是IT开发人员，发BT(Biology Technical)相关的东西也确实不好意思，既然门不当户不对，那就另觅出路呗。后来搞了搞，要整个独立博客还真有点费事，主机要花钱，IP要花钱。。。那会我还没听说过Jekyll，再加上自己平时比较忙也没什么时间，所以就一直搁着了，直到最近用了github，才在无意间发现了Jekyll+github的原来可以方便的搭建起独立博客来！这个中的具体情况我在上次的博文中提到了。 虽然碰到了不少困难，但也还是七手八手把个人网站搭起来了。现在算是有了块自己的地，接着自然也就是要配置一些“行当”了，一些旧的“家当”也是不能扔的。所以就趁热打铁，把以前发在cnblog上的几篇文章也一起给搬过来了。这搬的过程倒也不难（但比较遗憾的事评论貌似不好搬过来），这要感谢这位RichardUSTC写的一个格式转换程序，虽然如ta自己所言，格式转换有些瑕疵，但我觉得已经很好了！也就只是瑕疵，自己再稍微调整一下也就OK了！关键的目的还是在于能节省下不少时间，就这样顺利搬家了！多谢多谢！不过话也说回来，转换过程中有些图表和段落的问题还没解决好，在三代基因组测序技术原理简介这篇博文中尤为明显！算了等后面有时间再一一来解决吧！ 补充一下，以前我在cnblog上的网名是T&amp;S，后来想想还是换成YellowTree好了。]]></content>
      <categories>
        <category>Blog</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Science上发表的一个超赞聚类算法]]></title>
    <url>%2F2014%2F07%2F06%2F2014-07-06-Science-Public-Cluster.html</url>
    <content type="text"><![CDATA[作者(Alex Rodriguez, Alessandro Laio)提出了一种很简洁优美的聚类算法, 可以识别各种形状的类簇, 并且其超参数很容易确定. ##算法思想 该算法的假设是, 类簇的中心由一些局部密度比较低的点围绕, 并且这些点距离其他高局部密度的点的距离都比较大. 首先定义两个值: 局部密度$$\rho_i$$以及到高局部密度点的距离$$\delta_i$$: $$\rho_i=\sumj\chi(d{ij}-d_c)$$ 其中 $$\chi(x)= \begin{cases}1 &amp; if x&lt;0\ 0&amp; otherwise \end{cases}$$ $$d_c$$是一个截断距离，是一个超参数。所以$$\rho_i$$相当于距离点$$i$$的距离小于$$d_c$$的点的个数。由于该算法只对$$\rho_i$$的相对值敏感，所以对$$d_c$$的选择比较鲁棒，一种推荐的做法是选择$$d_c$$使得平均每个点的邻居数为所有点的1%-2%。 $$\deltai=\min{j:\rho_j&gt;\rhoi}(d{ij})$$ 对于密度最大的点，设置$$\delta_i=maxj(d{ij})$$。注意只有那些密度是局部或者全局最大的点才会有远大于正常的相邻点间距。 ##聚类过程 那些有着比较大的局部密度$$\rho_i$$和很大的$$\delta_i$$的点被认为是类簇的中心. 局部密度较小但是$$\delta_i$$较大的点是异常点.在确定了类簇中心之后, 所有其他点属于距离其最近的类簇中心所代表的类簇. 图例如下: 左图是所有点在二维空间的分布, 右图是以$$\rho$$为横坐标, 以$$\delta$$为纵坐标, 这种图称作决策图(decision tree). 可以看到, 1和10两个点的$$\rho_i$$和$$\delta_i$$都比较大, 作为类簇的中心点。 26, 27, 28三个点的$$\delta_i$$也比较大, 但是$$\rho_i$$较小, 所以是异常点。 ##聚类分析 在聚类分析中, 通常需要确定每个点划分给某个类簇的可靠性. 在该算法中, 可以首先为每个类簇定义一个边界区域(border region), 亦即划分给该类簇但是距离其他类簇的点的距离小于$$d_c$$的点。 然后为每个类簇找到其边界区域的局部密度最大的点, 令其局部密度为$$\rho_h$$该类簇中所有局部密度大于$$\rho_h$$的点被认为是类簇核心的一部分(亦即将该点划分给该类簇的可靠性很大), 其余的点被认为是该类簇的光晕(halo), 亦即可以认为是噪音. 图例如下: A图为生成数据的概率分布, B, C二图为分别从该分布中生成了4000, 1000个点. D, E分别是B, C两组数据的决策图(decision tree), 可以看到两组数据都只有五个点有比较大的$$\rho_i$$和很大的$$\delta_i$$。 这些点作为类簇的中心, 在确定了类簇的中心之后, 每个点被划分到各个类簇(彩色点), 或者是划分到类簇光晕(黑色点)。F图展示的是随着抽样点数量的增多, 聚类的错误率在逐渐下降, 说明该算法是鲁棒的。 最后展示一下该算法在各种数据分布上的聚类效果，非常漂亮。 参考文献: [1]. Clustering by fast search and find of density peak. Alex Rodriguez, Alessandro Laio 本文转载自Kemaswill’s Blog]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github+Jekyll建站]]></title>
    <url>%2F2014%2F07%2F05%2F2014-07-05-Build-MyOwn-blog-with-jekyll-and-githubpage.html</url>
    <content type="text"><![CDATA[这该如何说起。。。 关于搭建自己的网站，其实早有此心，只是。。。。。。靠，本想长篇大论一番，讲讲自己是如何搭建起这个网站的，讲讲自己是如何在对Jekyll，markdown，GithubPage，DNS解析等一无所知的情形下，怎样现学现卖，等等之类的，最后想想还是算了，网上的这些教程太多了，况且我也是参照着别人的经验做出来的，还是不装了。呵呵。 这里就只列一下之前看到的教程，权当作记录吧。 我自己其实是纯属意外才会真的动起来去用Jekyll+Github来建博客的，Jekyll和Github以前是有所听闻的，但一直不知道它们竟还能有这层关系！嗨，其实说来也是因为不知道，Github也还用了不到一个月的时间，要不是因为有一次很偶然的看到了代码仓库的setting之下有个Automatic page generator的按钮，一时好奇心起，想知道这玩意是个啥，于是一查，哟！原来如此！就这样我知道了GitHub Pages的神奇作用，发现了它完全符合我自己想要建个人网站的需求！除了能映射自己的域名之外，更重要的还是免费+无限空间，对于是静态还是动态网站于我来讲并不是很关心，我关心的是，我要能完全的把控它！接着也就自然而然的认识了Jekyll,markdown我在一开始使用Github的时候就知道了，所以这个没啥槛。 那么说干就干了，方向有了，怎么开始好呢！习惯性Google了一下，发现了这个使用Github Pages建独立博客,作者写的很好，也是这篇博文领我入门的。包括了如何设置DNSpod解析域名，添加CNAME，怎么添加A记录等等这些。再次千恩万谢！不过我重设DNSpod进行域名解析的时候，基本是瞬间生效，不需要如作者所言要等一天那么久，但不知这情况是否普遍。接下来又陆陆续续看了很多有关Jekyll+github的文章，基本上大同小异，也算不断填补自己的空白吧。 要特别提到的是今天看到的一篇迟来的博客一步步在GitHub上创建博客主页,讲的也是相当的清楚，而且作者写的一整个系列的一步步教程，对于新手而言非常具有参考价值，还对域名以及与其相关的一些概念，如A记录，CNAME，TTL之类的做了详细的解析，真心推荐新手读之，此外也看到作者的博客做的很漂亮。 其他的一些则如一步步构建Jekyll网站,Jekyll建站之旅等等之类的，也都不错。就不一一列出了！ OK，暂时先这样了！]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>github</tag>
        <tag>jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因组变异检测概述]]></title>
    <url>%2F2013%2F09%2F28%2F2013-09-28-An-Introduction-of-genome-variant-detect.html</url>
    <content type="text"><![CDATA[首先，在开始之前我觉得有必要稍微科普缓冲一下，以便不使得不熟悉生物信息或基因组的客官们疑惑。 基因组：每个人都有一个基因组，这里的基因组并不只是基因的集合，基因是控制性状的遗传单元（什么是性状呢？性状也可以狭义的理解为个体的各种外在和内在特征，比如头发和眼睛颜色，高矮胖瘦，抵抗力强等），但是基因组所指的其实是我们的所有遗传信息，而不单单只是一些外在和内在特征，也包含很多目前而言不明其功能性（或者被认为无功能）的DNA序列。 其实说白了就是整一个的DNA序列！因而，基因也只是基因组的一个子集。此外，需要特别指出的是，我们虽都为”人”，但人与人之间的基因组是不一样的（即是多态的），彼此之间都存在着一些差异，即使是和父母或是兄弟姐妹之间去比较。这些差异也是造成我们彼此之间为何如此这般不同的一个重要原因。而这些差异也是基因组多态性的来源。 Reads：这里的reads是一个在基因组测序中使用的名词（对测序原理感兴趣的客官请猛戳：三代基因组测序技术原理简介），指的就是一段特定长度的DNA片段，这个长度取决于测序仪的读长。 变异是一个相对的概念，只有在彼此的比较中才有存在的意义。目前关于人类基因组变异的讨论，都是以”人类基因组计划”;中所组装出来的人类基因组作为参照物。以下谈到的涉及比对过程所用的基因组指的就是这个人类参考基因组。 以下常出现”序列”，指的都是DNA序列片段。 OK！简单的科普就此完毕，剩余的在后面碰到了再说明，以下进入正文。 摘要：人类基因组上的结构性变异研究对于基因组进化，群体多态性分析以及疾病易感性等方面的研究有着重要的意义。第二代短reads高通量测序技术的发展在带来了测序成本降低的同时，这种短读长的测序方式也给人类的变异检测带来了很大的挑战。这里我主要对当前常用的变异检测方法、软件以及他们各自的有确定做一个简要的小结. 人类基因组上的变异主要分为三大类： 单核苷酸变异，（通常称为单核苷酸多态性，通俗的说法就是单个DNA碱基, 简称SNP） 小的Indel（Insertion 和 Deletion的简），指的是在基因组的某个位置上所发生的小片段序列的插入或者删除，其长度通常在50bp以下（这个长度范围的变异可以利用Smith-Waterman 的比对算法来获得）； 大的结构性变异，这种类型比较多，包括长度在50bp以上的长片段序列的插入或者删除、染色体倒位，染色体内部或染色体之间的序列易位，拷贝数变异，以及一些形式更为复杂的变异。为了和SNP变异作区分，第2和第3类变异通常也被称为基因组结构性变异（Structural variation，简称SV）。 这里值得一提的是，研究人员对基因组的结构性变异发生兴趣，主要是由于这几年的研究发现： 虽然还未被广泛公认，但研究人员发现SV对基因组的影响比起SNP来说还要大； 基因组上的SV比起SNP而言，似乎更能用于解释人类群体多样性的特征； 稀有且相同的一些结构性变异往往和疾病（包括一些癌症）的发生相关联甚至还是其致病的诱因。 不过应该注意的地方是，大多数的结构性变异并不真正与疾病的发生相关联，但是却确实与周围环境的响应或者其他的一些表型多态性相联系。 近年来，随着芯片技术（这里的芯片技术和IT领域所说的芯不是同一个概念，这里指的是一种用于抓获基因组特定序列片段的技术）和第二代高通量测序技术的发展，人类基因组上的结构性变异图谱才被真正全面而又集中地进行了研究。生物信息研究人员已针对这两种不同的技术开发了许多相对应的软件用于检测基因组的结构性变异。相比较而言，虽然成本较高，但是基于测序的方法要明显优于芯片的检测，其中最重要的一个方面是，高通量测序技术能够在单碱基精度之下对全基因组范围内所有类型的变异进行检测，而芯片技术实际上只对大片段的序列删除比较敏感。 接下来我将会对目前基于第二代测序技术的变异检测方法进行介绍。 在各大生物信息学期刊（包括Nature，Science，Cell等这些顶级期刊）上都有许多关于介绍变异检测方面的文章。这里我大致说一下四篇自己觉得在这方面比较重要的文章。 综述：Genome structural variation discovery and genotyping 综述：computational methods for discovering structural variation with next-generation sequencing， 这两篇文章所探讨的主要是，如何根据实验上和计算上的途径来检测和发现基因组上的各种变异，特别是对检测SVs而已。 另外两篇文章则是基于千人基因组计划的，他们描述的是如何利用trio家系全基因组测序的数据和群体低覆盖度的数据来做变异检测的生物信息学方法。然而需要指出的是，对于千人基因组计划，他们基本上只关注于一些大片段的序列删除和一些特定的序列插入方面的检测，而忽视了很多基因组上其他形式的变异。关于这方面的局限性，一方面可能是由于生物信息检测方法上的不完善，另一方面可能也和千人基因组本身的数据特点有关，使得他们难以准确地获得更多的信息。 目前主要有4种检测基因组上结构性变异的策略，分别为： Read pair（也称为Pair-end Mapping，简称PEM）； Split read（简称SR）； Read Depth（简称RD）； 基于de novo组装的方法（图1）。同时生物信息研究人员也已开发了众多根据以上4中策略中一种或者多种的软件用于结构性变异的检测。接下来我将对这四种策略以及他们各自的特点逐一进行介绍。 图1 基于Pair-end Mapping（PEM）图2是PEM方法的一个主要分析框架，理论上来讲，PEM方法能够检测到的变异类型包括：序列删除（deletion），序列插入（insertion），序列转置（inversion），染色体内部和染色体外部的易位（intra- and inter-chromosome translocation），序列串联倍增（tandem duplications）和序列在基因组上的散在倍增（interspersed duplications）。 这里有两个地方需要指出： 第一，对于序列删除的检测，其所能检测到的片段长度受插入片段长度的标准差（SD）所影响（这里的插入片段长度指的是测序之前在构建DNA测序文库阶段，所选取的经由超声波打断的DNA片段长度，这些片段也称之为测序片段，这是实验过程中的操作，并不是指基因组的变异），并且越大的序列删除约容易被检测到，并且准确性也越高； 第二，其所能检测的序列插入，长度只能在插入片段长度的范围内，并且最大长度也受限于测序的插入片段长度的标准差。目前，Breakdancer是应用PEM方法的软件，也是在使用变异检测方面用得最广泛的软件之一。其他类似的软件还包括：VariationHunter, Spanner, PEMer等。 但是，事实上整个过程并不像流程图中看起来的那么简单，而且绝大多数的软件都在检测复杂的序列结构方面（如序列易位和序列倍增）存在很大的困难。 图2 Split Read（分裂read，简称SR）对于这个方法，首先要求比对软件具备soft-clip reads的能力，如BWA 比对软件。我们知道目前illumina测序平台Pair-End测序的方法是对测序片段的两端来进行的，所以每次获得的都是来自同一个测序序列片段两端的一对read。当BWA成功地将这一对reads中的一条比对到参考序列上，而另一条却无法正常比上的时候，BWA会对这条read没能正常比上的read尝试在比对上的那条read附近使用更为宽松的Smith-Waterman局部比对策略搜索可能的比对位置。如果这条read只有一部分能够比上，那么BWA会对其进行soft-clip，而这里也往往是包含结构性变异的断点之处。Pindel，这是目前唯一一个使用SR方法进行变异检测的软件。它在千人基因组计划和生物信息分析人员中被广泛使用。 图1中也清楚地展示了Split reads的信号如何被用来进行结构性变异的检测。 首先，在获得了单端唯一比对到基因组上的PE read之后，Pindel会将不能比上的那条read切开成2或者3小段，然后再分别重新按照用户所设置的最大序列删除长度去比对，并获得最终的比对位置和比对方向，而断点位置的确定则是根据soft-clipped的结果来获得。 Pindel 理论上能够检测所有长度范围内的deletion，和小片段的insertion（长度在50bp以下），inversion，tandem duplication和一些large insertion。不过目前，作者并未公开发布关于检测lager insertion的原理。Split-reads的一个优势就在于，它们精确到单碱基。但是也和大多数的PEM方法一样，Pindel同样无法解决复杂结构性变异的情形。 Read Depth （read 覆盖深度，简称RD）目前存在两种利用Read depth的信息检测大拷贝数变异（Copy number variation，包括丢失序列和序列重复倍增，简称CNV）的策略。一种是，通过检测样本在一个参考基因组上read的深度分布情况来检测CNV，适用于单样本；另一种则是通过和识别出比较两个样本中所存在的丢失和重复倍增区，以此来获得相对的CNV，适用于case-control模型的样本。这有点像CGH芯片。CNVnator使用的是第一种策略，同时也广泛地被用于检测大的CNV。当然还有一些比较冷门的软件，但是由于他们没有发表相应的文章，这里就不再列举了。CNV-seq使用的是第二个策略。基于其原理，RD的方法能够很好地用于检测一些大的deletion或者duplication事件，但是对于小的变异事件就无能为力了。 基于De novo assembly理论上来讲，de novo assembly 的方法应该要算是基因组变异检测上最有效的方法了。就目前来说，它能够提供（特别是）对于long insertion和复杂结构性变异的最好检测方法。现在虽然研究人员开发了很多基于第二代测序技术数据来进行组装的软件，但是组装却仍然是一件棘手的事情，特别是脊椎动物的组装则更是如此。其中最主要的原因在于，脊椎动物基因组上所存在的重复性序列和序列的杂合会严重影响组装的质量，除去资金成本，这也在很大程度上阻碍了利用组装的方法在基因组变异检测方面的应用。 小结通过对上面四种不同的变异检测策略的比较可以发现，小长度范围内的变异以及较长的deletion，目前都能够较好地检测出来，但对于大多数的long insertion和更复杂的结构性变异情况，当前的检测软件基本都没法还解决。Assembly应是当前全面获得基因组上各种变异的最好方法，但是目前的局限却也发生在Assembly本身，若是基因组没能装得好，后面的变异检测就更是无从说起。从目前的情况看，de novo assembly的方法并不能很快进入实际的应用。因此，暂且不提assembly，其余的三种策略都各有各的优势，从目前的结果看，并没有哪一款软件能够一次性地将基因组上的各种不同情况变异类型都获得。因此就目前短reads高通量测序技术来说，最合适的方案应是结合多个不同的策略，将结果合并在一起，这样可以最大限度地将FP降低。HugeSeq pipeline在这方面做了一个比较好的总结，这个软件整合了BreakDancer, CNVnator, Pindel，BreakSeq以及GATK的结果。能够给出一个相对比较准确的变异检测结果。最后这句怎么看起来像是在帮别人卖广告o(╯□╰)o。 参考文献 1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DePristo, M. a et al. A framework for variation discovery and genotyping using next-generation DNA sequencing data. Nature genetics43, 491&ndash;8 (2011).2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Albers, C. a et al. Dindel: accurate indel calls from short-read data. Genome research21, 961&ndash;73 (2011).3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conrad, D. F. et al. Europe PMC Funders Group Origins and functional impact of copy number variation in the human genome. 464, 704&ndash;712 (2012).4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Campbell, P. J. et al. Identification of somatically acquired rearrangements in cancer using genome-wide massively parallel paired-end sequencing. Nature genetics40, 722&ndash;9 (2008).5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Berger, M. F. et al. The genomic complexity of primary human prostate cancer. Nature470, 214&ndash;20 (2011).6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Stephens, P. J. et al. Massive genomic rearrangement acquired in a single catastrophic event during cancer development. Cell144, 27&ndash;40 (2011).7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Alkan, C., Coe, B. P. &amp; Eichler, E. E. Genome structural variation discovery and genotyping. Nature reviews. Genetics12, 363&ndash;76 (2011).8.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mills, R. E. et al. Mapping copy number variation by population-scale genome sequencing. Nature470, 59&ndash;65 (2011).9.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Africa, W. A map of human genome variation from population-scale sequencing. Nature467, 1061&ndash;73 (2010).10.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hormozdiari, F., Alkan, C., Eichler, E. E. &amp; Sahinalp, S. C. Combinatorial algorithms for structural variation detection in high-throughput sequenced genomes. Genome research19, 1270&ndash;8 (2009).11.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Korbel, J. O. et al. PEMer: a computational framework with simulation-based error models for inferring genomic structural variants from massive paired-end sequencing data. Genome biology10, R23 (2009).12.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ye, K., Schulz, M. H., Long, Q., Apweiler, R. &amp; Ning, Z. Pindel: a pattern growth approach to detect break points of large deletions and medium sized insertions from paired-end short reads. Bioinformatics (Oxford, England)25, 2865&ndash;71 (2009).13.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Lam, H. Y. K. et al. Detecting and annotating genetic variations using the HugeSeq pipeline. Nature biotechnology30, 226&ndash;9 (2012).&nbsp;]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三代基因组测序技术原理简介]]></title>
    <url>%2F2013%2F08%2F02%2F2013-08-02-An-Introduction-of-NGS-Sequence.html</url>
    <content type="text"><![CDATA[摘要：从1977年第一代DNA测序技术（Sanger法）1，发展至今三十多年时间，测序技术已取得了相当大的发展，从第一代到第三代乃至第四代，测序读长从长到短，再从短到长。虽然就当前形势看来第二代短读长测序技术在全球测序市场上仍然占有着绝对的优势位置，但第三和第四代测序技术也已在这一两年的时间中快速发展着。测序技术的每一次变革，也都对基因组研究，疾病医疗研究，药物研发，育种等领域产生巨大的推动作用。在这里我主要对当前的测序技术以及它们的测序原理做一个简单的小结。 图1：测序技术的发展历程 生命体遗传信息的快速获得对于生命科学的研究有着十分重要的意义。以上图1（右键打开图片可查看大图，下同）所描述的是自沃森和克里克在1953年建立DNA双螺旋结构以来，整个测序技术的发展历程。第一代测序技术第一代DNA测序技术用的是1975年由桑格（Sanger）和考尔森（Coulson）开创的链终止法或者是1976-1977年由马克西姆（Maxam）和吉尔伯特（Gilbert）发明的化学法（链降解）. 并在1977年，桑格测定了第一个基因组序列，是噬菌体X174的，全长5375个碱基1。自此，人类获得了窥探生命遗传差异本质的能力，并以此为开端步入基因组学时代。研究人员在Sanger法的多年实践之中不断对其进行改进。在2001年，完成的首个人类基因组图谱就是以改进了的Sanger法为其测序基础，Sanger法核心原理是：由于ddNTP的2&rsquo;和3&rsquo;都不含羟基，其在DNA的合成过程中不能形成磷酸二酯键，因此可以用来中断DNA合成反应，在4个DNA合成反应体系中分别加入一定比例带有放射性同位素标记的ddNTP（分为：ddATP,ddCTP,ddGTP和ddTTP），通过凝胶电泳和放射自显影后可以根据电泳带的位置确定待测分子的DNA序列（图2）。这个网址为sanger测序法制作了一个小短片，形象而生动。值得注意的是，就在测序技术起步发展的这一时期中，除了Sanger法之外还出现了一些其他的测序技术，如焦磷酸测序法、链接酶法等。其中，焦磷酸测序法是后来Roche公司454技术所使用的测序方法2&ndash;4，而连接酶测序法是后来ABI公司SOLID技术使用的测序方法2,4，但他们的共同核心手段都是利用了Sanger1中的可中断DNA合成反应的dNTP。 图2：Sanger法测序原理 第二代测序技术总的说来，第一代测序技术的主要特点是测序读长可达1000bp，准确性高达99.999%，但其测序成本高，通量低等方面的缺点，严重影响了其真正大规模的应用。因而第一代测序技术并不是最理想的测序方法。经过不断的技术开发和改进，以Roche公司的454技术、illumina公司的Solexa，Hiseq技术和ABI公司的Solid技术为标记的第二代测序技术诞生了。第二代测序技术大大降低了测序成本的同时，还大幅提高了测序速度，并且保持了高准确性，以前完成一个人类基因组的测序需要3年时间，而使用二代测序技术则仅仅需要1周，但在序列读长方面比起第一代测序技术则要短很多。表1和图3对第一代和第二代测序技术各自的特点以及测序成本作了一个简单的比较5，以下我将对这三种主要的第二代测序技术的主要原理和特点作一个简单的介绍。&nbsp; 图3. 测序成本的变化 Illumine Illumina公司的Solexa和Hiseq应该说是目前全球使用量最大的第二代测序机器，这两个系列的技术核心原理是相同的2,4。这两个系列的机器采用的都是边合成边测序的方法，它的测序过程主要分为以下4步，如图4.&nbsp;&nbsp;&nbsp;&nbsp; （1）DNA待测文库构建利用超声波把待测的DNA样本打断成小片段，目前除了组装之外和一些其他的特殊要求之外，主要是打断成200-500bp长的序列片段，并在这些小片段的两端添加上不同的接头，构建出单链DNA文库。&nbsp;&nbsp;&nbsp;&nbsp; （2）FlowcellFlowcell是用于吸附流动DNA片段的槽道，当文库建好后，这些文库中的DNA在通过flowcell的时候会随机附着在flowcell表面的channel上。每个Flowcell有8个channel，每个channel的表面都附有很多接头，这些接头能和建库过程中加在DNA片段两端的接头相互配对（这就是为什么flowcell能吸附建库后的DNA的原因），并能支持DNA在其表面进行桥式PCR的扩增。&nbsp;&nbsp;&nbsp;&nbsp; （3）桥式PCR扩增与变性桥式PCR以Flowcell表面所固定的接头为模板，进行桥形扩增，如图4.a所示。经过不断的扩增和变性循环，最终每个DNA片段都将在各自的位置上集中成束，每一个束都含有单个DNA模板的很多分拷贝，进行这一过程的目的在于实现将碱基的信号强度放大，以达到测序所需的信号要求。&nbsp;（4）测序测序方法采用边合成边测序的方法。向反应体系中同时添加DNA聚合酶、接头引物和带有碱基特异荧光标记的4中dNTP（如同Sanger测序法）。这些dNTP的3&rsquo;-OH被化学方法所保护，因而每次只能添加一个dNTP。在dNTP被添加到合成链上后，所有未使用的游离dNTP和DNA聚合酶会被洗脱掉。接着，再加入激发荧光所需的缓冲液，用激光激发荧光信号，并有光学设备完成荧光信号的记录，最后利用计算机分析将光学信号转化为测序碱基。这样荧光信号记录完成后，再加入化学试剂淬灭荧光信号并去除dNTP 3&rsquo;-OH保护基团，以便能进行下一轮的测序反应。Illumina的这种测序技术每次只添加一个dNTP的特点能够很好的地解决同聚物长度的准确测量问题，它的主要测序错误来源是碱基的替换，目前它的测序错误率在1%-1.5%之间，测序周期以人类基因组重测序为例，30x测序深度大约为1周。&nbsp; 图4. Illumina测序流程 Roche 454 Roche 454测序系统是第一个商业化运营二代测序技术的平台。它的主要测序原理是（图5 abc）2：（1）DNA文库制备454测序系统的文件构建方式和illumina的不同，它是利用喷雾法将待测DNA打断成300-800bp长的小片段，并在片段两端加上不同的接头，或将待测DNA变性后用杂交引物进行PCR扩增，连接载体，构建单链DNA文库（图5a）。（2）Emulsion PCR （乳液PCR，其实是一个注水到油的独特过程）454当然DNA扩增过程也和illumina的截然不同，它将这些单链DNA结合在水油包被的直径约28um的磁珠上，并在其上面孵育、退火。乳液PCR最大的特点是可以形成数目庞大的独立反应空间以进行DNA扩增。其关键技术是&ldquo;注水到油&rdquo;（水包油），基本过程是在PCR反应前，将包含PCR所有反应成分的水溶液注入到高速旋转的矿物油表面，水溶液瞬间形成无数个被矿物油包裹的小水滴。这些小水滴就构成了独立的PCR反应空间。理想状态下，每个小水滴只含一个DNA模板和一个磁珠。这些被小水滴包被的磁珠表面含有与接头互补的DNA序列，因此这些单链DNA序列能够特异地结合在磁珠上。同时孵育体系中含有PCR反应试剂，所以保证了每个与磁珠结合的小片段都能独立进行PCR扩增，并且扩增产物仍可以结合到磁珠上。当反应完成后，可以破坏孵育体系并将带有DNA的磁珠富集下来。进过扩增，每个小片段都将被扩增约100万倍，从而达到下一步测序所要求的DNA量。（3）焦磷酸测序测序前需要先用一种聚合酶和单链结合蛋白处理带有DNA的磁珠，接着将磁珠放在一种PTP平板上。这种平板上特制有许多直径约为44um的小孔，每个小孔仅能容纳一个磁珠，通过这种方法来固定每个磁珠的位置，以便检测接下来的测序反应过程。 测序方法采用焦磷酸测序法，将一种比PTP板上小孔直径更小的磁珠放入小孔中，启动测序反应。测序反应以磁珠上大量扩增出的单链DNA为模板，每次反应加入一种dNTP进行合成反应。如果dNTP能与待测序列配对，则会在合成后释放焦磷酸基团。释放的焦磷酸基团会与反应体系中的ATP硫酸化学酶反应生成ATP。生成的ATP和荧光素酶共同氧化使测序反应中的荧光素分子并发出荧光，同时由PTP板另一侧的CCD照相机记录，最后通过计算机进行光信号处理而获得最终的测序结果。由于每一种dNTP在反应中产生的荧光颜色不同，因此可以根据荧光的颜色来判断被测分子的序列。反应结束后，游离的dNTP会在双磷酸酶的作用下降解ATP，从而导致荧光淬灭，以便使测序反应进入下一个循环。由于454测序技术中，每个测序反应都在PTP板上独立的小孔中进行，因而能大大降低相互间的干扰和测序偏差。454技术最大的优势在于其能获得较长的测序读长，当前454技术的平均读长可达400bp，并且454技术和illumina的Solexa和Hiseq技术不同，它最主要的一个缺点是无法准确测量同聚物的长度，如当序列中存在类似于PolyA的情况时，测序反应会一次加入多个T，而所加入的T的个数只能通过荧光强度推测获得，这就有可能导致结果不准确。也正是由于这一原因，454技术会在测序过程中引入插入和缺失的测序错误。&nbsp; 图5. Roche 454测序流程 Solid技术 Solid测序技术是ABI公司于2007年开始投入用于商业测序应用的仪器。它基于连接酶法，即利用DNA连接酶在连接过程之中测序（图6）2,4。它的原理是： 图6-a. Solid测序技术 （1）DNA文库构建 片段打断并在片段两端加上测序接头，连接载体，构建单链DNA文库。（2）Emulsion PCRSolid的PCR过程也和454的方法类似，同样采用小水滴emulsion PCR，但这些微珠比起454系统来说则要小得多，只有1um。在扩增的同时对扩增产物的3&rsquo;端进行修饰，这是为下一步的测序过程作的准备。3&rsquo;修饰的微珠会被沉积在一块玻片上。在微珠上样的过程中，沉积小室将每张玻片分成1个、4个或8个测序区域（图6-a）。Solid系统最大的优点就是每张玻片能容纳比454更高密度的微珠，在同一系统中轻松实现更高的通量。（3）连接酶测序这一步是Solid测序的独特之处。它并没有采用以前测序时所常用的DNA聚合酶，而是采用了连接酶。Solid连接反应的底物是8碱基单链荧光探针混合物，这里将其简单表示为：3&rsquo;-XXnnnzzz-5&rsquo;。连接反应中，这些探针按照碱基互补规则与单链DNA模板链配对。探针的5&rsquo;末端分别标记了CY5、Texas Red、CY3、6-FAM这4种颜色的荧光染料（图6-a）。这个8碱基单链荧光探针中，第1和第2位碱基（XX）上的碱基是确定的，并根据种类的不同在6-8位（zzz）上加上了不同的荧光标记。这是Solid的独特测序法，两个碱基确定一个荧光信号，相当于一次能决定两个碱基。这种测序方法也称之为两碱基测序法。当荧光探针能够与DNA模板链配对而连接上时，就会发出代表第1，2位碱基的荧光信号，图6-a和图6-b中的比色版所表示的是第1，2位碱基的不同组合与荧光颜色的关系。在记录下荧光信号后，通过化学方法在第5和第6位碱基之间进行切割，这样就能移除荧光信号，以便进行下一个位置的测序。不过值得注意的是，通过这种测序方法，每次测序的位置都相差5位。即第一次是第1、2位，第二次是第6、7位&hellip;&hellip;在测到末尾后，要将新合成的链变性，洗脱。接着用引物n-1进行第二轮测序。引物n-1与引物n的区别是，二者在与接头配对的位置上相差一个碱基（图6-a. 8）。也即是，通过引物n-1在引物n的基础上将测序位置往3&rsquo;端移动一个碱基位置，因而就能测定第0、1位和第5、6位&hellip;&hellip;第二轮测序完成，依此类推，直至第五轮测序，最终可以完成所有位置的碱基测序，并且每个位置的碱基均被检测了两次。该技术的读长在2&times;50bp，后续序列拼接同样比较复杂。由于双次检测，这一技术的原始测序准确性高达99.94%，而15x覆盖率时的准确性更是达到了99.999%，应该说是目前第二代测序技术中准确性最高的了。但在荧光解码阶段，鉴于其是双碱基确定一个荧光信号，因而一旦发生错误就容易产生连锁的解码错误。 图6-b. Solid测序技术 第三代测序技术测序技术在近两三年中又有新的里程碑。以PacBio公司的SMRT和Oxford Nanopore Technologies纳米孔单分子测序技术，被称之为第三代测序技术。与前两代相比，他们最大的特点就是单分子测序，测序过程无需进行PCR扩增。其中PacBio SMRT技术其实也应用了边合成边测序的思想5，并以SMRT芯片为测序载体。基本原理是： DNA聚合酶和模板结合,4色荧光标记 4 种碱基（即是dNTP）,在碱基配对阶段,不同碱基的加入,会发出不同光,根据光的波长与峰值可判断进入的碱基类型。同时这个 DNA 聚合酶是实现超长读长的关键之一,读长主要跟酶的活性保持有关,它主要受激光对其造成的损伤所影响。PacBio SMRT技术的一个关键是怎样将反应信号与周围游离碱基的强大荧光背景区别出来。他们利用的是ZMW（零模波导孔）原理：如同微波炉壁上可看到的很多密集小孔。小孔直径有考究,如果直径大于微波波长,能量就会在衍射效应的作用下穿透面板而泄露出来，从而与周围小孔相互干扰。如果孔径小于波长,能量不会辐射到周围，而是保持直线状态（光衍射的原理）,从而可起保护作用。同理,在一个反应管(SMRTCell:单分子实时反应孔)中有许多这样的圆形纳米小孔, 即 ZMW(零模波导孔),外径 100多纳米,比检测激光波长小(数百纳米),激光从底部打上去后不能穿透小孔进入上方溶液区,能量被限制在一个小范围(体积20X 10-21 L)里,正好足够覆盖需要检测的部分,使得信号仅来自这个小反应区域,孔外过多游离核苷酸单体依然留在黑暗中,从而实现将背景降到最低。另外，可以通过检测相邻两个碱基之间的测序时间，来检测一些碱基修饰情况，既如果碱基存在修饰，则通过聚合酶时的速度会减慢，相邻两峰之间的距离增大，可以通过这个来之间检测甲基化等信息（图7）。SMRT技术的测序速度很快，每秒约10个dNTP。但是，同时其测序错误率比较高（这几乎是目前单分子测序技术的通病），达到15%,但好在它的出错是随机的，并不会像第二代测序技术那样存在测序错误的偏向，因而可以通过多次测序来进行有效的纠错。 图7. PacBio SMRT测序原理 Oxford Nanopore Technologies公司所开发的纳米单分子测序技术与以往的测序技术皆不同，它是基于电信号而不是光信号的测序技术5。该技术的关键之一是，他们设计了一种特殊的纳米孔，孔内共价结合有分子接头。当DNA碱基通过纳米孔时，它们使电荷发生变化，从而短暂地影响流过纳米孔的电流强度（每种碱基所影响的电流变化幅度是不同的），灵敏的电子设备检测到这些变化从而鉴定所通过的碱基（图8）。该公司在去年基因组生物学技术进展年会(AGBT)上推出第一款商业化的纳米孔测序仪，引起了科学界的极大关注。纳米孔测序（和其他第三代测序技术）有望解决目前测序平台的不足，纳米孔测序的主要特点是：读长很长，大约在几十kb，甚至100 kb;错误率目前介于1%至4%，且是随机错误，而不是聚集在读取的两端;数据可实时读取;通量很高(30x人类基因组有望在一天内完成);起始DNA在测序过程中不被破坏;以及样品制备简单又便宜。理论上，它也能直接测序RNA。纳米孔单分子测序计算还有另一大特点，它能够直接读取出甲基化的胞嘧啶，而不必像传统方法那样对基因组进行bisulfite处理。这对于在基因组水平直接研究表观遗传相关现象有极大的帮助。并且改方法的测序准确性可达99.8%，而且一旦发现测序错误也能较容易地进行纠正。但目前似乎还没有应用该技术的相关报道。 图8. 纳米孔测序 其他测序技术目前还有一种基于半导体芯片的新一代革命性测序技术&mdash;&mdash;Ion Torrent6。该技术使用了一种布满小孔的高密度半导体芯片， 一个小孔就是一个测序反应池。当DNA聚合酶把核苷酸聚合到延伸中的DNA链上时，会释放出一个氢离子，反应池中的PH发生改变，位于池下的离子感受器感受到H+离子信号，H+离子信号再直接转化为数字信号，从而读出DNA序列（图9）。这一技术的发明人同时也是454测序技术的发明人之一&mdash;&mdash;Jonathan Rothberg，它的文库和样本制备跟454技术很像，甚至可以说就是454的翻版，只是测序过程中不是通过检测焦磷酸荧光显色，而是通过检测H+信号的变化来获得序列碱基信息。Ion Torrent相比于其他测序技术来说，不需要昂贵的物理成像等设备，因此，成本相对来说会低，体积也会比较小，同时操作也要更为简单，速度也相当快速，除了2天文库制作时间，整个上机测序可在2-3.5小时内完成，不过整个芯片的通量并不高，目前是10G左右，但非常适合小基因组和外显子验证的测序。&nbsp;&nbsp;&nbsp;&nbsp; 图9. Ion Torrent &nbsp;小结以上，对各代测序技术的原理做了简要的阐述，这三代测序技术的特点比较汇总在以下表1和表2中。其中测序成本，读长和通量是评估该测序技术先进与否的三个重要指标。第一代和第二代测序技术除了通量和成本上的差异之外，其测序核心原理（除Solid是边连接边测序之外）都是基于边合成边测序的思想。第二代测序技术的优点是成本较之一代大大下降，通量大大提升，但缺点是所引入PCR过程会在一定程度上增加测序的错误率，并且具有系统偏向性，同时读长也比较短。第三代测序技术是为了解决第二代所存在的缺点而开发的，它的根本特点是单分子测序，不需要任何PCR的过程，这是为了能有效避免因PCR偏向性而导致的系统错误，同时提高读长，并要保持二代技术的高通量，低成本的优点。 表1：测序技术的比较第X代公司平台名称测序方法检测方法大约读长(碱基数)优点相对局限性第一代ABI/生命技术公司3130xL-3730xL桑格-毛细管电泳测序法荧光/光学600-1000高读长，准确度一次性达标率高，能很好处理重复序列和多聚序列通量低；样品制备成本高，使之难以做大量的平行测序第一代贝克曼GeXP遗传分析系统桑格-毛细管电泳测序法荧光/光学600-1000高读长，准确度一次性达标率高，能很好处理重复序列和多聚序列；易小型化通量低；单个样品的制备成本相对较高第二代Roche/454基因组测序仪FLX系统焦磷酸测序法光学230-400在第二代中最高读长；比第一代的测序通量大样品制备较难；难于处理重复和同种碱基多聚区域；试剂冲洗带来错误累积；仪器昂贵第二代IlluminaHiSeq2000,HiSeq2500/MiSeq可逆链终止物和合成测序法荧光/光学2x150很高测序通量仪器昂贵；用于数据删节和分析的费用很高第二代ABI/Solid5500xlSolid系统连接测序法荧光/光学25-35很高测序通量；在广为接受的几种第二代平台中，所要拼接出人类基因组的试剂成本最低测序运行时间长；读长短，造成成本高，数据分析困难和基因组拼接困难；仪器昂贵第二代赫利克斯Heliscope单分子合成测序法荧光/光学25-30高通量；在第二代中属于单分子性质的测序技术读长短，推高了测序成本，降低了基因组拼接的质量；仪器非常昂贵第三代太平洋生物科学公司PacBio RS实时单分子DNA测序荧光/光学~1000高平均读长，比第一代的测序时间降低；不需要扩增；最长单个读长接近3000碱基并不能高效地将DNA聚合酶加到测序阵列中；准确性一次性达标的机会低（81-83%）；DNA聚合酶在阵列中降解；总体上每个碱基测序成本高（仪器昂贵）；第三代全基因组学公司GeXP遗传分析系统复合探针锚杂交和连接技术荧光/光学10在第三代中通量最高；在所有测序技术中，用于拼接一个人基因组的试剂成本最低；每个测序步骤独立，使错误的累积变得最低低读长；&nbsp;模板制备妨碍长重复序列区域测序；样品制备费事；尚无商业化供应的仪器第三代Ion Torrent/生命技术公司个人基因组测序仪（PGM）&nbsp;合成测序法以离子敏感场效应晶体管检测pH值变化100-200对核酸碱基的掺入可直接测定；在自然条件下进行DNA合成（不需要使用修饰过的碱基）一步步的洗脱过程可导致错误累积；阅读高重复和同种多聚序列时有潜在困难；第三代牛津纳米孔公司&nbsp;gridION纳米孔外切酶测序电流尚未定量有潜力达到高读长；可以成本生产纳米孔；无需荧光标记或光学手段切断的核苷酸可能被读错方向；难于生产出带多重平行孔的装置&nbsp;&nbsp;表2：主流测序机器的成本测序比较 以下图10展示了当前全球测序仪的分布情况。图中的几个热点区主要分布在中国的深圳（主要是华大），南欧，西欧和美国。 图10. 测序仪全球分布 参考文献 1. Sanger, F. &amp; Nicklen, S. DNA sequencing with chain-terminating. 74, 5463&ndash;5467 (1977).2. Mardis, E. R. Next-generation DNA sequencing methods. Annual review of genomics and human genetics 9, 387&ndash;402 (2008).3. Shendure, J. &amp; Ji, H. Next-generation DNA sequencing. Nature biotechnology 26, 1135&ndash;45 (2008).4. Metzker, M. L. Sequencing technologies - the next generation. Nature reviews. Genetics 11, 31&ndash;46 (2010).5. Niedringhaus, T. P., Milanova, D., Kerby, M. B., Snyder, M. P. &amp; Barron, A. E. Landscape of Next-Generation Sequencing Technologies. 4327&ndash;4341 (2011).6. Rothberg, J. M. et al. An integrated semiconductor device enabling non-optical genome sequencing. Nature 475, 348&ndash;52 (2011).&nbsp; 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>测序技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测序量估计]]></title>
    <url>%2F2013%2F07%2F11%2F2013-07-11-Sequence-estimate.html</url>
    <content type="text"><![CDATA[考虑这样一个问题，如果要保证基因组上95%的区域其覆盖深度在30x以上的话，那么最低的平均测序深度应该是多少? 关于测序量的估计，对于做生物信息的人来讲应算是家常便饭了，多数时候我们都能直接根据以往项目的经验来获得，或是说的更具体些，在变异检测中一般要有25x以上的覆盖度才能得到一个比较靠谱的结果，于是以此为目的给出测序量的估计值；当然少数情况下也会有直接拍脑袋拍出一个值来的疯狂行为，不过嘛，虽说是拍脑袋，但也不是随便拍的，拍脑袋的背景靠的是身后丰富的经验。相对更好一些的估计方式就是直接模拟数据，不过总是用模拟数据还是让人觉得麻烦，最好是能不用花多少时间，也不用做很多的计算就能脱口给出。我想在这里说一下这种情况下我的解法。当然了并不一定完全准确，仅作交流，欢迎各位拍砖。 闲话说完，回到上面的问题，在不通过数据模拟也不拍脑袋的情况下，要如何才能估算出一个合理的值呢？其实在作出任何推断之前我们都应当要先有一个合理的前提假设，或者说是理论依据来作为后续分析的基础。我们都知道短序列测序的一个特点是，在理论情况下位点被覆盖到的深度符合泊松分布（测序没什么问题的话，实际的情形也相差不多），但实际上在这种情况下用正态分布来考虑也是合理的，作为一个估计值，误差也是能够接受的，这是我们的基础。之所以想用正态分布来考虑，是因为正态分布有许多方便于计算的性质。其中一个很有用的法则，就是68-95-99法则，意思就是距离均值一个标准差的区域围起来的面积大约是总体的68%，2个标准差的区域范围的面积是总体的95%，3个标准差区域范围占到了总体的99%，如果你自己想要验证这一法则也并不困难，只需做些积分就能算出来，但这里就不做计算了。如下图，均值用$\mu$表示，标准差用$\sigma$表示。 现在事情就很简单了，从图中我们可以看出，只要30x深度的位置在$-2\sigma$以下，那么就能达到理论的要求。要得到这一结果，问题就只剩下一个了，此时我们只需要知道测序深度分布的标准差就能粗略估计出此时我们所需要的最低平均测序深度。虽然这个标准差跟许多因素有关，这里以illumina公司的Hiseq系列测序仪为例子，依照以往基因组重测序的经验，$\sigma$约等于10x。那么，简单算一下，此刻，理论上我们只需要测50x就可以使得基因组上有97.7%的区域其覆盖深度在30x以上了，注意这里不是95%了，因为我们的区域实际上是$[-2\sigma, +\infty)$，而不是$[-2\sigma,+2\sigma]$! 再除掉一些边边角角的误差，50x这个值在这里应当是合理的了. 以上计算都是以正态分布为基础而做出的估计。当然了，如果一定要用泊松分布去推算也可以，只是运算起来会麻烦很多。此外，如果是不同系列或是不同公司的测序仪，&sigma;就不一定是10了。 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>测序技术</tag>
        <tag>估计</tag>
      </tags>
  </entry>
</search>
