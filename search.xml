<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[从零开始完整学习全基因组测序（WGS）数据分析：第2节 FASTA和FASTQ]]></title>
    <url>%2F2017%2F08%2F12%2F2017-08-12-Begining-WGS-Data-Analysis-Fasta-And-Fastq.html</url>
    <content type="text"><![CDATA[在WGS数据的分析过程中，我们会接触到许多生物信息学/基因组学领域所特有的数据文件和它们特殊的格式，在这一节中将要介绍的FASTA和FASTQ便是其中之一二。这是我们存储核苷酸序列信息（就是DNA序列）或者蛋白质序列信息最常使用的两种 文本文件，虽然看起来名字有些古怪，但它们完全是纯文本文件（如同.txt）！名字的发音分别是fast-A和fast-Q。这一篇文章内容虽然比较简单，但还是比较长，我在这里详细介绍了这两类文件的格式特点和一些在分析的时候需要考虑的地方。 FASTA我相信许多人（包括生物信息工程师们）一定不知道FASTA这个文件的来源，竟然是一款名叫“FASTA”的比对软件！名字中最后一个字母A，其实就是Alignment的意思！但这已经是上个世纪的事情了，最初是由William. R. Pearson 和 David. J. Lipman在1988年所编写，目的是用于生物序列数据的处理。 自那之后，生物学家和遗传学家们也没做过多的考虑，就草率地决定（其实类似的‘草率’行为在组学领域经常碰到）把FASTA作为这种存储 有顺序的序列数据的文件后缀【注】，这包括我们常用的参考基因组序列、蛋白质序列、编码DNA序列（coding DNA sequence，简称CDS）、转录本序列等文件都是如此，文件后缀除了.fasta之外，也常用.fa或者.fa.gz（gz压缩）。 【注】这里的序列、序列数据，指的其实就是表示DNA或者蛋白质的一条字符串。 这里再特别强调三个字：有！顺！序！说的是从1开始一个个按顺序往下排列的意思——这不也正是序列这个词的含义！ 因此，我们可以通过数个数，就知道某个DNA碱基在某个基因组上的准确位置，这个位置会用所在序列的名字和所在位置来表达，比如基因数据比对的结果（下一篇会介绍），方便后续数据分析。 FASTA文件主要由两个部分构成：序列头信息（有时包括一些其它的描述信息）和具体的序列数据。头信息独占一行，以大于号（&gt;）开头作为识别标记，其中除了记录该条序列的名字之外，有时候还会接上其它的信息。紧接的下一行是具体的序列内容，直到另一行碰到另一个大于号（&gt;）开头的新序列或者文件末尾。下面给出一个FASTA文件的例子，这是我们人类一个名为EGFR基因的部分序列。 12345678910&gt;ENSMUSG00000020122|ENSMUST00000138518CCCTCCTATCATGCTGTCAGTGTATCTCTAAATAGCACTCTCAACCCCCGTGAACTTGGTTATTAAAAACATGCCCAAAGTCTGGGAGCCAGGGCTGCAGGGAAATACCACAGCCTCAGTTCATCAAAACAGTTCATTGCCCAAAATGTTCTCAGCTGCAGCTTTCATGAGGTAACTCCAGGGCCCACCTGTTCTCTGGT&gt;ENSMUSG00000020122|ENSMUST00000125984GAGTCAGGTTGAAGCTGCCCTGAACACTACAGAGAAGAGAGGCCTTGGTGTCCTGTTGTCTCCAGAACCCCAATATGTCTTGTGAAGGGCACACAACCCCTCAAAGGGGTGTCACTTCTTCTGATCACTTTTGTTACTGTTTACTAACTGATCCTATGAATCACTGTGTCTTCTCAGAGGCCGTGAACCACGTCTGCAAT 可以看到，FASTA其实很简单，但它往往都很大，比如人类基因组有30亿个碱基，就是30亿个字符存储在这样的一个文本文件中，就算是压缩也要占用约1GB的存储空间。 另外，有两个地方，我觉得有必要提及： 第一，除了序列内容之外，FASTA的头信息并没有被严格地限制。这个特点有时会带来很多麻烦的事情，比如有时我们会看到相同的序列被不同的人处理之后、甚至是在不同的网站上或者数据库中它们的头信息都不尽相同，比如以下的几种情况都是可能存在的。 12345&gt;ENSMUSG00000020122|ENSMUST00000125984&gt; ENSMUSG00000020122|ENSMUST00000125984&gt;ENSMUSG00000020122|ENSMUST00000125984|epidermal growth factor receptor&gt;ENSMUSG00000020122|ENSMUST00000125984|Egfr&gt;ENSMUSG00000020122|ENSMUST00000125984|11|ENSFM00410000138465 这对于程序处理来说，凌乱的格式显然是不合适的。因此后来在业内也慢慢地有一些不成文的规则被大家所使用，那就是，用一个空格把头信息分为两个部分：第一部分是序列名字，它和大于号（&gt;）紧接在一起；第二部分是注释信息，这个可以没有，就看具体需要，比如下面这个序列例子，除了前面gene_00284728这个名字之外，注释信息（length=231;type=dna）给出这段序列的长度和它所属的序列类型。 12345&gt;gene_00284728 length=231;type=dnaGAGAACTGATTCTGTTACCGCAGGGCATTCGGATGTGCTAAGGTAGTAATCCATTATAAGTAACATGCGCGGAATATCCGGGAGGTCATAGTCGTAATGCATAATTATTCCCTCCCTCAGAAGGACTCCCTTGCGAGACGCCAATACCAAAGACTTTCGTAAGCTGGAACGATTGGACGGCCCAACCGGGGGGAGTCGGCTATACGTCTGATTGCTACGCCTGGACTTCTCTT 这对于程序处理来说，凌乱的格式显然是不合适的。因此后来在业内也慢慢地有一些不成文的规则被大家所使用，那就是，用一个空格把头信息分为两个部分：第一部分是序列名字，它和大于号（&gt;）紧接在一起；第二部分是注释信息，这个可以没有，就看具体需要，比如下面这个序列例子，除了前面gene_00284728这个名字之外，注释信息（length=231;type=dna）给出这段序列的长度和它所属的序列类型。 12345&gt;gene_00284728 length=231;type=dnaGAGAACTGATTCTGTTACCGCAGGGCATTCGGATGTGCTAAGGTAGTAATCCATTATAAGTAACATGCGCGGAATATCCGGGAGGTCATAGTCGTAATGCATAATTATTCCCTCCCTCAGAAGGACTCCCTTGCGAGACGCCAATACCAAAGACTTTCGTAAGCTGGAACGATTGGACGGCCCAACCGGGGGGAGTCGGCTATACGTCTGATTGCTACGCCTGGACTTCTCTT 虽然这样的格式还不算是真正的标准，但却非常有助于我们的数据分析和处理，很多生信软件（如：BWA，samtools，bcftools，bedtools等）都是将第一个空格前面的内容认定为序列名字来进行操作的。 第二，FASTA由于是文本文件，它里面的内容是否有重复是无法自检的，在使用之前需要我们进行额外的检查。这个检查倒不用很复杂，只需检查序列名字是否有重复即可。但对于那些已经成为标准使用的参考序列来说，都有专门的团队进行维护，因此不会出现这种内容重复的情况，可以直接使用，但对于其它的一些序列来说，谨慎起见，最好进行检查。 FASTQ这是目前存储测序数据最普遍、最公认的一个数据格式，另一个是uBam格式，但这篇文章中不打算对其进行介绍。上面所讲的FASTA文件，它所存的都是已经排列好的序列（如参考序列），FASTQ存的则是产生自测序仪的原始测序数据，它由测序的图像数据转换过来，也是文本文件，文件大小依照不同的测序量（或测序深度）而有很大差异，小的可能只有几M，大的则常常有几十G上百G，文件后缀通常都是.fastq，.fq或者.fq.gz（gz压缩），以下是它的一个例子： 12345678@DJB775P1:248:D0MDGACXX:7:1202:12362:49613TGCTTACTCTGCGTTGATACCACTGCTTAGATCGGAAGAGCACACGTCTGAA+JJJJJIIJJJJJJHIHHHGHFFFFFFCEEEEEDBD?DDDDDDBDDDABDDCA@DJB775P1:248:D0MDGACXX:7:1202:12782:49716CTCTGCGTTGATACCACTGCTTACTCTGCGTTGATACCACTGCTTAGATCGG+IIIIIIIIIIIIIIIHHHHHHFFFFFFEECCCCBCECCCCCCCCCCCCCCCC 你可以看到它有着自己独特的格式：每四行成为一个独立的单元，我们称之为read。具体的格式描述如下： 第一行：以‘@’开头，是这一条read的名字，这个字符串是根据测序时的状态信息转换过来的，中间不会有空格，它是 每一条read的唯一标识符，同一份FASTQ文件中不会重复出现，甚至不同的FASTQ文件里也不会有重复； 第二行：测序read的序列，由A，C，G，T和N这五种字母构成，这也是我们真正关心的DNA序列，N代表的是测序时那些无法被识别出来的碱基； 第三行：以‘+’开头，在旧版的FASTQ文件中会直接重复第一行的信息，但现在一般什么也不加（节省存储空间）； 第四行：测序read的质量值，这个和第二行的碱基信息一样重要，它描述的是每个测序碱基的可靠程度，用ASCII码表示。 那么，重点说一下什么是质量值？顾名思义，碱基质量值就是能够用来定量描述碱基 好坏程度的一个数值。它该如何才能恰当地描述这个结果呢？我们试想一下，如果测序测得越准确，这个碱基的质量就应该越高；反之，测得越不准确，质量值就应该越低。也就是说可以利用碱基被测错的概率来描述它的质量值，错误率越低，质量值就越高！如下图，红线代表错误率，蓝线代表质量值，这便是我们希望达到的效果： 这里我们假定碱基的测序错误率为: $${P}_{error}$$质量值为Q，它们之间的关系如下： $$Q=-10log_{10}{P}_{error}$$ 即，质量值是测序错误率的对数（10为底数）乘以-10（并取整）。这个公式也是目前测序质量值的计算公式，它非常简单，p_error的值和测序时的多个因素有关，体现为测序图像数据点的清晰程度，并由测序过程中的base calling 算法计算出来；公式右边的Q我们称之为Phred quality score，就是用它来描述测序碱基的靠谱程度。比如，如果该碱基的测序错误率是0.01，那么质量值就是20（俗称Q20），如果是0.001，那么质量值就是30（俗称Q30）。Q20和Q30的比例常常被我们用来评价某次测序结果的好坏，比例越高就越好。下面我也详细给出一个表，更进一步地解释质量值高低的含义： 测序平台 ASCII码范围 下限 质量值类型 质量值范围 备注 Sanger, Illumina(版本1.8及以上) 33-126 33 Phred quality score 0-93 现在沿用 Solexa, Illumina早期版本(&lt;1.3版本) 59-126 64 Solexa quality score 5-62 除了已测序数据之外，不再使用 Illumina(版本1.3-1.7) 64-126 64 Phred quality score 0-62 除了已测序数据之外，不再使用 现在回过头来说说为什么要用ASCII码来代表，直接用数字不行吗？行！但很难看，而且数字不能直接连起来，还得在中间加一个分隔符，长度也对不齐，还占空间，又不符合美学设计，真！麻！烦！ 因此，也是为了格式存储以及处理时的方便，这个数字被直接转换成了ASCII码，并与第二行的read序列构成一一对应的关系——每一个ASCII码都和它正上面的碱基对应，这就很完美。 不过，值得一提的是，ASCII码虽然能够从小到大表示0-127的整数，但是并非所有的ASCII码都是 可见的字符，比如所有小于33的ASCII码值所表示的都是不可见字符，比如空格，换行符等，因此 为了能够让碱基的质量值表达出来，必须避开所有这些不可见字符。最简单的做法就是加上一个固定的整数！也的确是这么干的。 但一开始对于要加哪一个整数，并没有什么指导标准，这就导致了在刚开始的时候，不同的测序平台加的整数也不同，总的来说有以下3种质量体系，演变到现在也基本只剩下第一种了，如下表： Phred Quality Score Probability of incorrect base call Base call accuracy 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9% 40 1 in 10,000 99.99% 50 1 in 100,000 99.999% 60 1 in 1,000,000 99.9999% 从表中可以看到下限有33和64两个值，我们把加33的的质量值体系称之为Phred33，加64的称之为Phred64（Solexa的除外，它叫Solexa64）。不过，现在一般都是使用Phred33这个体系，而且33也恰好是ASCII的第一个可见字符（’!’），完美+2。 如果你在实际做项目的过程不知道所用的质量体系（经验丰富者是可以直接看出来的），那么可以用我下面这一段代码，简单地做个检查: 123456789less $1 | head -n 1000 | awk '&#123;if(NR%4==0) printf("%s",$0);&#125;' \| od -A n -t u1 -v \| awk 'BEGIN&#123;min=100;max=0;&#125; \ &#123;for(i=1;i&lt;=NF;i++) &#123;if($i&gt;max) max=$i; if($i&lt;min) min=$i;&#125;&#125;END \ &#123;if(max&lt;=126 &amp;&amp; min&lt;59) print "Phred33"; \ else if(max&gt;73 &amp;&amp; min&gt;=64) print "Phred64"; \ else if(min&gt;=59 &amp;&amp; min&lt;64 &amp;&amp; max&gt;73) print "Solexa64"; \ else print "Unknown score encoding"; \ print "( " min ", " max, ")";&#125;' 将上面这段代码复制到任意一份shell文件中（比如：fq_qual_type.sh），就可以用它来进行质量值类型的检查了。代码的思路其实比较简单，就是截取FASTQ文件的前1000行数据，并抽取出质量值所在的行，分别计算出其中最小和最大的ASCII值，再比较一下就判断出来了。下面给出一个例子，这是我们在本文中用到的FASTQ文件，它是Phred33的： 123$ sh fq_qual_type.sh untreated.fqPhred33( 34, 67 ) 另外，在查看碱基质量值的过程中，如果你心中存有ASCII码表当然可以直接“看”出各个碱基的质量值，但在实际的场景中都是通过程序直接进行转换处理。下面我就用Python的ord()函数举个转换的例子： 123456In [1]: qual='JJJJJIIJJJJJJHIHHHGHFFFFFFCEEEEEDBD'In [2]: [ord(q)-33 for q in qual]Out[2]:[35, 20, 17, 18, 24, 34, 35, 35, 35, 34, 35, 34, 29, 29, 32, 32, 34, 34, 33, 29, 33, 33, 32, 35, 35, 35, 34, 34, 34, 34, 35, 35, 34, 35, 34, 35, 34, 35, 34, 34, 34, 35, 35, 35, 35, 34, 33, 33, 30, 33, 24, 27] 这里的ord()函数会将字符转换为ASCII对应的数字，减掉33后就得到了该碱基最后的质量值（即，Phred quality score）。 另外，根据上面phred quality score的计算公式，我们可以很方便地获得每个测序碱基的错误率，这个错误率在我们的比对和变异检测中都十分重要，后续文章中我将会讲述该部分的具体内容，以下先给出一个转换的例子，还是以上述qual为例子： 12345678910In [1]: qual='JJJJJIIJJJJJJHIHHHGHFFFFFFCEEEEEDBD'In [2]: phred_score = [ord(q)-33 for q in qual]In [3]: [10**(-q/10.0) for q in phred_score]Out[3]:[3e-04, 1e-02, 2e-02, 2e-02, 4e-03, 4e-04, 3e-04, 3e-04, 3e-04, 4e-04, 3e-04, 4e-04, 1e-03, 1e-03, 6e-04, 6e-04, 4e-04, 4e-04, 5e-04, 1e-03, 5e-04, 5e-04, 6e-04, 3e-04, 3e-04, 3e-04, 4e-04, 4e-04, 4e-04, 4e-04, 3e-04, 3e-04, 4e-04, 3e-04, 4e-04, 3e-04, 4e-04, 3e-04, 4e-04, 4e-04, 4e-04, 3e-04, 3e-04, 3e-04, 3e-04, 4e-04, 5e-04, 5e-04, 1e-03, 5e-04, 4e-03, 2e-03] 这其实就是根据phred quality score的定义进行简单的指数运算。 小结到这里就说完了，虽然一开始只不过是想介绍两个普通的文件格式，但写着写着就变得很长，可见，越是看似简单的东西，其实越不容易说明白。关于FASTQ还有很多需要说的内容，我打算将其留到该系列的第四篇文章里，到时我会讲述该如何构造流程对其进行有效的数据质控等，这都是构造WGS分析流程之前非常重要的内容。 我一直觉得，生物信息学（或者说基因组学）中的许多数据文件，它们的格式都有着比较特殊的一面，为了能够真正有效地进行数据分析，多花些时间搞清楚它们的细节和来龙去脉是非常重要的。不然，你有可能在后续的数据分析过程掉入意想不到的陷阱，从而浪费大量宝贵的时间去寻找可能出错的地方。 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>WGS</tag>
        <tag>数据格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始完整学习全基因组测序（WGS）数据分析：第1节 测序技术]]></title>
    <url>%2F2017%2F08%2F04%2F2017-08-04-Begining-WGS-Data-Analysis-Sequecing-Tech.html</url>
    <content type="text"><![CDATA[前言 基因测序已是时下热门，目前除了华大基因之外，其他分布于全中国的大型测序平台（HiSeq X 10）还有约10个，每个每年大概能完成1.8万人的高深度全基因组测序，加起来就是18万人，如果加上华大，可能需要翻倍！而且随着新技术的快速发展和成本的下降，WGS正变得越来越普遍！再加上国家十三五规划已经提出了构建大规模中国人群遗传队列图谱的要求，全基因组测序技术正在逐渐替代其它测序手段，这也是我打算写这一个系列的原因。 HiSeq X 10分布（来源：转化医学网） 首先，全基因组测序的英文是Whole Genome Sequencing，简称WGS，目前默认指的是人类的全基因组测序。所谓全（Whole），指的就是 把物种细胞里面完整的基因组序列从第1个DNA开始一直到最后一个DNA，完完整整地检测出来，并排列好，因此这个技术几乎能够鉴定出基因组上任何类型的突变。对于人类来说，全基因组测序的价值是极大的，它的信息包含了所有基因和生命特征之间的内在关联性，当然也意味着更大的数据解读和更高的技术挑战。但，没关系，在这个系列中，我将从测序技术、常用文件解析，数据质控和流程构建等各个方面结合实际的例子，详细阐述什么是全基因组测序以及 该如何构造流程 分析全基因组测序（WGS）数据。 这是这一组学入门技术系列的第一篇（这篇文章修改自我以前的一篇博客，该文也已被各种形式转载），我首先将介绍当前的基因组测序原理及其发展历程。 第一节 NGS测序技术 在真正开始数据分析之前先知道我们是如何将那些原本存在于细胞中的DNA信息获取出来的——也就是测序的原理，总是有益的。 测序，简单来说就是将DNA化学信号转变为计算机可处理的数字信号。 它从1977年的第一代Sanger技术发展至今，已经足有40年时间。在这个技术发展的更迭历程中，测序读长从长到短，再从短到长。虽然就当前形势看第二代短读长测序技术在全球范围内上占有着绝对的垄断位置，但第三测序技术也已在这几年快速地发展着。测序技术的每一次变革和突破，都对基因组学研究，疾病医疗研究，药物研发，育种等领域产生巨大的推动作用。所以在这个系列的第一篇里我将对当前最主流的测序技术以及它们的测序原理做一个全面的介绍。 图1. 测序技术发展历程 第一代测序技术 第一代DNA测序技术用的是1975年由桑格（Sanger）和考尔森（Coulson）开创的链终止法或者是1976-1977年由马克西姆（Maxam）和吉尔伯特（Gilbert）发明的化学法（链降解）. 并在1977年，由桑格老人家测定了第一个基因组序列——噬菌体phiX-174，全长只有5,375个碱基。虽然与今日的技术比起来根本不算什么，但自此之后，人类获得了窥探生命本质的能力，并以此为开端真正步入了基因组学时代。 研究人员在Sanger法的多年实践之中不断对其进行改进。在2001年，完成的首个人类基因组图谱就是以改进了的Sanger法为基础进行测序的。Sanger法的核心原理是：由于ddNTP（4种带有荧光标记的A,C,G,T碱基）的2’和3’都不含羟基，其在DNA的合成过程中不能形成磷酸二酯键，因此可以用来中断DNA的合成反应，在4个DNA合成反应体系中分别加入一定比例带有放射性同位素标记的ddNTP（分别为：ddATP,ddCTP,ddGTP和ddTTP），然后利用凝胶电泳和放射自显影后可以根据电泳带的位置确定待测分子的DNA序列（图2）。这个网址为Sanger测序法制作了一个小短片，形象而生动。值得注意的是，在测序技术起步发展的这一时期中，除了Sanger法之外还出现了一些其他的测序技术，如焦磷酸测序法、连接酶法等。其中，焦磷酸测序法是后来Roche公司454技术所使用的测序方法，而连接酶测序法是后来ABI公司SOLID使用的测序方法，但他们的核心手段都是利用了Sanger中可中断DNA合成反应的dNTP。 图2. Sanger测序发原理 第二代测序技术 总的来说，第一代测序技术的主要特点是测序读长可达1,000bp，准确性高达99.999%，但其测序成本高，通量低等方面的缺点，严重影响了其真正大规模的应用。因而第一代测序技术并不是理想的测序方法。经过不断的技术开发和改进，以Roche公司的454技术、illumina公司的Solexa/HiSeq技术和ABI公司的SOLID技术为标记的第二代测序技术诞生了。第二代测序技术在大幅提高了测序速度的同时，还大大地降低了测序成本，并且保持了高准确性，以前完成一个人类基因组的测序需要3年时间，而使用二代测序技术则仅仅需要1周，但其序列读长方面比起第一代测序技术则要短很多，大多只有100bp-150bp。图3. 是第一代和第二代测序技术测序成本作了一个简单的比较，可以看出自第二代测序技术发展出来之后，历史开始发生根本性的改变，测序的成本开始快速实现断崖式下降，也就是业内经常提到的 超摩尔定律 现象。 图3. 测序成本比较（来源：NIH网站） 接下来我以illumina（目前最大、最成功的NGS测序仪公司）的技术为基础简要单介绍第二代测序测序技术的原理和特点。 目前illumina的测序仪占全球75%以上，以HiSeq系列为主。它的机器采用的都是边合成边测序的方法，主要分为以下4个步骤： 图4. illumina测序原理（来源：illumina官网） 1）构建DNA测序文库，图4-1 简单来说就是把一堆乱糟糟的DNA分子用超声波打断成一定长度范围的小片段。目前除了一些特殊的需求之外，基本都是打断为300bp-800bp长的序列片段，并在这些小片段的两端添加上不同的接头【注】，构建出单链DNA文库，以备测序之用； 【注】接头在illumina中一般分为P5和P7接头，其中一个带有和flowcell上的探针反向互补的序列，以完成待测序列和探针结合的作用，另外一个接头带有barcord序列以区分不同的样本。 2）测序流动槽（flowcell），图4-2 flowcell是用于吸附流动DNA片段的槽道，也是核心的测序反应容器——所有的测序过程就发生在这里。当文库建好后，这些文库中的DNA在通过flowcell的时候会随机附着在flowcell表面的槽道（称为lane）上。每个flowcell有8个lane（图5），每个lane的表面都附有很多接头，这些接头能和建库过程中加在DNA片段两端的接头相互配对，这就是为什么flowcell能吸附建库后的DNA的原因，并能支持DNA在其表面进行桥式PCR的扩增，理论上这些lane之间是不会相互影响的。 图5. flowcell（实物 VS 示意图） 3）桥式PCR扩增与变性 图6. 桥式PCR扩增（来源：illumina官网） 这是NGS技术的一个核心特点。桥式PCR以flowcell表面所固定的序列为模板，进行桥形扩增，如图6所示。经过不断的扩增和变性循环，最终每个DNA片段都将在各自的位置上集中成束，每一个束都含有单个DNA模板的很多分拷贝，这一过程的目的在于实现将单一碱基的信号强度进行放大，以达到测序所需的信号要求。 4）测序，如图4-4和图7所示 图7. 边合成边测序（来源：illumina官网） 测序方法采用边合成边测序的方法。向反应体系中同时添加DNA聚合酶、接头引物和带有碱基特异荧光标记的4中dNTP（如同Sanger测序法）。这些dNTP的3’-OH被化学方法所保护，因而每次只能添加一个dNTP，这就确保了在测序过程中，一次只会被添加一个碱基。同时在dNTP被添加到合成链上后，所有未使用的游离dNTP和DNA聚合酶会被洗脱掉。接着，再加入激发荧光所需的缓冲液，用激光激发荧光信号（图7），并有光学设备完成荧光信号的记录，最后利用计算机分析将光学信号转化为测序碱基。这样荧光信号记录完成后，再加入化学试剂淬灭荧光信号并去除dNTP 3’-OH保护基团，以便能进行下一轮的测序反应。 Illumina的这种每次只添加一个dNTP的技术特点能够很好的地解决同聚物长度的准确测量问题，它的主要测序错误来源是碱基的替换，目前它的测序错误率在1%-1.5%左右。测序周期以人类基因组重测序为例，30x-50x测序深度对于Hisq系列需要3-5天时间，而对于2017年初最新推出的NovaSeq系列则只需要40个小时！ 表1. 测序量比较（双流动槽为例，如为单流动槽则测序量减少为下表的一半，时间不变）一次测序的数据总产量的单位Gb，不是计算机字节，而是测序碱基的数目（Giga base） 图8. NovaSeq与其他测序仪测序通量的比较（来源：illumina官网） 上面表1和图8是NovaSeq和其他测序系列的比较，数据相当好。按照这个数据量估算，一台NovaSeq 6000（S4）在跑满的情况下，一年就可以测序6400多人！而且按照以往的经验，illumina的官方公布的数据都是偏于保守的，我们在实际的使用过程中发现 高质量（Q30）的read其实占到了总数据的90%以上，远高于官方公布的75%，数据的总产量也同样更高。 第三代测序技术 这是一个新的里程碑。以PacBio公司的SMRT和Oxford Nanopore Technologies的纳米孔单分子测序技术为标志，被称之为第三代测序技术。与前两代相比，最大的特点就是 单分子测序，测序过程无需进行PCR扩增，超长读长，以下图9是PacBio SMRT技术的测序读长分布情况，平均达到10Kb-15Kb，是二代测序技术的100倍以上，值得注意的是在测序过程中这些序列的读长也不再是相等的，下文有解析！ 图9. PacBio SMRT 测序read读长分布（来源：PacBio官网） PacBio SMRT PacBio SMRT技术其实也应用了边合成边测序的思想，并以SMRT芯片为测序载体（如同flowcell）。基本原理是： DNA聚合酶和模板结合，用4色荧光标记A,C,G,T这4种碱基（即是dNTP）。在碱基的配对阶段，不同的碱基加入，会发出不同的光，根据光的波长与峰值可判断进入的碱基类型。 图10. PacBio SMRT 测序原理 这个DNA聚合酶是实现超长读长的关键之一，读长主要跟酶的活性保持有关，它主要受激光对其造成的损伤所影响。PacBio SMRT技术的一个关键点是在于如何将反应信号与周围游离碱基的强大荧光背景区别出来。他们利用的是ZMW（零模波导孔）原理：如同微波炉壁上可看到的很多密集小孔。这些小孔的直径是有严格要求的，如果直径大于微波波长，能量就会在衍射效应的作用下穿透面板从而泄露出来（光波的衍射效应），从而与周围小孔相互干扰（光波的干涉）。如果孔径能够小于波长，那么能量就不会辐射到周围，而是保持直线状态，从而可起到保护的作用。同理，在一个反应管(SMRTCell:单分子实时反应孔)中有许多这样的圆形纳米小孔,，即 ZMW(零模波导孔)，外径100多纳米，比检测激光波长小(数百纳米)，激光从底部打上去后不会穿透小孔进入上方的溶液区，能量会被限制在一个小范围(体积20X 10-21 L)里（图10-A），正好足够覆盖需要检测的部分，使得信号仅仅只是来自于这个小反应区域，孔外过多的游离核苷酸单体依然留在黑暗中，从而实现将背景噪音降到最低的目的。 PacBio SMRT技术除了能够检测普通的碱基之外，还可以通过检测相邻两个碱基之间的测序时间，来检测碱基的表观修饰情况，如甲基化。因为假设某个碱基存在表观修饰，则通过聚合酶时的速度会减慢，那么相邻两峰之间的距离会增大，我们可以通过这个时间上的差异来检测表观甲基化修饰等信息（图11）。 图11. PacBio SMRT 检测甲基化修饰（来源：PacBio官网） SMRT技术的测序速度很快，每秒约10个dNTP。但这么快的测序速度也带来了一些明显的缺点——测序错误率比较高（这几乎是目前单分子测序技术的通病），可以达到10%-15%，而且以缺失序列和错位居多，但好在它的出错是随机的，并不会像第二代测序技术那样存在一定的碱基偏向，因此可以通过多次测序来进行有效纠错。 Oxford Nanopore Oxford Nanopore 的MinION是另一个比较受关注的第三代测序仪，俗称U盘测序仪，它真的很小，我亲手拿过，并拆过，图12（左）！这家公司开发的纳米单分子测序技术与以往的测序技术相比都不一样，它是基于电信号而不是光信号的测序技术！ 图12. Oxford Nanopore MinION 这个技术的关键点在于他们所设计的一种特殊纳米孔，孔内共价结合分子接头。当DNA分子通过纳米孔时，它们使电荷发生变化，从而短暂地影响流过纳米孔的电流强度（每种碱基所影响的电流变化幅度是不同的），最后高灵敏度的电子设备检测到这些变化从而鉴定所通过的碱基（图13）。 图13. MinION 测序原理 纳米孔测序以及其他第三代测序技术，有可能会彻底地解决目前第二代测序平台的诸多不足。另外，MinION的主要特点是：读长很长，而且比PacBio的都长得多，基本都是在几十kb上百kb以上，最新的数据显示可以达到900 kb！错误率是5%-15%，也是随机错误，MinION最大的特点除了极小的体积之外，就是数据将是可实时读取的，并且起始DNA在测序过程中不被破坏！这真是个可以上天的能力。然鹅，遗憾地多说几句，目前还没真正公布，细节也不知，自从2012开过一次发布会之后，就没什么声响了。 这种纳米孔单分子测序仪还有另一大特点，它能够 直接 读取出甲基化的胞嘧啶，而不必像二代测序方法那样需要事先对基因组进行bisulfite处理。这对于在基因组水平直接研究表观遗传相关现象有极大的帮助。下面是对PacBio和Oxford Nanopore这两家第三代测序技术公司的测序仪做的一个简单比较，可以看出其实成本还是蛮高的，质量也只是还行，期待他们的下一次进化吧。 总结 以上，便是对各代测序技术的原理做了简要的阐述。在这个比较的过程中，可以看到测序成本，读长和通量是该测序技术先进与否的三个重要指标。其实第一代和第二代测序技术除了通量和成本上的差异之外，测序的核心原理都来自于边合成边测序的思想。第二代测序技术的优点是通量大大提升，成本大大减低，使得昔日王榭堂前燕，可以飞入寻常百姓家。总之，只有变成白菜价，才能真正对大众有意义；但它的缺点是所引入PCR过程会在一定程度上增加测序的错误率，并且具有系统偏向性，同时读长也比较短。第三代测序技术是为了解决第二代所存在的缺点而开发的，它的根本特点是单分子测序，不需要任何PCR的过程，这是为了能有效避免因PCR偏向性而导致的系统错误，同时提高读长，但这个技术还不是很成熟，需要再进化，成本也偏高。 图14. 全球测序仪数量分布 参考文献 Sanger, F. &amp; Nicklen, S. DNA sequencing with chain-terminating. 74, 5463–5467 (1977). Mardis, E. R. Next-generation DNA sequencing methods. Annual review of genomics and human genetics 9, 387–402 (2008). Shendure, J. &amp; Ji, H. Next-generation DNA sequencing. Nature biotechnology 26, 1135–45 (2008). Metzker, M. L. Sequencing technologies - the next generation. Nature reviews. Genetics 11, 31–46 (2010). Niedringhaus, T. P., Milanova, D., Kerby, M. B., Snyder, M. P. &amp; Barron, A. E. Landscape of Next-Generation Sequencing Technologies. 4327–4341 (2011). Rothberg, J. M. et al. An integrated semiconductor device enabling non-optical genome sequencing. Nature 475, 348–52 (2011). 欢迎通过我的公众号（解螺旋的矿工），更及时了解更多信息]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>测序技术</tag>
        <tag>WGS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于深度学习的序列模型预测非编码区变异的功能效应]]></title>
    <url>%2F2017%2F05%2F30%2FPredicting-effects-of-noncoding-variants-with-deep-learning-based-sequence-model.html</url>
    <content type="text"><![CDATA[Deep Learning，现在几乎到处都能看到它的应用。看！紧随DeepBind，在基因组学应用中又来了一个DeepSEA——这是一个适用于表观遗传研究和应用的工具，它只从DNA序列出发，并没用其他有关于表观研究的实验或者测序技术，通过直接输入fasta sequence，vcf或者bed文件，就可以预测转录因子结合位点(Transcription factors binding site), DNase I超敏感位点（DNase I hypersensitive sites）和组蛋白靶点（histone marks），这么多年来，这样的做法还是头一回。下面这张示意图展示的是各个主要的表观修饰在染色体中的位置和相关实验测定技术。 为什么要有这么个东西呢？ 众所周知，人类基因组上绝大部分的序列都是非编码序列——不直接编码蛋白质的序列，这些序列在很长的一段时间里都被误解为所谓的“垃圾DNA”！但其实它们各自都有着独特的作用——调控着机体的正常运作，只是要想正确地理解它们确实不是一个容易的事情。DeepSEA想要干的就是尝试从序列的基础功能预测着手去解决这么一个难题。 它先通过学习大量已知的染色质修饰数据——主要来自于ENCODE和Roadmap Epigenomics等大型项目，经过不断的训练，学习到了许多种在非编码区域中序列调控的序列模式或者说是序列特征（注意是序列模式，不是功能模式），之后，便可以通过这些模式和特征去预测序列上单碱基的突变会如何影响染色质的修饰功能。从发表的文章来看，其精确程度是目前所有方案中最高也是在同等数据下最有效的了。 DeepSEA 在Nature Method的原文http://www.nature.com/nmeth/journal/v12/n10/full/nmeth.3547.html更赞的是它的代码和相关训练数据都一起公开在网站上：http://deepsea.princeton.edu/ 可以尝试玩起来了。 【注】本文首发于泛基因和公众号。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>非编码区</tag>
        <tag>深度学习</tag>
        <tag>功能预测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GATK中如何计算Inbreeding coefficient（近交系数）]]></title>
    <url>%2F2017%2F05%2F28%2Fhow-to-calculate-inbreading-coefficient-in-GATK.html</url>
    <content type="text"><![CDATA[关于近交系数是什么的定义，除了英文资料，中文上也给出了清晰的定义，这里引用一下： 近交系数（inbreeding coefficient）是指根据近亲交配的世代数，将基因的纯化程度用百分数来表示即为近交系数，也指个体由于近交而造成异质基因减少时，同质基因或纯合子所占的百分比也叫近交系数，普遍以F或f来表示。 GATK近交系数的计算程序在github上可以找到：AS_InbreedingCoeff.java 代码不短，但计算很简单，我主要说展示一下这个计算的核心部分并在代码中做些注释，如下： 1234567891011121314151617181920212223242526protected double calculateIC(final VariantContext vc, final Allele altAllele) &#123; final int AN = vc.getCalledChrCount(); final double altAF; final double hetCount = heterozygosityUtils.getHetCount(vc, altAllele); final double F; //shortcut to get a value closer to the non-alleleSpecific value for bialleleics if (vc.isBiallelic()) &#123; double refAC = heterozygosityUtils.getAlleleCount(vc, vc.getReference()); double altAC = heterozygosityUtils.getAlleleCount(vc, altAllele); double refAF = refAC/(altAC+refAC); altAF = 1 - refAF; F = 1.0 - (hetCount / (2.0 * refAF * altAF * (double) heterozygosityUtils.getSampleCount())); // inbreeding coefficient &#125; else &#123; //compare number of hets for this allele (and any other second allele) with the expectation based on AFs //derive the altAF from the likelihoods to account for any accumulation of fractional counts from non-primary likelihoods, //e.g. for a GQ10 variant, the probability of the call will be ~0.9 and the second best call will be ~0.1 so adding up those 0.1s for het counts can dramatically change the AF compared with integer counts altAF = heterozygosityUtils.getAlleleCount(vc, altAllele)/ (double) AN; // 计算inbreeding coefficient F = 1.0 - (hetCount / (2.0 * (1 - altAF) * altAF * (double) heterozygosityUtils.getSampleCount())); // heterozygosityUtils.getSampleCount() 获取总样本数 &#125; return F;&#125; 总的来说，是利用哈迪温伯格定律来计算的。 1.0 - (hetCount / (2.0 (1 - altAF) altAF(double)N ，N是人数。这个值给出的是期望的杂合变异的个数。所以参数F说的就是”实际的hetCount”除以”期望的hetCount”再与1.0取差。当F值越接近0，就意味着实际的hetCount与理论的hetCount越接近。]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>GATK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[癌症基因组学研究全程回顾]]></title>
    <url>%2F2017%2F01%2F02%2F2017-01-02-cancer-genomics-review.html</url>
    <content type="text"><![CDATA[【注】本文同时发于泛基因fungenomics以及微信公众号。 癌症，一种慢性的基因病。 简介在癌症研究中，每个癌症样品呈现在研究人员眼前的已经是一个发生了改变的基因组，其中包含着独特且难以预测的诸多点突变、序列的插入缺失、易位、融合以及其他畸变。并且，这些发生的变异中，许多往往都是之前所未观察到的（Novel mutations），它们也不会只存在于基因组的编码区域中，因而为了能够真正做到全面研究癌症基因组本身所发生的所有突变事件，全基因组测序已被视为肿瘤基因变异研究中 唯一 严谨的方法。 然而，在所有这些变异中，却只有少数的几个主导着癌症这一疾病的发展和演变。要有效揭示这一演变和发展的过程，就需要监控基因表达水平上的变化，那么RNA-Seq便是用以确定这些遗传改变是否会影响疾病发展有用技术。但遗传改变有可能影响所有的细胞过程，包括染色质结构、DNA甲基化、RNA剪接异构体、RNA编辑和microRNA（miRNA）等。这就意味着，只有对所有这些独立的过程都进行检测和综合分析，才能在癌症研究中取得真正的突破。这些内容我们都将在下文一一展开。 当前基因组测序技术的一大特点是在于能够在很短的时间内并行测得数十亿（甚至数百亿）的独立序列片段——read，而每个read来源于单个的DNA分子。由此产生的数据我们可将其视为是对DNA分子的随机抽样，这反过来代表肿瘤样品中每个细胞的基因组的情况。这是我们解开癌症的原因和机制的一种强大工具。 肿瘤异质性癌症基因组的突变是复杂的。 每个人都携带一套独特的来自父母遗传的胚系突变（germline mutations）信息。但随着癌症的发展，体细胞突变（Somatic mutations）和基因组重排（genomics rearrangements）会逐渐增加。这些改变往往会引发耐药性以及转移。越来越多的研究证据表明，这些过程竟然可以是 有意的！——它们其实可以认为是癌细胞面临药物刺激的过程中不断进化的结果。我们想要全面地理解这一复发和耐药性的原因，就有必要进行纵向实验，按照癌症的发展过程，分阶段采集样品来进行研究。 如上图，这是处于正常组织背景下的多克隆肿瘤（polyclone tumor）。大多数的肿瘤样品都会同时包含肿瘤细胞和正常细胞，如基质细胞、血管和免疫细胞。并且肿瘤本身也常常包含几种不同的克隆亚型（clone types），每一种都有着不同的治疗反应和复发可能。 根据传统病理学的估计，大部分研究的结果其实都只集中在那些肿瘤细胞比例 &gt;60% 的区域中。并且为了确定哪些突变是肿瘤特异的，通常都要包含来自同一个体的正常组织样本作为参考。 然而，肿瘤本身也往往是异质性的。在癌症发展的过程中，个别细胞会发生新的突变，包含这些新突变的细胞会继续增殖，形成克隆亚型。因此，对于晚期癌症我们通常检测到的都是一个多克隆肿瘤，其中每一个克隆有一套独特的突变信息、独特的病理学和药物反应机制。这其实也正是癌症难以完全治疗的原因，而它的这种 异质性其实正是肿瘤基因组复杂性导致的一种表型性质。目前的深度测序可以检测样本中含量低至1%的克隆。 肿瘤内的异质性。体细胞突变的逐步累积产生了一个异质的多克隆肿瘤，其中不同克隆可能对治疗的反应不同。 而且，异质性一般还可以分两种情况讨论，第一种情况指的是：同一个病人的肿瘤细胞具有异质性。处于肿瘤发生的不同时期的肿瘤细胞的基因突变情况不同，造就了每一个肿瘤细胞群体内还有许多亚群（subclones），肿瘤细胞在通过转移时，就会有属于不同亚群的肿瘤细胞去侵入新的地方，形成新的肿瘤；第二种情况指的是：除了同一病人的不同肿瘤细胞会造成肿瘤的异质性外，肿瘤的异质性还体现在不同病人可能得了相同的肿瘤，但是那个『相同』未必真是相同——仅仅是表型相同，不代表着基因型也相同。 在某些基因中，突变频繁发生在同一个位置，这应该有特定的机制在起作用，这些有规律性的情况都会稍微容易对付。然而遗憾的是，对于大多数的基因，突变显然是随机出现在整个基因中的，这其实说明了DNA的复制和修复机制失灵了。 图中为两个假定的基因，他们有着两种不同的突变模式。深灰色框代表外显子组，而红色柱子代表存在突变的位置。A: 特定位置的频发突变可能表示有产生突变的生物学机制的参与。B: 散发突变存在于整个基因中，如P53，可能是由于复制和修复机制的失效。我们可以通过测序检测这两种情况下的突变。 参考文献 2012年Ding Li等人发表在Natrue上的一项研究弄清了急性髓细胞白血病（AML）的复发原因。他们发现了两个一般机理：（1）原发肿瘤中的始发克隆（founding clone）获得突变，进化成复发克隆；（2）始发克隆的一个亚克隆挺过了初次治疗，获得了更多突变并在复发时增殖。在一个病例中，原发肿瘤里原本仅占5.1%的亚克隆在复发后却成长为主要的克隆。而且对于所有的病例，化疗并不能够根除这些始发克隆。这项研究直接表明了在诊断后和初次治疗后检测并根除掉那些原本不起眼的小细胞群体有多么的重要！Ding L., Ley T. J., Larson D. E., Miller C. A., Koboldt D. C., et al. (2012) Clonal evolution in relapsed acute myeloid leukaemia revealed by whole-genome sequencing. Nature 481: 506-510 同样是2012年，同样是Ding Li这波人，同样是研究AML，他们这次发现大约三分之一的骨髓增生异常综合征患者会发展成继发性急性髓细胞白血病(AML)。这个研究的目的是为了确定骨髓增生异常综合征中的突变，它们有可能预测AML的进展。他们对七名继发性AML患者的七组皮肤和骨髓样品以及先前的骨髓增生异常综合征的配对骨髓样品一并做了全基因组测序。结果发现，在所有病例中，占主导地位的继发性AML克隆都是来自一个骨髓增生异常综合征的始发克隆。这其实就是说，骨髓增生异常综合征样品包含了对预后很重要的突变，针对这些突变的治疗方案是可能改善预后的。Walter M. J., Shen D., Ding L., Shao J., Koboldt D. C., et al. (2012) Clonal architecture of secondary acute myeloid leukemia. N Engl J Med 366: 1090-1098 继续肿瘤治疗和预后的话题，Gerlinger M这一波人发起了一项研究，利用全外显子组测序来研究多个样品，这些样品不仅来自两名患者体内的原发肾癌，还包括了患者相关转移部位的空间分隔区域。最后，他们在原发肿瘤中看到了广泛存在的异质性现象！而且还特别指出了在每个肿瘤区域内有63%-69%的体细胞突变是无法检测到的。同时，还检测了同一肿瘤不同区域内预后良好和不良的基因表达特征。这其实还是突出了在突变积累之前早期诊断的重要性，以及对较大肿瘤需要进行多个部位的活检。利用同一名患者的多个样本得到的信息，他们能够重建疾病的发展进程！这是一种极为强大的方法，不仅能检测引发事件，还能检测表现出平行进化的基因。平行进化通常是基因在进化压力下的一种表现，其实也表示那些基因可能成为有效的治疗靶点。Gerlinger M., Rowan A. J., Horswell S., Larkin J., Endesfelder D., et al. (2012) Intratumor heterogeneity and branched evolution revealed by multiregion sequencing. N Engl J Med 366: 883- 892。 转移肿瘤的转移是一个复杂的过程，其中癌细胞脱离原发肿瘤，通过血液或者淋巴系统循环到身体的其他部位。在新部位，细胞继续繁殖，最终形成更多肿瘤，这些肿瘤包含了反映其组织来源的细胞。肿瘤（胰腺癌和葡萄膜肿瘤）转移的能力大大增加了它们的致死性。关于转移肿瘤的克隆结构、转移酶之间的系统发育关系、转移和原发部位的平行进化规模，肿瘤如何扩散，以及肿瘤微环境在转移部位决定中的作用如何等，许多这些基本问题目前仍然没有很好地解决。 转移瘤可能来源于原发肿瘤中一个主要克隆（如上图：Metastasis1），也可能来源于次要克隆（Metastasis2）。转移瘤也会经历克隆进化（如Metastasis1所示） 参考文献 Hsieh A. C., Liu Y., Edlind M. P., Ingolia N. T., Janes M. R., et al. (2012) The translational landscape of mTOR signalling steers cancer initiation and metastasis. Nature 485: 55-61 这篇文章证明了前列腺癌基因组经由致癌mTOR通路的特殊翻译机制，这其中产生了一个特定的基因群，它们参与了细胞增殖、代谢和侵袭。随后作者对一类翻译控制前侵袭信使RNA进行了功能鉴定，并指出了这些mRNA主导了癌症侵袭和转移。 基因组突变所有的肿瘤在其发展的过程中都会不断积累体细胞突变（somatic mutations）。大多数常见的肿瘤与不同的癌基因相关联，这些癌基因以低频率发生突变。从大型癌症数据库中观察到的一个最令人惊讶的现象是癌症间甚至各个癌症类型内的显著遗传异质性。然而，似乎只有有限的细胞通路对肿瘤的细胞生物学很重要。目前很多人正在编辑收录各种癌症类型的体细胞突变综合列表，这对于更好地了解这种疾病背后的机制将有很大的指引作用。 研究参考 Nik-Zainal S., Alexandrov L. B., Wedge D. C., Van Loo P., Greenman C. D., et al. Mutational processes molding the genomes of 21 breast cancers. Cell 149: 979-993这篇文章中研究了21个乳腺癌基因组，并给出了它的一个体细胞突变列表。发现带BRCA1或者BRCA2突变的癌症会有一种特别的替换突变特征和与众不同的缺失图谱。文章中还描述了一种局部的超突变现象，这称为『kataegis』(kataegis，希腊语中『雷雨』的意思，文中指的是在一个小区域中出现大量突变的机制，如下图)。并且这些区域中的碱基替换几乎都发生在TpC二核苷酸的胞嘧啶上！ 这是Kataegis图像。纵轴是突变间距（对数刻度）。这个图中基因组内大部分的突变都有着$$~10^6bp$$至$$~10^6bp$$的突变距离。其中超突变区即是表现为突变间距较低的簇。 Govindan R., Ding L., Griffith M., Subramanian J., Dees N. D., et al. Genomic landscape of non-small cell lung cancer in smokers and never-smokers. Cell 150: 1121-1134这是另一篇文章，主要是对17个非小细胞肺癌（NSCLC）患者的肺癌及癌旁正常组织样本进行了全基因组和转录组测序。值得注意的是吸烟者中所观察到的突变频率比不吸烟者高10倍！这是通过深度测序揭示出的这些群体间所不同的克隆模式。而且其中所有经过验证的EGFR和KRAS突变都存在于原始克隆中，这其实也就表明了它们在癌症启动中可能发挥非常重要的作用。 镶嵌性对于这个现象我们也同样通过实际的研究来说明。AML（急性髓细胞白血病）基因组中发现的大多数突变实际上是随机事件，在造血干细胞/祖细胞（HSPC）获得原始突变之前就存在了；但是随着克隆的扩增，细胞的突变历史被『捕获』了。如何理解这里的『捕获』？其实说的就是，原本那些突变都没什么鸟用，就摆在那无所事事，但是，在许多情况下，偏偏只需要再来一个或者两个额外的突变来协助就能共同作用，最后产生恶性的原始肿瘤克隆！ 原发肿瘤和转移的镶嵌性。非同义点突变和插入缺失（绿色块）的区域分布的假设热图。行代表来自七个原发肿瘤区域和六个转移区域的样品。 研究参考 Abyzov A., Mariani J., Palejev D., Zhang Y., Haney M. S., et al. (2012) Somatic copy number mosaicism in human skin revealed by induced pluripotent stem cells. Nature 492: 438-442这里作者发现了一个现象：平均而言，iPSC细胞系表现出2个拷贝数变异（CNV），而这些CNV在iPSC来源的成纤维细胞中不明显。他们发现，至少50%的CNV以低频体细胞变异存在于亲本的成纤维细胞中。根据这一观察，他们估计大约30%的成纤维细胞的基因组中携带体细胞CNV，这表明体细胞镶嵌性广泛存在于人体中。 基因融合基因融合是非常普遍的，也是癌症的一个重要特征。现在的研究发现，一个强启动子与一个下游功能基因（比如：原癌基因）的融合在某些癌症中很普遍。据估计，半数的前列腺癌含有TMPRSS2和ETS转录因子家族成员之间的融合。基因融合是由两个原本分开的基因或位点融合形成的。他们可能形成一种基因产物，很多时候表现出来的功能都是全新的，与两个融合的基因个体都不同。这种阴差阳错的情况可能引起致癌机制的激活，就像费城染色体阳性急性淋巴细胞白血病一样。这种基因融合导致BCR-ABL酪氨酸激酶表达，从而激活细胞增殖。有几种机制会导致基因融合的发生，这个现象是一些癌症类型的特点。胰腺癌的特点便是染色体重排的频繁断裂-融合-桥循环。目前有几种方法可以通过测序研究融合事件，如对肿瘤的全基因组测序和mRNA-Seq。 mRNA-Seq与全基因组测序组合的方法对于发现基因融合及其机制特别高效。原因就是mRNA-Seq可以提供直接的证据，来支持观察到的融合是否发生，并同时为融合基因是否表达提供了证据。而全基因组测序可以发现那些mRNA-Seq所发现不了的区域的信息，如基因间区和UTR。 由折回倒位所引起的融合事件可捕获基因组中遥远区域的片段，如着丝粒重复或参与体细胞重排的区域。在这个例子中，6号染色体上的片段被插入到19号染色体上的重复区域之间。注意19号染色体的第二个拷贝是倒置的，这是折回倒位的特点。 MED1（红色）与几个伙伴基因（蓝色）：ACSF2，USP32 和 STXBP4形成基因融合。 实验上的设计参考 以Pair-end进行全基因组测序是目前检测基因融合最准确、最全面的工具，这些融合包括重复、倒位、通读和单碱基插入缺失。可以说Pair-end是检测融合基因成功与否的一个关键因素。另外就是高深度测序结合更长的读长可以分辨融合连接中微同源的单碱基。而且这种能力是测序独有的。 研究参考 Robinson D. R., Wu Y. M., Kalyana-Sundaram S., Cao X., Lonigro R. J., et al. Identification of recurrent NAB2-STAT6 gene fusions in solitary fibrous tumor by integrative sequencing. Nat Genet 45: 180-185文章主要是利用了全外显子组和转录组测序发现了转录抑制因子NAB2与转录激活因子STAT6的基因融合现象。其中27个独立性纤维性肿瘤（SFT）的转录组测序发现所有肿瘤中存在NAB2-STAT6基因融合。NAB2-STAT6基因融合的过表达诱导了培养细胞的增殖，并激活了EGR应答基因的表达，最后导致了肿瘤。 Seshagiri S., Stawiski E. W., Durinck S., Modrusan Z., Storm E. E., et al. Recurrent R- spondin fusions in colon cancer. Nature 488: 660-664这一篇文章则主要分析了70个原发性人结肠癌的外显子组、转录组和拷贝数变异。拷贝数和RNA-Seq的数据分析确定了在一部分结直肠癌中存在IGF2的扩增和相应过表达。他们还利用RNA-Seq，在10%的结直肠癌中发现了与R-脊椎蛋白家族成员（RSPO2和RSPO3）相关的基因融合。这项研究表明了综合多项技术去了解复杂的癌症基因组很重要。 Thompson-Wicking K., Francis R. W., Stirnweiss A., Ferrari E., Welch M. D., et al. (2012) Novel BRD4-NUT fusion isoforms increase the pathogenic complexity in NUT midline carcinoma. Oncogene这篇文章则提到了PER-624中一种新的BRD4-NUT融合竟然编码了一种功能蛋白，并且它对这些细胞的致癌机制很关键。BRD4-NUT融合转录本是通过易位后的RNA剪接而产生的，这似乎是这些癌症的一个共同特征。这种有助于融合基因的替代异构体表达的机制是第一次报道。 Wen H., Li Y., Malek S. N., Kim Y. C., Xu J., et al. (2012) New fusion transcripts identified in normal karyotype acute myeloid leukemia. PLoS ONE 7: e51203在这项研究中，作者运用双端RNA-Seq来发现染色体核型中的融合，它们经传统的细胞遗传学分析未检测到异常。他们发现了临近基因间的融合转录本以及7个只存在于正常核型中的融合本。 染色体碎裂这是一个不希望发生的现象，染色体碎裂是一个一次性的细胞危机，在单次事件中发生数十次至数百次基因组重排。这种灾难性事件的后果是复杂的局部重排和拷贝数变异，其中染色体上2个（偶尔3个）拷贝的有限范围可被检测。这种单次灾难性事件的模式不同于癌症发展的逐步积累突变的典型模式。在突变积累的癌症发展模式中，拷贝数无上限，因此通常有一个较大的范围。据估计，在所有癌症及其不同亚型之间，染色体碎裂的发生概率约2-3%，而在骨癌中发生概率则大约25%。 染色体碎裂的图示。 研究参考 Rausch T., Jones D. T., Zapatka M., Stutz A. M., Zichner T., et al. (2012) Genome Sequencing of Pediatric Medulloblastoma Links Catastrophic DNA Rearrangements with TP53 Mutations. Cell 148: 59-71 文章提到一名Sonic-Hedgehong髓母细胞瘤（SHH-MB）患者的大量、复杂的染色体重排，此患者带有生殖细胞系TP53突变（Li-Fraumeni综合征）。同时将规模扩大到11名Li-Fraumeni综合征患者的筛查，发现有36%的肿瘤表现出与染色体碎裂一致的重排。这比一般肿瘤群体所观察到的2%染色体碎裂发生率要高得多。P53的生殖细胞系突变与一些肿瘤中凋亡中止导致染色体碎裂的假说是一致的。 拷贝数变异（CNV）结构性变异影响基因量——可转录基因的功能拷贝数。肿瘤发展、药物反应及耐药性的发生通常是由基本的基因扩增和删除来驱动的。这些基因组上的改变可分成大的畸变和小的畸变。大的畸变包括整个染色体或部分染色体的丢失或重复，这被称为非整倍体。小的畸变可能只包含一个碱基，比如点突变和小片段的插入缺失。与健康的基因组不同，这些基因表达的改变会受到转录因子的严格调控，癌症基因组则通过基因的重复和删除来适应和逃避这种调控。癌症耐药性的发生正是此反应的速度和效率的绝佳证明。 基因表达基因表达分析测定基因转录、RNA加工和表观遗传控制的产物。因此，基因表达分析不仅可以看出这些过程的『健康』程度，也可以深入研究细胞里面的分子机制。基于芯片的mRNA分析曾在癌症的基因变得研究中广泛使用，但基于测序的mRNA分析（mRNA-Seq）的出现代表我们测定和解析基因表达产物能力的又一次飞跃。mRNA-Seq可检测修饰过的RNA和表达水平极低的RNA的能力让它特别适合癌症研究。基于mRNA-Seq的方法也可检测非常快的转录变化、剪接异构体、融合基因以及可变聚腺苷酸化位点。 Feng H., Qin Z. and Zhang X. (2012) Opportunities and methods for studying alternative splicing in cancer with RNA-Seq. Cancer Lett 这篇综述关注了RNA-Seq在研究癌症相关的可变剪切中的应用。文中包含一个生物信息学工具列表，以及有关估计可变剪切异构体的表达水平的详尽讨论。 利用RNA-Seq研究癌症中基因表达和选择性剪接的典型生物信息学流程。首先，将短read定位到参考基因组或转录组。在定位之后，估算注释基因和转录本的表达与剪接。 van Delft J., Gaj S., Lienhard M., Albrecht M. W., Kirpiy A., et al. (2012) RNA-Seq provides new insights in the transcriptome responses induced by the carcinogen benzo[a]pyrene. Toxicol Sci 130: 427-439 作者发现，RNA-Seq所检测到的基因比芯片技术多约20%，而表达差异明显的基因更是接近三倍之多。因此，他们检测到的受影响的通路和生物学机制达2-5倍。作者还在许多基因中发现了可变异构体的表达，包括细胞死亡和DNA修复的调控因子，如TP53、BCL2和XPA，它们与基因毒性反应相关。他们还发现了功能未知的新亚型，如已知转录本的片段、带有额外外显子的转录本、内含子保留或外显子跳跃事件。 Kaur H., Mao S., Li Q., Sameni M., Krawetz S. A., et al. (2012) RNA-Seq of human breast ductal carcinoma in situ models reveals aldehyde dehydrogenase isoform 5A1 as a novel potential target. PLoS ONE 7: e50249作者将三个DCIS模型（MCF10.DCIS、SUM102和SUM225）的表达与三维（3D）覆盖培养的非致癌乳腺上皮细胞的MCF10A模型进行了比较，确定了DCIS模型共用的表达变化。他们发现，差异表达的基因编码了与多个信号通路相关的蛋白。 Meyer J. A., Wang J., Hogan L. E., Yang J. J., Dandekar S., et al. (2013) Relapse-specific mutations in NT5C2 in childhood acute lymphoblastic leukemia. Nat Genet 45: 290-294 作者利用RNA测序，报道了诊断和复发相配对的骨髓标本的转录本图谱，这些标本来自十名患有小儿B淋巴细胞白血病的个体。转录组测序鉴定出20个新获得的突变，它们不存在于最初的诊断中，而2名个体带有复发特异的突变。带有NT52C2突变的所有个体都在初步诊断后36个月内复发。 实验设计上的注意事项 RNA-Seq已成为一种研究肿瘤分子变化的常规应用，大部分研究人员采用生产商的试验流程。rRNA的去除可提高信噪比，实现低表达转录本的检测。 癌症中的体细胞突变基本上是 de novo。测序不需要关于突变的先验知识，即可准确定位突变以及得到转录本丰度。 肿瘤通常包含各种细胞。mRNA-Seq的可延伸检测范围和准确性对检测微小的表达变化非常宝贵。只要肿瘤转录本包含了独特的体细胞突变或剪接变异体，那么就可将它与正常的细胞区分开来。 二代双端对读测序检测基因融合的灵敏度取决于许多因素，包括表达水平、转录本长度、所使用的样品制备方法以及cDNA文库的片段长度。 大部分实验方案采用poly（A）富集的RNA制备方法来测定mRNA水平。然而，非编码RNA，如miRNA，也在细胞的生物学中发挥重要作用，并常常介导对肿瘤生长和存活很关键的过程。非编码RNA可通过现有的poly(A)-(rRNA去除)实验方案轻松分析。 RNA表达是组织和细胞类型特异的。在选择肿瘤-正常对照中的对照时，应考虑这一点。 选择性剪接癌症的生物起源、发展、转移与转录组中的许多变异相关联。癌症特异的选择性剪接是个普遍存在的现象，也是个主要的转录后调控机制，涉及到许多癌症类型。 Seo J. S., Ju Y. S., Lee W. C., Shin J. Y., Lee J. K., et al. (2012) The transcriptional landscape and mutational profile of lung adenocarcinoma. Genome Res 22: 2109-2119作者分析了韩国200个肺腺癌。他们在LMTK2、ARID1A、NOTCH2和SMARCA4中发现了新的驱动突变。他们还发现了45个融合基因，其中8个是嵌合的络氨酸激酶。在17个反复发生的选择性剪接事件中，原癌基因MET中的第14号外显子跳过可能是癌症驱动因素。这项研究表明了这种癌症的复杂性以及运用几种技术的价值。 Liu J., Lee W., Jiang Z., Chen Z., Jhunjhunwala S., et al. (2012) Genome and transcriptome sequencing of lung cancers reveal diverse mutational and splicing events. Genome Res 22: 2315- 2327作者对19个肺癌细胞系和3组肺部肿瘤/正常样本配对开展了全基因组测序和转录组测序。他们鉴定出106个与癌症特异性的异常剪接相关的剪接位点突变，包括一些已知的癌症相关基因中的突变。RAC1b是RAC1 GTP酶的一个异构体，含有一个额外的外显子，被认为在肺癌中优先上调，并对MAP2K（MEK）抑制剂PD-0325901敏感。 Thompson-Wicking K., Francis R. W., Stirnweiss A., Ferrari E., Welch M. D., et al. (2012) Novel BRD4-NUT fusion isoforms increase the pathogenic complexity in NUT midline carcinoma. Oncogene这篇文章发现了PER-624中一种新的BRD4-NUT基因融合编码了一种功能蛋白，它对这些细胞的致癌机制很关键。BRD4-NUT融合转录本是通过易位后的RNA剪接而产生的，这似乎是这些癌症的一个共同特征。这种现象以及促进融合基因的可变异构体表达的机制，过去一直未被发现。 RNA编辑在我们人体中，DNA和RNA序列之间的差异也被称之为 RNA编辑，这是一种广泛存在的现象。最频繁的RNA编辑类型是通过腺苷脱氨酶作用于RNA(ADAR)从而实现由腺苷到肌苷的转换。然后紧接着，剪接和翻译机制会将肌识别为鸟苷。在一些肿瘤基因组比正常基因组有着更更比例的RNA-DNA差异。 Jiang Q., Crews L. A., Barrett C. L., Chun H. J., Court A. C., et al. (2013) ADAR1 promotes malignant progenitor reprogramming in chronic myeloid leukemia. Proc Natl Acad Sci U S A 110: 1041-1046作者发现，慢性髓细胞白血病(CML)急变期的祖细胞有着更高的IFN-r通路基因表达以及BCR-ABL扩增。在CML发展期间，他们还发现IFN应答的ADAR1 p150亚型的表达增强，并且腺苷-肌苷的RNA编辑增加。 MicroRNA和非编码RNAMicroRNA(miRNA)的长度很短，大小集中在17bp-25bp之间，属于非编码RNA（ncRNA）家族的成员。它们调控多种不同的生物学功能，包括发育、细胞增殖、细胞分化、信号转导、凋亡、代谢和细胞寿命。 通过RNA-诱导沉默复合物（RISC）与转录本的3-UTR或者与编码区中的识别位点相互作用。miRNA的一个主要作用是抑制基因的转录后表达。在多种癌症中，许多miRNA位于存在序列缺失删除或扩增的基因组区域，这表明它们很可能在癌症的发展过程中扮演很重要的角色。miRNA的编辑位点已经在近年来的研究中被发现了，表明RNA编辑和miRNA介导的调控之间可能存在着关联。同时由于miRNA的测定简单、相对稳定，并且在大量mRNA的控制上起作用，这就让miRNA成为癌症诊断以及治疗期间的检测和分期过程中极具吸引力的标志物。 Law P. T., Qin H., Ching A. K., Lai K. P., Co N. N., et al. (2013) Deep sequencing of small RNA transcriptome reveals novel non-coding RNAs in hepatocellular carcinoma. J Hepatol这篇文章描述了一种新的PIWI-互作RNA(piRNA)piR-Hep1，它参与了肝脏肿瘤发展。与周围正常肝脏相比，46.6%的肝癌细胞(HCC)存在piR-Hep1表达上调。piR-Hep1的沉默抑制了细胞活力、运动和侵袭。作者还发现miR-1323在HCC中的大量表达，以及它与肝硬化背景下所产生的肿瘤的独特关联。 实验设计上的注意事项 测序深度与检测灵敏度是直接相关的。在典型的实验中，如果测序流动槽（Flowcell）的一条通道（lane）只上一个样本，那么测序深度会非常高，这能实现极其灵敏的检测。基于此原因，miRNA的测序深度很少成为考虑因素。在筛查应用中，或不需要如此高深度检测的研究中，样品可加上测序Index标签，从而能够在同一个测序lane中上样多个不同的样本。需要注意的是，在设计miRNA的测序深度时，要时刻记住miRNA控制基因表达，因而miRNA水平的小变化可能影响许多编码蛋白。 新发现的miRNA应当通过功能分析(如Ago2结合或敲除实验)来确认。 实验应当包含足够的样品，以便结论具有真正的统计意义和高的可信度。一般来说，建立一个可能的假设相对来说是比较容易的，只要能证明miRNA存在于患者的肿瘤中，即便样本数量不多也是足够的。但是要真正验证假设就没那么容易了，通常需要大量的患者来检验，并建立统计学可信度。目前，关于如何在测序研究中建立统计学可信度和多重检验纠正，还没有特别公认的方法。当前基于测序的miRNA分析并没有实现miRNA表达的绝对定量，而仅是不同miRNA(如肿瘤-正常配对)的相对量。 样品分层是癌症样品的一个问题。一个特定的癌症表型可能代表几种不同的病因和机理。为了要进行严格的分析，应当有足够多的样品量，从而能够充分代表每个肿瘤亚型。而miRNA的表达会随着肿瘤的发展而变化，因此在实验设计时应建立肿瘤分期和分级。 RNA-蛋白结合（CLIP-Seq）在人类细胞中，大多数mRNA(或前体mRNA)与核不均一性核糖蛋白(hnRNP)相结合，形成大的hnRNP-RNA复合物。hnRNA蛋白在RNA加工的所有关键环节中都发挥重要作用，包括前体mRNA剪接以及mRNA出核、定位、翻译和稳定性。几十种RNA结合蛋白(RBP)和基因的hnRNP蛋白与癌症相关联。 RNA-蛋白的相互作用可通过交联免疫沉淀测序(CLIP-Seq)来测定。在CLIP-Seq中，细胞经过紫外线处理，让RBP与RNA复合物共价交联。细胞随后被裂解，RBP-RNA复合物被免疫共沉淀，从而测序相应的RNA。 Wilbert M. L., Huelga S. C., Kapeli K., Stark T. J., Liang T. Y., et al. LIN28 binds messenger RNAs at GGAGA motifs and regulates splicing factor abundance. Mol Cell 48: 195-206LIN28是个保守的RNA结合蛋白，它与多能性、重编程和肿瘤的形成相关。在各种不同的癌细胞和原发肿瘤组织中都发现LIN28的异常上调。在这篇文章中，作者利用CLIP-Seq鉴定了大约四分之一分布于人类转录本中LIN28的结合位点。从这些结合位点中他们发现，LIN28与mRNA中富含环结构的GGAGA序列结合。同时还发现了，LIN28的表达能够导致可变剪切中的下游变化。 表观遗传和甲基化癌症发展过程中的表观遗传改变与异常的基因表达相关联。近期的研究证据表明，表观遗传上的改变可能在癌症 起始中 发挥作用。表观遗传的控制是通过多个不同过程进行介导的，包括DNA修饰（甲基化或乙酰化）、组蛋白修饰和核小体重塑。发现控制表观基因组的基因发生突变，在人类癌症中是一个相当普遍的现象。NGS测序技术提供了一整套工具，可定位突变并测定它们对癌症发展的影响。 癌症中表观遗传修饰物的基因突变。在不同类型的癌症中经常观察到三类表观遗传修饰物发生突变，这突出了遗传学和表观遗传学之间的串扰。表观遗传修饰物的突变有可能引起癌症的全基因组表观遗传改变。了解遗传和表观遗传变化的关系将为癌症治疗提供新的见解。 DNA修饰DNA修饰目前可以很容易地通过多种技术来进行测定。不同技术的选择取决于所需的通量和分辨率。 Bert S. A., Robinson M. D., Strbenac D., Statham A. L., Song J. Z., et al. Regional activation of the cancer genome by long-range epigenetic remodeling. Cancer Cell 23: 9-22文章作者通过协同的长距离表观遗传激活（LREA）技术，鉴定出一种结构域基因去调控的机制。这些区域通常会跨越1Mb的基因组长度，包括关键癌基因、microRNA和癌症的生物标志物基因。LREA结构域中基因启动子的特点是活性染色体标记的的获得和抑制标记的丢失。 Brastianos P. K., Horowitz P. M., Santagata S., Jones R. T., McKenna A., et al. Genomic sequencing of meningiomas identifies oncogenic SMO and AKT1 mutations. Nat Genet 45: 285-289为了鉴定和验证脑膜瘤中的体细胞遗传改变，作者对17个脑膜瘤进行了全基因组或外显子组测序，并对另外48个肿瘤进行了靶向测序。他们所观察到的突变谱是分布广泛，但他们证实了43%的肿瘤中存在病灶NF2失活，并在另外8%的肿瘤中发现表观遗传修饰物的改变。 Duncan C. G., Barwick B. G., Jin G., Rago C., Kapoor-Vazirani P., et al. A heterozygous IDH1R132H/WT mutation induces genome-wide alterations in DNA methylation. Genome Res 22: 2339-2355脑胶质瘤、急性骨髓性白血病和软骨瘤中经常发生NADP+依赖的异柠檬酸脱氢酶IDH1和IDH2的单等位点基因点突变。作者表明，IDH1R132H等位基因的杂合表达足以诱导这些肿瘤特有的以DNA甲基化为特征的全基因组改变。这说明了IDH1R132H/WT突变体是推动人类癌细胞的表观遗传不稳定性的原因。 Zhang J., Benavente C. A., McEvoy J., Flores-Otero J., Ding L., et al. A novel retinoblastoma therapy from genomic and epigenetic analyses. Nature 481: 329-334视网膜母细胞瘤是一种发生于视网膜的侵袭性儿童癌症，由RB1失活所引发，但潜在机理仍然未知。在此类高度侵袭性的癌症中，许多基因都参与其中，但RB1是唯一的已知发生突变的癌基因。与有限的体细胞突变不同，相对正常的成视网膜细胞，肿瘤的甲基化图谱表现出巨大的改变。最惊人的结果之一是人视网膜母细胞瘤中原癌基因脾络氨酸激酶（SYK）的诱导表达。SYK是肿瘤细胞生存所必需的。研究人员接着表明，小分子抑制剂对SYK的抑制导致培养和体内的视网膜母细胞瘤的细胞死亡。 实验设计上的注意事项 每种组织和细胞类型都有着独特的甲基化模式；因此必须获得感兴趣的组织以便分析。癌症研究中，肿瘤组织-癌旁正常组织的配对可简化分析。 通过重亚硝酸氢盐测序所产生的超大量CpG标志物很难解释，且可靠的统计学分析目前仍然困难重重。不过，下面一些实际的方法可简化分析： RRBS-Seq通过限制覆盖度而简化分析； 综合分析大大改善了结果的可解释性。例如，将表达分析与甲基化分析相结合，让我们可专注于表达水平改变的基因； 将分析限制在某个感兴趣的基因或区域中。这种方法对GWAS的后续研究很有效，也适合已有实验证据说明目的区域存在基因调控或染色质重塑的研究。与降低代表性的方法不同。这种方法实现了更多区域的分析，因此可获得更多信息。 组织培养物应当谨慎使用。随着时间的推移和组织增殖，培养物的甲基化水平可能已经改变，不大能代表原先的组织样本。 组蛋白修饰组蛋白修饰通常指的是甲基化和乙酰化。组蛋白H3K9、H3K27和H4K20的甲基化与基因转录的抑制相关，而H3K4和H3K36的三甲基化与活性转录的染色质相关。组蛋白乙酰化几乎总是与染色质可接近性和转录活性水平的增高相关。通过操控染色质状态和DNA可接近性，表观遗传修饰在各个发育阶段、组织类型和疾病中都对基因表达的控制起着关键作用。 Wilkinson A. C., Ballabio E., Geng H., North P., Tapia M., et al. (2013) RUNX1 is a key target in t(4;11) leukemias that contributes to gene activation through an AF4-MLL complex interaction. Cell Rep 3: 116-127这篇文章报道了一种转化机制，其中两个致癌融合蛋白合作激活目的基因，然后调节其下游产物的功能。 实验设计上的注意事项 每种组织和细胞类型都有着独特的甲基化模式；因此必须获得目的组织以便分析。 组蛋白修饰可以通过各种ChIP-Seq方法进行检测，原理是通过抗体与目标甲基化组蛋白进行特异结合。 同样，组织培养物应当谨慎使用。随着时间的推移和组织增殖，培养物的甲基化水平可能已经改变，不大能代表原先的组织样本。 染色质结构与重排染色体重排需要DNA双链断裂的形成和连接。这些事件的发生，会破坏基因组的完整性，并经常在白血病、淋巴瘤和肉瘤中观察到。并且特定基因间的反复的基因融合在不同的个体中均观察到，这表明这些基因一定在细胞周期中的某个阶段他们之间的物理位置非常接近。 这是一个设想的三维、具有转录活性的复合物，它包含了致密的环化位置。这个示意图是基于检测到的成环事件，并假设所有成环事件都能发生在单个细胞内。在这个模型中，所有小环汇集到一个共同的核心（蓝色球）。环降低了转录活性复合物的物理大小，从而推动转录因子接近待定的基因组位点。 检测染色质相互作用。在三维空间中，相同或不同染色体上远端的基因组区域相互作用，而这种相互作用是由一个或多个DNA结合蛋白介导的。a) ChIP-Seq 利用染色质免疫共沉淀来鉴定DNA和蛋白的相互作用。人们采用各种DNA-片段化方法和核酸外切酶，来缩小片段的大小分布。b)染色质构像捕获实验利用一个连接步骤将互作的染色质片段相连接。这种方法可鉴定与遥远序列相结合的蛋白。c)配对末端标签测序分析染色质相互作用（ChIA-PET）同样也利用连接步骤来检测染色质相互作用，将不相邻的互作区域配对。然而，ChIA-PET利用染色质免疫共沉淀（ChIP）步骤，只能鉴定特定蛋白的相互作用，如RNA聚合酶II。 参考文献 Papantonis A., Kohro T., Baboo S., Larkin J. D., Deng B., et al. TNFalpha signals through specialized factories where responsive coding and miRNA genes are transcribed. EMBO J 31: 4404- 4414作者利用测序以及染色体构像捕获（3C）和ChIA-PET表明，TNF_alpha诱导响应基因聚集在分散的『NF-B工厂』。一些『工厂』还专门转录编码miRNA的响应基因，这些miRNA靶定下调的mRNA。 Rocha P. P., Micsinai M., Kim J. R., Hewitt S. L., Souza P. P., et al. Close proximity to Igh is a contributing factor to AID-mediated translocations. Mol Cell 47: 873-885细胞核组织可决定『脱靶』活性以及融合伴侣的选择。这项研究表明，绝大多数已知的活化诱导胞嘧啶核苷脱氨酶（AID）介导的Igh转位伴侣在类型转换过程中与此位点接触的染色体结构中被发现。此外，这些相互作用结构域可用来鉴定被AID靶定的其他基因。 Theodoratou E., Montazeri Z., Hawken S., Allum G. C., Gong J., et al. Systematic meta- analyses and field synopsis of genetic association studies in colorectal cancer. J Natl Cancer Inst 104: 1433-1457这篇研究文章表明，T细胞特异的转录因子GATA3在介导增强子接近调控区域上扮演了重要角色，这些调控区域参与了雌激素受体（ESR1）介导的转录。GATA3沉默导致在雌激素刺激之前辅助因子和活性组蛋白标记的整体重新分配。 Hakim O., Resch W., Yamane A., Klein I., Kieffer-Kwon K. R., et al. (2012) DNA damage defines sites of recurrent chromosomal translocations in B lymphocytes. Nature 484: 69-74作者发现，在培养的小鼠B淋巴细胞中，缺乏经常性的DNA损伤时，Igh或Myc与其他所有基因之间的易位与它们的接触频率直接相关。反过来，与经常性位点指向的DNA损伤相关的易位与DNA断裂形成的速率成正比。他们认为，非定向重排反映了细胞核结构，而DNA断裂形成决定了包括驱动B细胞恶性肿瘤在内的经常性易位的位置和频率。 综合分析（多组学分析）所有的生物过程都是相互关联的，而在癌细胞发生过程的任何一个变化都会影响其他所有过程。突变可能影响所表达的活性，继而又影响DNA甲基化，再就影响其他许多基因的表达等等一连串的反应。每个个体都有着大量的特有突变，再加上这一连串的事件，能够让人们深入研究各种用于区分癌症的疾病表型。综合分析可以使揭示癌症生物学的真正复杂性向前迈进了一步。研究人员如今能够检测大部分的单个过程，但认识和治疗癌症的真正进步将来自于对所有这些过程的综合分析，也就是常说的多组学分析。 参考文献 Weischenfeldt J., Simon R., Feuerbach L., Schlangen K., Weichenhan D., et al. (2013) Integrative genomic analyses reveal an androgen-driven somatic alteration landscape in early-onset prostate cancer. Cancer Cell 23: 159-170作者发现，早发性前列腺癌的形成涉及到雄激素驱动的结构重排。相比之下，老年发病的前列腺癌积累了非雄激素相关的结构重排，表明一种不同的肿瘤形成机制。 Cowper-Sal lari R., Zhang X., Wright J. B., Bailey S. D., Cole M. D., et al. (2012) Breast cancer risk- associated SNPs modulate the affinity of chromatin for FOXA1 and alter gene expression. Nat Genet 44: 1191-1198作者表明，与乳腺癌风险相关的SNP集中在FOXA1和ESR1的顺反组（cistrome），以及组蛋白H3赖氨酸4单甲基化（H3K4me1）的表观基因组。大多数的风险相关SNP调控FOXA1在远端调控元件的染色质亲和力，这导致等位基因特异的基因表达。 Peifer M., Fernandez-Cuesta L., Sos M. L., George J., Seidel D., et al. (2012) Integrative genome analyses identify key somatic driver mutations of small-cell lung cancer. Nat Genet 44: 1104-1110作者发现了TP53和RB1失活的证据，并在编码组蛋白修饰物的基因中发现了反复的突变。此外，他们还在PTEN、SLIT2和EPHA7中观察到突变，以及FGFR1络氨酸激酶基因的病灶扩增。这种综合分析表明，组蛋白修饰可能与小细胞肺癌（SCLC）有关。 技术参考一个良好的实验设计可以提高技术性能，从而产生最易解释和可靠的结果。这里重点强调研究人员在设计实验时必须牢记的生物学和技术的特性。 癌症中的实验设计面临一些独特的挑战。典型的肿瘤样本包含两个基因组：遗传自父母的生殖细胞系（germline）和在疾病发展过程中积累的体细胞突变（somatic mutations）。肿瘤细胞在样本中的比例可能在10%-100%之间。肿瘤基因组也是动态的，会快速积累 de novo 突变。因此，每一个肿瘤中又可能同时包含几个克隆亚型。 目前大部分已发表研究的样本量都非常小，可被视为仅是提出了相关的假说。而随着越来越多的测序信息被我们所获得，大部分癌症类型可根据其分子表型，被分成多个亚群。这严重降低了实验的能力，并增加了严格分析所需的样本数量。部分解决这一难题的方案是在探索阶段利用全基因组测序来寻找新的突变。在第二阶段，利用全外显子组或靶向测序来确认新发现的这些突变，并确定他们在大型队列中的丰度。然而，在未来，统计学上严谨的全基因组测序实验有可能是非常大的，需要数千个样本。 使用NGS测序技术进行深度测序是指多次生成定位到同一区域的序列片段，有时达上百次。但由于每条序列片段是从单个DNA分子中产生的，故提高深度测序能够实现原始样品中低至1%的克隆的检测。比较同一个体的肿瘤和癌旁正常组织的序列，我们可以很容易识别浸润组织的序列片段。最佳的读取深度将取决于癌症类型和所需的灵敏度，但一般建议为正常基因组最低为40倍的覆盖深度，而癌症基因组需要80倍以上的覆盖深度。在肿瘤高度异质时，可能需要肿瘤不同部位的多次活检，才能包含所有的细胞类型。 在这个假定的例子中，肿瘤含有两个癌症克隆和邻近组织。肿瘤样本中正常细胞所产生的序列（图中：比对中的前两个序列）可通过与邻近正常组织所产生的序列比较后确定。肿瘤样本中的剩余序列可分为两组，分别代表主要和次要的肿瘤克隆。次要克隆，若不及时治疗，可能在复发时成为肿瘤的主要组分。在实际分析中，肿瘤样本至少要有40倍的覆盖深度，并覆盖靶定的基因集合、全外显子组或全基因组。 检测癌症基因组中的体细胞突变通常有三种方法：全基因组测序、全外显子组测序和靶向基因测序。下表简要介绍了每种方法的有点和缺点。在比较多发性骨髓瘤的全基因组和外显子组测序时，半数的蛋白编码突变通过染色体畸变（如易位）而存在，其中大部分不能单独被外显子组测序而发现。靶向重测序是一种有用的技术，可收录超大队列中已知癌症相关基因的突变。从长远来看，随着我们对基因组的了解日益加深，并且我们处理和解释大型数据集的能力的提高，全基因组测序无疑是最好的肿瘤分子鉴定方法。而在短期内，靶向基因测序可为患者匹配出市场上已有的药物，让他们立刻受益。 参考来源 Illumina cancer research]]></content>
      <categories>
        <category>癌症基因组</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>癌症</tag>
        <tag>综述</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用Shapeit2对人类基因组数据进行phasing]]></title>
    <url>%2F2016%2F03%2F15%2F2016-03-15-how-to-phase-the-human-genome-by-shapeit2.html</url>
    <content type="text"><![CDATA[SHAPEIT(2.0)是专门用于对推断基因组单体型的软件，有牛津大学的团队所开发，并且一直应用与千人基因组计划中。 以下，我将记录如何通过shapeit2对人群的变异数据集（VCF 格式）进行phasing，并构造出reference panel的过程。 首先，准备文件。整个过程只需要变异数据集（VCF 格式），样本信息文件(sample.fam)，genetic_map文件和参考序列（fasta格式）。对于genetic_map文件需要单独做些说明，这个记录的是基因组中各个位点的重组率和物理距离之间的关系，这是phasing过程非常重要的一个文件。它来自于人类单体型计划-Hapmap计划，可以下载,最新版是b37或者说hg19，如果你的reference版本高于hg19，则需要liftover，liftover之后那些顺序发生交叉的位点，是liftover的错误导致的，要去掉。但是要注意的是，genetic-map中两个位点之间的重组率（recombination rate）是不变的，这其实也很好理解，reference之所以需要升级，是因为它的组装结果并非是百分百符合真实情形的，随着技术的进步，我们会不断去升级逼近这个真实情况，但重组率是根据群体的重组情况来计算的，它是由真实情况反映出来的，因此即便reference版本改变了，它的值也不需要改变。不过对于两个位点之间的物理距离（physical distance）就不同了，leftover之后，这个距离是会发生变化的，通过和原点（一般是重组率为0的点或者就是各个染色体的第一个位点）的距离比例来调节。 至于样本信息文件，格式如下： 12345671009 1009-01 0 0 1 341009 1009-02 0 0 2 331009 1009-06 1009-01 1009-02 2 671030 1030-01 0 0 1 431030 1030-02 0 0 2 441030 1030-06 1030-01 1030-02 1 71 其他的两份文件就不必多说了。 准备好以上文件之后接下来就是主要的步骤了： 第一步，将vcf转化为bed/bim/fambed/bim/fam这三个文件是phasing的常用谱系文件格式。做这一步转换的工具有很多，我们这里直接借助GATK的VariantsToBinaryPed模块进行转换： 123456789time java -Xmx8g -jar GenomeAnalysisTK.jar -T VariantsToBinaryPed \ -R hg20.fa \ -V chr22.vcf \ --metaData sample.fam \ -mgq 0 \ -bed chr22.bed \ -bim chr22.bim \ -fam chr22.fam \ -log gatk.log &amp;&amp; echo "** done **" &amp;&amp; sed 's/^chr//g' chr22.bim &gt; t.bim &amp;&amp; mv -f t.bim chr22.bim 这个执行命令的最后多加了一小步：将原来输出的.bim文件中第一列的chr22换成了22。之所以要费这个小周折，是因为如果不做这个小操作，接下来的plink步骤中，会直接报ERROR: Problem reading BIM file, line 1退出，原因就是它不允许chr的开头，至于具体的原因我也没去细查。 第二步，过滤genotype高missing rate和孟德尔错误的位点12plink=/com/extra/testing/bin/plinktime $plink --noweb --bfile chr22 --keep-allele-order --me 1 1 --set-me-missing --make-bed --out chr22.nomendel &amp;&amp; echo "** nomendel done **" &amp;&amp; time $plink --noweb --bfile chr22.nomendel --keep-allele-order --geno 0.05 --make-bed --out chr22.nomendel.filter &amp;&amp; echo "** fileter done **" 第三步，phasing这是phasing的最后一步了，这里分成两小步，phasing和输出格式转换：1234567891011121314# phasingtime shapeit2 \ --duohmm \ -W 5 \ --input-bed chr22.nomendel.filter.bed chr22.nomendel.filter.bim chr22.nomendel.filter.fam \ --input-map genetic_map.chr22.txt \ -O hapData \ --thread 1 &amp;&amp; echo "** panel done **"# 格式转换time shapeit2 -convert \ --input-haps hapData \ --output-vcf chr22.haps.vcf \ --output-ref chr22.phased.hap chr22.phased.leg chr22.phased.sam &amp;&amp; echo "** all done **" 以上输出结果中，chr22.haps.vcf便是进行phasing之后的结果，而chr22.phased.hap和chr22.phased.leg这两个文件是从chr22.haps.vcf中得到的，它们便是Imputation分析中的reference panel。]]></content>
      <categories>
        <category>phasing</category>
      </categories>
      <tags>
        <tag>shapeit2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何使用Python绘制GWAS分析中的曼哈顿图和QQ图]]></title>
    <url>%2F2016%2F02%2F02%2F2016-02-02-How-to-create-manhattan-and-qq-plot-for-GWAS-study-in-Python.html</url>
    <content type="text"><![CDATA[【前言】这篇文章使用geneview完成这两类图的作法，它是一个Python高级库，建立在matplotlib的基础之上，专门用于基因组数据的可视化，目的是为了使创建高大上（精致）的基因组数据图表变得简单。目前该发布的Python包中已经内置多个优美的调色板和风格（默认情况下就能创建赏心悦目的图形），同时已经集成了曼哈顿图和Q-Q图的绘制函数。作为该Python包的主要开发者，只是如此是远远不够的，在未来的日子里，我希望它能在功能不断完善的同时也变得更加易用。 曼哈顿图和QQ图是两个在全基因组关联（GWAS）分析里面最常出现的图形，基本上已经是GWAS的标配，几乎在每篇GWAS的文章都会见到，它们的作用和所要传达出来的信息我也在上一篇关于GWAS的博文中做了些说明，在这里我们就只集中在如何用Python和geneview将其有效地展现出来。 首先，准备一些数据来作为例子。 我这里用来展现的数据是2011年丹麦人所做过的一个关于年轻人过度肥胖的GWAS研究——GOYA，数据也是从他们所发表的结果中获得，总共有5,373个样本，其中超重的个体（case）有2,633个，正常的个体（control）是2,740个，从样本量上看还算可以。为了方便使用，我对其做了相关的处理，包括从PED和MAP文件到GEN文件的生成，并重复了一次case-control的关联性分析，计算出了芯片上所研究的各个SNP位点与肥胖相关的显著性程度（即p-value），最后又将结果数据抽取出来做成数据集——放在这里供下载（15.6Mb，csv格式）。 【注】以上内容虽提及到了一些领域内术语和相关文件格式，但若不懂也请不必纠结，因为后续处理都是基于这个最终的数据集来完成的。 接着，需要将geneview软件包加入到你的Python中，有多种不同的方式，但推荐直接使用pip，以下是安装比较稳定的发布版，直接在终端命令行下(Linux or Mac)输入： 1pip install geneview 或者，也可以直接从github上安装正在开发的版本： 1pip install git+git://github.com/ShujiaHuang/geneview.git#egg=geneview 第三种办法就是直接下载源码，然后自行编译，虽然不推荐这种做法（因为还有依赖包必须自行下载安装，过程会比较麻烦低效），但对于某些不能连接外网的集群也只能如此，这三种方式都是可行的。 曼哈顿图将示例数据下载下来： 1wget https://raw.githubusercontent.com/ShujiaHuang/geneview-data/master/GOYA.csv ./ 先简单地查看一下数据的格式: 12345678910chrID,rsID,position,pvalue1,rs3094315,742429,0.1445861,rs3115860,743268,0.2300221,rs12562034,758311,0.6443661,rs12124819,766409,0.1462691,rs4475691,836671,0.4581971,rs28705211,890368,0.3627311,rs13303118,908247,0.229121,rs9777703,918699,0.379481,rs3121567,933331,0.440824 一共是4列（逗号分隔），分别为：[1]染色体编号，[2]SNP rs 编号，[3] 位点在染色体上的位置，[4]显著性差异程度（pvalue）。在本例曼哈顿图中我们只需要使用第1,3和4列；而QQ图则只需要第4列——pvalue。 下面我们先从绘制曼哈顿图开始。我们先将需要的数据读取到一个列表中，可以这样做： 12345678import csvdata = []with open("GOYA.csv") as f: f_csv = csv.reader(f) headers = next(f_csv) data = [[row[0], int(row[2]), float(row[3])] for row in f_csv] 现在GOYA.csv中的数据就都存放在data列表中了，由于Python在读取文件中数据时，都是以string类型存放，因此对于第3和第4列的数据有必要事先把做点类型转换。 接下来，调用geneview中的曼哈顿图函数。 123456import matplotlib.pyplot as pltfrom geneview.gwas import manhattanplotax = manhattanplot(data, xlabel="Chromosome", ylabel="-Log10(P-value)") # 这就是Manhattan plot的函数plt.show() 只需这样的一句代码就能创建一个漂亮的曼哈顿图，有必要再次指出的是，geneview是以matplotlib为基础开发出来的，所创建的图形对象实际上仍属于matplotlib，geneview内部自定义了很多图形风格，同时封装了大量只属于基因组数据的图表类型，但图形的输出格式以及界面显示都仍和matplotlib一样，因此在这里我们使用matplotlib.pyplot的show()函数(上例中：plt.show())将所绘制出来的曼哈顿图显示出来。如果要将图形保存下来，则只需执行plt.savefig(&quot;man.png&quot;)，这样就会在该目录下生成一个名为『man.png』png格式的曼哈顿图，若是要存为pdf格式，则只需将所要保存的文件名后缀改成『.pdf』（plt.savefig(“man.pdf”)）就可以了。下面这些格式：emf, eps, pdf, png, jpg, ps, raw, rgba, svg, svgz等都是支持的，至于最新的还有多少种，还请参照matplotlib文档中说明。 此外，geneview中的每个画图函数都有着足够的灵活性，我们也可以根据自己的需要做一些调整，比如： 123456789xtick = ['1', '2','3','4','5','6','7','8','9','10','11','12','13','14','16','18', '20','22']manhattanplot(data, xlabel="Chromosome", # 设置x轴名字 ylabel="-Log10(P-value)", # 设置y轴名字 xtick_label_set = set(xtick), # 限定横坐标轴上的刻度显示 s=40, # 设置图中散点的大小 alpha=0.5, # 调整散点透明度 color="#f28b1e,#9a0dea,#ea0dcc,#63b8ff", # 设置新的颜色组合 ) 实现新的颜色组合、限定x轴上的刻度显示和散点大小的调节。甚至还可以将散点改为线： 12345678manhattanplot(data， xlabel="Chromosome", # 设置x轴名字 ylabel="-Log10(P-value)", # 设置y轴名字 xtick_label_set = set(xtick), # 限定横坐标轴上的刻度显示 alpha=0.5, # 调整散点透明度 color="#f28b1e,#9a0dea,#ea0dcc,#63b8ff", # 设置新的颜色组合 kind="line" ) 其它方面的调整请查看geneview文档中的相关说明。 Q-Q图qq图只需用到上例中的pvalue那一列：123456789101112import csvimport matplotlib.pyplot as pltfrom geneview.gwas import qqplotpvalue=[]with open("GOYA.csv") as f: f_csv = csv.reader(f) headers = next(f_csv) pvalue = [float(row[3]) for row in f_csv]ax = qqplot(pvalue, color="#00bb33", xlabel="Expected p-value(-log10)", ylabel="Observed p-value(-log10)") # Q-Q 图plt.show() 同样，也可以根据自己的需要对改图进行相关的调整。 以上，便是如何使用Python来制作Manhattan图和QQ图的方法，geneview的集成函数简化了这样的一个过程。 另外，如果你也看过丹麦人的这个GOYA研究，就会发现实际以上的两个图和其文章中的基本是一致的，当然我自己做了些数据清洗的操作，结果上仍然会有些许的不同。虽然此刻下结论还有点为时尚早，但总的来讲，我应该也可以通过这个数据集比较顺利的将其结果重复出来了。 最后，附上利用geneview画曼哈顿图和QQ图的代码： （1）曼哈顿图： 123456789101112131415import sysimport csvimport matplotlib.pyplot as pltfrom geneview.gwas import manhattanplotwith open("data/GOYA.csv") as f: f_csv = csv.reader(f) headers = next(f_csv) data = [[row[0], int(row[2]), float(row[3])] for row in f_csv]ax = manhattanplot(data, xlabel="Chromosome", ylabel="-Log10(P-value)")plt.show() （2）QQ图：12345678910111213import csvimport matplotlib.pyplot as pltfrom geneview.gwas import qqplotpvalue=[]with open("data/GOYA.csv") as f: f_csv = csv.reader(f) headers = next(f_csv) pvalue = [float(row[3]) for row in f_csv]ax = qqplot(pvalue, color="#00bb33", xlabel="Expected p-value(-log10)", ylabel="Observed p-value(-log10)")plt.show()]]></content>
      <categories>
        <category>数据可视化</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>GWAS</tag>
        <tag>manhattan plot</tag>
        <tag>Q-Q plot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Flask开发基因组数据web服务的RESTful API（1）]]></title>
    <url>%2F2015%2F12%2F20%2F2015-12-20-Develop-a-genome-data-RESTful-API-server-by-flask-1.html</url>
    <content type="text"><![CDATA[这真的是一个系统工程！ 首先，我要开发一个在线的基因组数据库，目的是能够以符合RESTful API设计准则来进行访问的数据Web服务。虽然，此前我没接触过任何Web开发，想想也是困难重重，但这并不能阻止我——兴趣所在，而且我自己也更清楚需求是什么。 那么说干就要干。经过一番有限的比较之后（主要是看各种技术论坛和博客），我发现Rails、Django和Flask都适合用来干这个事情，他们的文档都很好，是目前Web开发的主流，社区活跃，用的人多了碰到问题也容易找到解决方案，但是觉得Rails和Django太大，太系统，以至于冗余，我希望的是微框架，因为初步搭成只是第一步，后面一定会有很多自定的优化和开发，而不是一次就完成，太冗余不灵活的话反而会极大影响自己的后续步骤，灵活的可拓展性对我而言反而更重要。这么考虑之后，我就选了Flask，其他的组件需要的时候加入即可，还可以随时换掉，或者自己重写，而且项目结构是完全自定的，这就很适合我的口味了，框架功能不需多，只要解决了Web开发中最重要的问题，作为一个最小可行集就OK——就像人体肠道最小宏基因组一样，浓缩即是精华，其他的枝叶要能够被灵活地删增。 那么选好之后，接下来我是怎么做的呢？第一，学会了Flask，读了很多技术博客，操练了一些如何例子，比如使用 Python 和 Flask 设计 RESTful API，The Flask Mega-Tutorial等，同时读了《Flask Web开发：基于Python的Web应用开发实战》，这对于我这种从未接触过Web应用开发的小白来说，真是一本好书，读了之后真是有一种相见恨晚的感觉，内容很好读[但我并不是说它读起来容易。虽然它确实是从基础的讲起，然而对我这类没有任何基础的人来说并不十分容易，好在]从开发到测试到部署每一步都十分清晰，很多内容讲得比博客清楚得多多了，而且整本书本身就是项目驱动的，就像我也是项目驱动要去用Flask一样。第二，要学会数据库。其实这一步和前面是分不开了，基本都是同时进行。但在选择使用那种数据库这一步中我也花了不少心思，最后按照我的数据情况，我选择了非关系型数据库MongoDB。 在这个一周多的学习时间中（工作之外的时候），我学到了很多，不但实现了一个完整的Web应用，还进一步加深加强了对如何更有效应用python的理解和认识。对我提升最大的还不是会了如何用flask去做一个简单的web，而是它们的设计理念，程序/项目如何布局，如何分离，如何做到低耦合，测试和性能分析应该怎么做才合理有效，这些理念是我们平时做基因组数据分析所缺少（马虎）的，因为本来许多生物信息工程师并不懂web开发，甚至从未有过任何IT软件设计的训练，平时也难有时间专门去学习，很多时候都是，任务来了要赶紧写个程序解决一下，然后再来一个任务，又再写个程序处理一下，如此反复，时间久了这些零散的程序根本难以复用，最后慢慢地也就成了垃圾程序。就算是比较大型的生信软件的开发，过程和布局也欠缺规范，关于这一点我深有体会，也想抽时间去系统学习，但实际操作起来并没严格注意，更多的是直接参看github上一些项目的布局和设计来依样画葫芦而已。 接下来就难了，要做符合RESTful API准则的数据web服务，那么就得开始设计API了。怎么做？不同版本如何管理，如何语义化，如何条理化管理新旧版本的内容和功能？如何把基因组数据资源转换为JSON格式的序列化字典，大数据资源如何分页等等诸多的细节都需要一一考虑，我参考了Solvebio，23andme，豆瓣等，最后得到了我自认为比较合理的设计方式。 （未完待续）]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>RESTful API</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么说FPKM/RPKM是错的]]></title>
    <url>%2F2015%2F08%2F25%2Fwhy-fpkm-and-rpkm-are-wrong.html</url>
    <content type="text"><![CDATA[两周前，我接触了一个RNA-seq的项目，做完之后，我重新思考了FPKM和RPKM的计算，觉得它们很可能是不对的，后来查阅了一些文献终于验证了我的想法。现在我重新将这个过程记录了下来： FPKM和RPKM分别是什么RPKM是 Reads Per Kilobase Per Million的缩写，它的计算方程非常简单： $$RPKM = \frac{10 ^ 6 \times n_r}{L × N}$$ 其中，$n_r$ 是比对至某一个基因的read数量；$L$是该基因的外显子长度之和除以1000，因此，要注意这里的$L$单位是kb，不是bp；$N$ 是 有效比对至基因组的总read数量。FPKM是 Fregments Per Kilobase Per Million的缩写，它的计算与RPKM极为类似，如下: $$FPKM = \frac{10 ^ 6 \times n_f}{L \times N}$$ 其中，$n_f$是比对至目标基因的fregment数量。FPKM与RPKM唯一的区别是：F代表fragments，R代表reads。如果是Pair-end测序，每个fragments会由这两个成对的reads构成，因此FPKM只计算两个reads能比对到同一个转录本的fragments数量；而RPKM计算的是可以比对到转录本的reads数量（不管Pair-end的两个reads是否能比对到同一个转录本上）。如果是single-end测序，那么FPKM和RPKM计算的结果将是一致的。 以上是这两个量的计算方式。这样计算的目是为了解决在计算RNA-seq转录本丰度时的两个bias： （1）相同表达丰度的转录本，往往会由于其基因长度上的差异，导致测序获得的Read（Fregment）数不同。总的来说，越长的转录本，测得的Read（Fregment）数越多，但这并不代表表达量就真的多。 （2）由测序文库的不同大小而引来的差异。即同一个转录本，其测序深度越深，通过测序获得的Read（Fregment）数就越多。 FPKM和RPKM通过同时除以L（转录本长度）和除以N（有效比对的Read（Fregment）总数）的办法，最终将不同样本（或者同个样本在不同条件下）的转录本丰度归一化到一个能够进行量化比较的标准上。 上面的式子看起来似乎合情合理，但是它们却都做错了。 为什么FPKM/RPKM是错的要回答这个问题，我们需要先撇开所有形式上的计算，重新思考这个问题——到底什么是RNA转录本的表达丰度？事实上，对于任何一个取得的样本，它上面任何一个基因的表达量（或者说丰度），都将已是一个客观存在的值，这个值是不管你改变了多少测序环境都不会变的。而且细胞中此刻总共有多少个基因在表达，实际上也已经是客观定好了的。一旦我们开始以这样一种“先知”的形式来理解的时候，有趣的事情就开始出现了。 此时，我们可以假定，对于样本X，其中有一个基因G被转录了g次，同时样本X中所有基因的转录总次数假定是total，那么正确描述基因G转录丰度的值应该是： $$r_{g}=\frac{mRNAg}{mRNA{total}}$$ 没毛病！而且与此同时，样本X中其他基因转录丰度的计算也和以上式子类似，除了要把分子换为其他基因对应的转录次数之外，分母都一样。于是这个有趣的事情就是，所有基因转录本丰度的均值$r_{mean}$将是一个恒定不变的数，由以上定义这个数就是： $$r{mean} = \frac{1}{g{total}}\sum_{g}^{G}{rg} = \frac{1}{g{total}}\frac{\sum_{g}^{G}{mRNAg}}{mRNA{total}}$$ 而 $$\sum_{g}^{G}{mRNAg} = mRNA{total}$$ 所以 $$r{mean} = \frac{1}{g{total}}$$ 这个值是由基因的总数决定的，也就是说，对于同一个物种，不管它的样本是哪种组织（正常的或病变的等），也不管有多少个不同的样本，只要它们都拥有相同数量的基因，那么它们的$r_{mean}$都将是一致的。这是一个在进行比较分析的时候，非常有意义的恒等关系。 但在实际的操作中，我们是难以直接计算这些r值的。好在只要能够保证模型的自洽性，我们是能通过自建一些统计量来对r值进行间接描述的，比如FPKM和RPKM。本质上它们的目的就是为了描述r。虽然如此，但我们也要注意，所有这些要用来描述转录本丰度的统计量，都应该能等价描述这一恒等关系。也就是说，不管我们使用了什么统计量，它所描述出来的转录本丰度应该且必须是真实丰度$rg$的m倍（m必须是一个根据模型定出的不变值），它的均值也将是$r{mean}$的m倍，至少这样才是得到有意义结果的前提！ （那么）现在，我们回过头来看看FPKM和RPKM的计算式，就会发现它们根本做不到。 举个例子来说明（以FPKM的计算为例），我们假定有两个来自同一个个体不同组织的样本X和Y，这个个体只有5个基因，分别为A、B、C、D和E，它们的长度分别如下： 由此，我们可以得到，样本X和Y的转录本的不变量，$r{mean}$值都是$r{mean} = \frac{1}{5} = 0.2$。如果FPKM或RPKM是一个合适的统计量的话，那么至少，样本X和Y的平均FPKM（或RPKM）值应该相等。 我们以FPKM的计算的为例子，以下这个表格列出的分别是样本X和Y在这5个基因中比对上的fregment数和各自总的fregment数量： 于是，按照以上公式我们可以得到样本X和Y在这5个基因上的FPKM值分别为： 接下来就可以计算FPKM的均值了。我们得到，样本X在这5个基因上的FPKM均值$FPKM{mean} = 5,680$；而样本Y的FPKM均值却是$FPKM{mean} = 161,840$!! 它们根本不同，而且差距相当大，那么究竟为什么会有如此之大的差异？难道这是我故意构造出来的例子所造成的吗？当然不是，这是由其数学计算上的缺陷所导致的。 首先，我们可以把FPKM的计算式拆分成两个部分：（1）等价（其实严格来讲也没那么等价）描述某个基因转录本数量的统计量（$\frac{n_f}{L}$） 和（2）测序获得的总有效Fregment数量的百万分之一（$\frac{N}{10 ^ 6}$）；看，FPKM便是这两部分的商！分开来看它们貌似都有点道理，但是合起来的时候其实很没逻辑，尤其是第二部分$\frac{N}{10 ^ 6}$，本来式子的第一部分 （$\frac{n_f}{L}$）就已经是描述某个基因的转录本数量，那么正常来讲，第二部分就应该是描述样本总体的转录本数量（或至少是其等价描述）才能说得通，而且可以看得出FPKM(RPMK)是有此意的，因为这本身就是这一统计量的目的。然并卵，它失败了！$\frac{N}{10 ^ 6}$的大小其实是由RNA-seq的测序深度所决定的，并且是一个和总转录本数量无直接线性关系的统计量——N与总转录本数量之间的关系还受转录本的长度分布所决定，而这个分布往往在不同样本中是有差异的！比如，有些基因，虽然有效比对到它们身上的Fregment数目是相等的，但很明显，长度越长的基因，其被转录的次数就越少。也就是说，N必须将各个被转录的基因的长度考虑进去才能正确描述总体的转录本数！而FPKM（RPKM）显然没有做到这一点，这便是FPKM（RPKM）出错的内在原因。 那么应该是用什么样统计量才合适其实，通过以上分析，我们已经可以确定一个更加合理的统计量来描述RNA转录本的丰度了。我意外地发现，这个统计量其实在2012年所发表的一篇关于讨论RPKM的文章（RPKM measure is inconsistent among samples. Wagner GP, Kin K, Lynch VJ. Theory Biosci. 2012.）中就已被提到过了，它被称之为TPM —— Transcripts Per Million，它的计算是： $$TPM = \frac{\frac{n_r \times read_l}{ g_l} \times {10}^{6}} {T} = \frac{n_r \times read_l \times {10} ^ {6} } {g_l \times T}$$ $$T =\sum_{g=i}^{G}{ (\frac{n_r \times read_l}{g_l})_i }$$ 其中，$read_l$是比对至基因G的平均read长度，$g_l$ 是基因G的外显子长度之和（这里无需将其除以1000了）。在不考虑比对剪切的情况下，$read_l$这个值往往都是一个固定值（如100bp或者150bp等），因此我们也可以将$read_l$统一约掉，那么分子就会蜕变成RPKM计算式的第一部分，但把$read_l$留着会更合理。这样，整个统计量就很好理解了，分子是基因G的转录本数（等价描述），分母则为样本中总转录本的数量，两者的比值TPM——便是正确描述基因G的转录本丰度！并且，简单计算我们就可以知道TPM的均值是一个独立于样本之外的恒定值： $$TPM_{mean} = \frac{10^6}{N}$$ 这个值也刚好是$r_{mean}$的$10^6$倍，满足上述等价描述的关系。我们仍然通过上面的例子来进作说明，为简单起见我们只把fregment换为read，其他数字都一样，并且统一假设$read_l$都是一样的： 接着，我们可以分别计算样本X和Y的TPM_mean,并且很明显它们都是$200000 = 10^6 / 5$. 而且，经过这样的标准化之后，X和Y就处于同样的一个标准上了，此刻，彼此之间的比较分析才是真正有意义的。 既然FPKM/RPKM是错的，那为什么大家直到现在都还在用，而且还真找到了（能被实验所验证）有价值的结果呢？关于对于这个问题，我也思考过。而且我们都知道2008那篇关于RPKM的文章更是用实验结果证明了，RPKM是一个合适的统计量，符合qPCR的验证结果。但归根到底，我觉得眼见未必为实，很多实验其实是表象的，我们更应该从其本质意义和原理上去考虑。FPKM/RPKM之所以看起来会是一个合适的值，我想主要原因有二： 其一，它们和TPM之间存在一定的正比关系。这可通过它们各自的数学计算方程式看出来（以RPKM的计算为例）： $$RPKM = \frac{T \times 10^3}{ N \times read_l } \times TPM$$ 而且在同一个样本内部由于T，N和$read_l$实际上都是定值，因此同个样本内的RPKM和TPM是可以恒等转换的。然而在样本与样本之间就不行，因为不同样本T和N是不同的（假定测序长度$read_l$都一样），这就导致它们之间的转换因子大小不一样！ 如以上例子，对于样本X，TPM转换到RPKM的转换因子为：0.0284，但在样本Y中，它的转换因子却是：0.8092。而由于这个基础标准的改变，导致其原本所要描述的“转录本丰度”变得不可比较。然，这其实不是最根本的原因，更本质的原因是，这个转换会对本来已经正确标准化了的结果——TPM，再次做了一次无意义的不等变换，最终导致了结果不可解释。如何理解呢，后文会有补充，这里先简单说一下：这个数学转换式子仅是告诉了我们这样子来计算是可行的，但是在RNA-seq的实际应用场景中，它其实是无生物（或物理）意义的； 其二，实验验证的精度是有限的，常用的qPCR也只能给出定性的比较结果，而且实验验证也未必总能成功。 总结现在回过头来总结一下。事实上，FPKM/RPKM最大的问题就在于其无意义性。我们所要表达出来的任何统计量，它的变化都应该要能对应到物理或生物过程中的变化，如果做不到这一点，那么这个统计量往往都是无意义的，用它得到的结果就算看起来符合预期也只不过是数值上的巧合，本质上是不可解释的。FPKM/RPKM的分母($N/10^6$)并不具有任何形式的生物意义，它所能表达出来的这个量，只能代表测序深度的变化，而无法作为表达生物过程的量，比如无法代表（等价代表）样本中转录本的总量。 一个统计量该如何计算，说到底都只是一个“术”的问题，而我们应该尽可能在接近其本质意义的地方去确定。 FPKM/RPKM和TPM存在一定的正比关系，因此我们在使用FPKM/RPK时，有些时候确实也能获得可以被实验所验证的“好”结果，但其实它是一个橡皮筋，它的单位刻度是会随着样本的不同而改变的。到头来，样本之间的差异比较实际上也只是在不同的标准下进行的，这样的比较就算得到了所谓的“好”结果，那又有什么意义，根本就是个错误的东东。想想就是由于这种统计量，我们一定已经获得了许多的假阳性结果，同时也肯定错过了许多本来真正有意义的差异，真是弯路走尽也不知，而且还浪费了大堆的心情和时间。 这篇文章：A comprehensive evaluation of normalization methods for Illumina high-throughput RNA sequencing data analysis. Briefings in Bioinformatics.10.1093/bib/bbs046. 对7种主要的RNA-seq标准化方法（但不包含本文提到的TPM）做了一个详细的比较，它用实际结果进行比较（不同于本文所用的数学方式）也得出了RPKM/FPKM这些统计量应该被摒弃的结论，因为它所描述出来的结果是最不合理的，其实所有类似于RPKM/FPKM的统计量在描述转录本丰度的时候都应该被摒弃。 【注】本文已同时发布于泛基因fungenomics以及我的个人微信公众号。 http://bib.oxfordjournals.org/content/early/2012/09/15/bib.bbs046.fullhttp://www.ncbi.nlm.nih.gov/pubmed/22872506]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>RNA</tag>
        <tag>RPKM</tag>
        <tag>FPKM</tag>
        <tag>TPM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目前最好最完整的SOAPdenovo使用说明]]></title>
    <url>%2F2015%2F07%2F09%2FThe-best-manual-for-soapdenovo2.html</url>
    <content type="text"><![CDATA[由于丹麦人国家基因组项目的原因，近期我整理了一份关于SOAPdenovo2的使用说明，内容包括了程序使用、参数的详细说明、参数如何调整、各个主要输出文件的格式说明等，而且我敢说这是目前最好最全的！ 简介SOAPdenovo（目前最新版是SOAPdenovo2）是一种应用de Bruijn graph组装短read的方法，它以kerm为节点单位，利用de Bruijn图的方法实现全基因组的组装，与其他短序列组装软件相比，它可以进行大型基因组，比如人类基因组的组装，组装结果更加准确可靠，可以通过组装的结果非常准确地鉴别出基因组上的序列结构性变异，为构建全基因组参考序列和以低测序成本对未知基因组实施精确分析创造了可能。 下载地址：http://soap.genomics.org.cn/soapdenovo.html 安装： 下载SOAPdenovo的压缩包 解压缩 将得到可执行文件SOAPdenovo和一个配置文件的模板example.contig 使用程序及参数SOAPdenovo可以一步跑完，也可以分成四步单独跑，一步跑完的脚本: 1./SOAPdenovo all -s lib.cfg -K 29 -D 1 -o ant &gt;&gt;ass.log 四步单独跑的脚本:1234./SOAPdenovo pregraph -s lib.cfg -d 1 -K 29 -o ant &gt;pregraph.log./SOAPdenovo contig -g ant -D 1 -M 3 &gt;contig.log./SOAPdenovo map -s lib23.cfg -g ant &gt;map.log./SOAPdenovo scaff -g ant -F &gt;scaff.log 参数说明1用法：/PathToProgram/SOAPdenovo all -s configFile [-K kmer -d KmerFreqCutOff -D EdgeCovCutoff -M mergeLevel -R -u -G gapLenDiff -L minContigLen -p n_cpu] -o Output 12345678910111213-s STR 配置文件-o STR 输出文件的文件名前缀-g STR 输入文件的文件名前缀-K INT 输入的K-mer值大小，默认值23，取值范围 13-63-p INT 程序运行时设定的线程数，默认值8-R 利用read鉴别短的重复序列，默认值不进行此操作-d INT 去除频数不大于该值的k-mer，默认值为0-D INT 去除频数不大于该值的由k-mer连接的边，默认值为1，即该边上每个点的频数都小于等于1时才去除-M INT 连接contig时合并相似序列的等级，默认值为1，最大值3。-F 利用read对scaffold中的gap进行填补，默认不执行-u 构建scaffold前不屏蔽高覆盖度的contig，这里高频率覆盖度指平均contig覆盖深度的2倍。默认屏蔽-G INT 估计gap的大小和实际补gap的大小的差异，默认值为50bp。-L 用于构建scaffold的contig的最短长度，默认为：Kmer参数值 ×2 使用方法及示例（1）示例1SOAPdenovo all -s HCB.lib -K 25 -d -o test （2） 输入文件configFile，配置文件内容如下，非程序生成，需要软件使用者自己配置。各个说明参考如下：1234567891011121314151617181920212223242526272829303132333435363738#maximal read length （read的最大长度）以“#”开头的行是注释内容max_rd_len=50 #该值一般设置的比实际read读长稍微短一些，截去测序最后的部分，具体长度看测序质量[LIB]#文库信息以此开头avg_ins=200#文库平均插入长度，一般取插入片段分布图中给出的文库大小reverse_seq=0#序列是否需要被反转，目前的测序技术，插入片段大于等于2k的采用了环化，所以对于插入长度大于等于2k文库，序列需要反转，reverse_seq＝1，小片段设为0asm_flags=3#该文库中的read序列在组装的哪些过程（contig/scaff/fill）中用到设为1：只用于构建contig；设为2：只用于构建scaffold；设为3：同时用于构建contig和scaffold；设为4：只用于补洞【注意】短插入片段(&lt;2K)的设为3，同时用于构建contig和scaffold，长插入片段(&gt;=2k)设为2，不用于构建contig，只用于构建scaffold，454single 长reads只用于补洞。rank=1#rank该值取整数，决定了reads用于构建scaffold的次序，值越低，数据越优先用于构建scaffold。设置了同样rank的文库数据会同时用于组装scaffold。一般将短插入片段设为1；2k设为2；5k设为3；10k设为4；当某个档的数据量较大时，也可以将其分为多个档，同样，当某档数据量不足够时，可以将多个档的数据合在一起构建scaffold。这里说的数据量够与不够是从该档的测序覆盖度和物理覆盖度两个方面来考虑的。pair_num_cutoff=3#可选参数，pair_num_cutoff该参数规定了连接两个contig 或者是pre-scaffold 的可信连接的阈值，即，当连接数大于该值，连接才算有效。短插入片段(&lt;2k)默认值为3，长插入长度序列默认值为5map_len=32#map_len该参数规定了在map过程中 reads和contig的比对长度必须达到该值（比对不容mismacth和gap），该比对才能作为一个可信的比对。可选参数，短插入片段(&lt;2k)一般设置为32，长插入片段设置为35，默认值是K＋2。q1=/path/**LIBNAMEA**/fastq_read_1.fq#read 1的fastq格式的序列文件，“/path/**LIBNAMEA**/fastq_read_1.fq”为read的存储路径q2=/path/**LIBNAMEA**/fastq_read_2.fq#read 2的fastq格式的序列文件，与read1对应的read2文件紧接在read1之后）f1=/path/**LIBNAMEA**/fasta_read_1.fa#read 1的fasta格式的序列文件f2=/path/**LIBNAMEA**/fasta_read_2.fa#read 2的fasta格式的序列文件q=/path/**LIBNAMEA**/fastq_read_single.fq#单向测序得到的fastq格式的序列文件f=/path/**LIBNAMEA**/fasta_read_single.fa#单向测序得到的fasta格式的序列文件p=/path/**LIBNAMEA**/pairs_in_one_file.fa#双向测序得到的一个fasta格式的序列文件 输出文件及说明SOAPdenovo 分四部分别对应的输出文件：12341. pregraph 生成7个文件 *.kmerFreq *.edge *.preArc *.markOnEdge *.path *.vertex *.preGraphBasic2. contig 生成4个文件 *.contig *.ContigIndex *.updated.edge *.Arc3. map 生成3个文件 *.readOnContig *.peGrads *.readInGap4. scaff 生成6个文件 *.newContigIndex *.links *.scaf *.scaf_gap *.scafSeq *.gapSeq *.contig：contig序列文件，fasta格式； *.scafSeq：fasta格式的scaffold序列文件，contig之间的gap用N填充； 对于得到的*.scafSeq文件还需要用GapCloser去合并其中的gap，最后的contig文件则是对补洞之后的scaffold文件通过打断N区的方法得到。 以上两个文件是组装结果中最主要的输出。 *.scaf：包括scaffold中contig的详细信息；在scaffold行中包括scaffold名字、contig长度和该scaffold长度。在contig行包括contig名字、contig在scaffold上的起始位置、正反链、长度和contig间的链接信息; *.links：contig间的pair-end连接信息; *.readOnContig：reads在contig上的位置; *.peGrads： 主要可以通过调整本文件中的参数来显示构建scaffold所用到的插入片段库的个数，总共要到的read数，最长的read的长度，每个库对应的哪些reads，rank设置，pair_num_cutoff设置。例如： 1234567891011grads&amp;num: 10 522083934 70323 104577616 1 3334 180770522 1 3345 226070520 1 3486 361955834 2 32200 392088076 3 52290 422272580 3 52400 445522690 3 54870 475666064 4 59000 511030930 5 89110 522083934 5 5 该文件中共分成4列。组装的配置文件中有n个文库，该文件则有n+1行，且按照文库大小顺序排列。第1行中，第二三四列分别是 所用文库，reads总数和组装中用到的最长的reads长度。第2行中，四列分别是文库大小，文库中的reads数目，该文库reads用到的rank等级和该文库中reads用到的pair_num_cutoff。第3～n+1行，四列分别是文库大小，文库中的reads数目加上前面的文库中的reads总数，该文库reads用到的rank等级和该文库中reads用到的pair_num_cutoff。如果配置文件中没有设置pair_num_cutoff，即使用默认参数，则最后一列显示为0。 对于SOAPdenovo的每个步骤都有日志文件输出，要保存好日志文件，日志文件中包含有很多有用的信息。 SOAPdenovo日志输出说明1）pregraph.log: 其中有很多的统计信息，包括构建debruijn-graph时用到多少reads数，构图中生成了多少uniq的kmer以及设置-d参数后去除了多少kmer。在pregraph中，可选参数有 –R –K –d 结果如： 125467781332 nodes allocated, 70662750348 kmer in reads, 70662750348 kmer processed3283081670 kmer removed 其中Kmer 数是取决于所设k值大小以及数据量，nodes数即特异性的kmer数目，当nodes数目过高（一般和基因组大小差不多大小），可能是数据的错误率比较高，也可能是存在杂合。若nodes数目偏小，并且kmer数目很多，则基因组本身可能存在一定的重复度。对于k值的选取，当数据量充足时（&gt;=40X），植物基因组一般采用大kmer会有比较好的效果，而对于动物基因组，k值一般多取27和29则足够。kmer removed表示的 –d 参数所去除的低频的kmer。 2）contig.log: contig 中，可选参数 –R –D –M，注，-R 参数的选定，必须pregraph和contig中同时选择才有效。结果例子： 116430183 pairs found, 2334584 pairs of paths compared, 1674493 pairs merged 从merged的数量可以作为估计杂合以及测序错误的程度。 12sum up 1932549703bp, with average length 1170the longest is 36165bp, contig N50 is 2871 bp,contig N90 is 553 bp 3）map.log:12Output 415219610 out of 1956217742 (21.2)% reads in gaps1661094582 out of 1956217742 (84.9)% reads mapped to contigs 一般情况下，reads in gap的比例和map to contig 的比例总和大于1。可能是因为reads map到多个地方都被算在其中的原因。当map to contig的比例很高（80%左右时），但是组装效果并不很好，可能是重复序列比较多。reads in gap比例较高（大于40%），是因为基因组组装的较碎，gap区域较多。map_len 默认值=K+5，当默认值大于设置的map_len时，以默认值为准，当默认值小于map_len值时，设置的map_len为准。 4）scaff.log:1average contig coverage is 23, 5832270 contig masked 构建scaffold是对高频覆盖的contig进行屏蔽（即频率高于average contig coverage的两倍的contig不用于构建scaffold），从这里可以看出组装的基因组一定的重复情况。 12estimated PE size 162, by 40034765 pairson contigs longer than 173, 38257479 pairs found,SD=8, insert_size estimated: 163 173 是配置文件中该文库的insertsize，163 是根据reads map到contig上的距离的估计值，8是这个分布的标准偏差。一般考虑 比对上去的pair数目和SD值。若pair对数很多且SD值很小（小片段文库数据不超过三位数，大片段文库数据部超过500），那我们一般可以将配置文件中的文库插入片段的值改对短插入片段文库（=2K），因为是把reads map到contig上，若最长contig较短时，可能找不到成pair比对上去的reads，这时，无法估计文库大小，需要自己将大片段一级一级的map到前一级的组装结果上，然后再分析大片段文库的插入片段大小。注，需要调整insertsize信息时，只需要修改 .peGrads文件中的第一列，然后删除.links文件，重新跑scaff这一步即可。即构建scaffold时，主要是根据*.links文件的信息进行连接。 12Cutoff for number of pairs to make a reliable connection: 31124104 weak connects removed (there were 4773564 active cnnects)) Cutoff for number是在配置文件中设的pair_num_cutoff值，weak connects是低于这个值被认定为无效的连接数，active connects是满足cutoff的连接数，根据这个数值可对pair_num_cutoff做调整 1Picked 25241 subgraphs,4 have conflicting connections conflicting connections 是表示构建scaffold时的矛盾数，矛盾数比较高（&gt;100）时，可根据前面的有效连接数，适当提高pair_num_cutoff值，即提高scaffold连接要求的最少关系数 12182483 scaffolds&amp;singleton sum up 1990259817bp, with average length 10906the longest is 6561520bp,scaffold N50 is 836795 bp, scaffold N90 is 157667 bp scaffold 统计信息，将是根据rank分梯度的统计:1Done with 13301 scaffolds, 2161915 gaps finished, 2527441 gaps overall -F 参数补洞的统计信息。 参数调整一般组装时需要调整的参数，主要分两种： 一种是针对脚本中的参数改动：如调整 -K -R -d -D -M-K 值一般与基因组的特性和数据量相关，目前用到的SOAPdenovo软件主要有两个版本，grape1123和grape63mer，其中grape1123是最新版的组装软件，K值范围13-31，grape63mer是可以使用大kmer的组装版本，K值范围13-63。 【经验】：植物基因组的组装采用大kmer效果会比较好（要求短片段reads长度75bp），动物基因组很少有用到大kmer后有明显改进效果的，且动物基因组的组装K值一般设置为27和29较多。 -R参数，对于动物基因组，R参数一般不设置，植物基因组由于较多的repeat区，则设置R参数后，效果更好。注意，设置-R时，一般使用-M 的默认值。（熊猫基因组组装时得出的结论） -M 参数，0-3,默认值1。一般杂合率为千分之几就设为几。熊猫基因组组装时-M 2 。 -d 参数，对于没有纠错，没有处理的质量又较差的原始数据，kmer的频数为1的很多的数据的组装，一般设置为-d 1 则足够。对于处理过，或者是测序质量较好的数据，可以不用设置。数据量很多时，也可以以-d 参数去除部分质量稍差的数据。 -D 参数，默认为1，一般不用另行设置。 第二种，从map这一过程去调节参数。可以调整配置文件的map_len的值和调整文件*.peGrads。 当文库插入片段分布图中文库大小与实验给出的文库大小差异很大时，调整*.peGrads文件中的插入片段大小。 根据每一档数据的数据量去调整文库的rank等级。当该文库的数据量很多或者是在构建scaffold的过程中的冲突数很多时，可是适当的调大第四列 的pair_num_cutoff，把条件设置的更严一些。 内存估计SOAPdenovo的四个步骤消耗的内存是不一样的，其中第一步消耗的内存最多，使用没有纠错的的reads，(K&lt;=31)第一步消耗的内存在基因组大小的80－100倍左右，纠过错则在40－50倍左右，第二步相对消耗的内存会少很多，第三步消耗的内存是仅次于第一步的，在第一步的一半左右，第四步消耗的内存也会比较少。对于CPU的使用，默认是8个，如果申请内存时申请一个计算节点的所有内存测将CPU就设置为该计算节点的CPU个数充分利用计算资源，如果仅申请一个节点的部分内存则根据实际情况考虑。对于大kemr(K&gt;31)其内存使用是(k&lt;=31)的1.5倍左右，有时甚至更多，要充分估计内存的使用，在第一次运行的时候考虑不能太保守。 常见错误1）配置文件中read存储路径错误 只输出日志文件。pregraph.log中的错误信息：“Cannot open /path/LIBNAMEA/fastq_read_1.fq. Now exit to system…” 2）-g 后所跟参数与pregraph（第一步） -o 后所跟参数名不一致 contig map scaff 这三个步骤都只是输出日志文件。 contig.log中的错误信息：“Cannot open *.preGraphBasic. Now exit to system…” map.log中的错误信息：“Cannot open *.contig. Now exit to system…” scaff.log中的错误信息：“Cannot open *.preGraphBasic. Now exit to system…” 3）从map开始重新跑时，需要删除*.links文件，否则会生成core文件，程序退出。 【注】仅同时发布于泛基因fungenomics]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>组装</tag>
        <tag>SOAPdenovo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[de Bruijn graph组装基因组的时候，Kmer数必须是奇数的原因]]></title>
    <url>%2F2015%2F05%2F22%2Fwhy-Kmer-count-should-be-odd-number-in-assembly-algorithm.html</url>
    <content type="text"><![CDATA[根本原因就是为了避免导致 正反链混淆。 一开始，我并没弄明白，后来仔细想想也终于懂了。 如果kmer是偶数，我们会发现基因组上有些序列（如，CGCGCGCG，kmer=4）的Kmer在反向互补后得到的序列仍然是它自身！而这是不能允许的，因为这将导致你无法区分某段序列的kmer到底是属于它自身还是说只是来自于它的互补链！！这会给解de Bruijn graph带来极大的困难！ 或许你会觉得“为什么我需要纠结于序列是不是来自互补链呢？毕竟双链DNA的正反链是严格互补的啊，目前的基因组组装技术也就是把它们合并装在一起的呀！”，若你能这样来理解其实是没问题的，但前提却是基因组必须能够被一次性完整地（至少是非常接近完整）测出来，这时的测序深度只需是1就可以了——如果都已经把基因组完整测序出来了，那还要组装个屁呀 ！！！！！ (╯‵□′)╯︵┻━┻。 并且目前的NGS测序技术也做不到通测，一般来说都是测出成千上百万千万亿万个小小的片段（也叫read，长度一般是100bp-250bp）。同时基因组会被反复测很多层，所以这个时候构建kmer的单位实际上就是对这些read进行的，具体的操作就是按照kmer的长度把这些read切割成更小的片段。这个时候在构建de Bruijn graph时，能够保证正确地把同属于一条read上的kmer连接起来，就显得极其重要了呀！我们总不能一会儿把A kmer正确地连到它自己所在的read，一会儿又连到它互补链的read上去呀！这就是为何kmer不能是偶数的原因了，因为只有奇数，才能保证每个kmer序列的反向互补Kmer与自身是不同的，而这个不同的真正意义就是为了避免正反链混淆，如 ：5-mer的 CGCGC，反向互补后是 GCGCG， 它们是不同的；这就不会像 4-mer，CGCG发现它反向互补后仍然是CGCG！！ 最后上一个来自GenomeResearch的图，给大家欣赏下一个Repeat序列的组装过程。]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>组装</tag>
        <tag>de Bruijn graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Python中调用C++模块]]></title>
    <url>%2F2015%2F04%2F03%2F2015-04-03-How-Does-Python-call-Cpp-module.html</url>
    <content type="text"><![CDATA[在Python中成功实现了对原来C++代码模块的复用！这个好处多多，Python写得快，C++跑得快，那就是既快又快了！方法很简单，以至于我能够用一张截图记录下整个过程（点击图片看大图）！ 其实，注意到，必须在原来的C++代码后面添加extern “C”来辅助（C则不需要，这也是与复用C代码时最大的不同点），不然Python在调用这个构建后的动态链接库时是找不到原来的方法或者函数的，说到底还都是因为当前Python的设定中只能调用C函数，而不能直接调用C++的方法，因此extern “C”封装的函数必须是C风格的，也即就是说，函数中的所有参数（输入参数和返回参数）都不能包含任何非C定义的类型（包括不能包含只属于C++的类型）。]]></content>
      <categories>
        <category>信息图</category>
        <category>编程技术</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[何不争当数据科学家？]]></title>
    <url>%2F2015%2F03%2F22%2F2015-03-22-how-to-become-a-data-scientist.html</url>
    <content type="text"><![CDATA[当今世上大数据横行，于是duang许多人都想成为数据科学家。 分享两张路线图： （1）成为数据科学家的8个简单步骤！ （2）数据科学家之路 OK，步骤就是这样，图样说简单，过程其实不简单，但却也都是可操作的！这种东西看多了，只觉得什么都是虚的，这些也都是术，关键还是在人，既然有抱负，地图就在那了，接下来，就看你是否有耐心，是否愿意花时间走下去。奋斗吧，Fighting！ http://blog.datacamp.com/how-to-become-a-data-scientist]]></content>
      <categories>
        <category>信息图</category>
      </categories>
      <tags>
        <tag>数据科学家</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二代测序：碱基平衡性与barcode选择]]></title>
    <url>%2F2015%2F02%2F01%2F2015-02-01-base-balance-and-barcode-selection.html</url>
    <content type="text"><![CDATA[这是转载过来的一篇文章，虽然基础，但却觉得是很重要的知识,所以便记录了下来，原始出处来自微信公众号“基因测序资讯AGCT”。 碱基平衡性 碱基复杂度与碱基多样性是一个意思；复杂度高，碱基即平衡。低多样性(low diversity)即碱基不平衡，指碱基的组成太单纯了，种类少。碱基复杂度本来无关紧要，从前除了设计PCR的时候考虑高GC(GC-rich)以外，基本没人思考这个问题，没人觉得这是一个问题。随着Illumina的二代测序技术风靡全球，独占鳌头，这个不起眼的概念意外地变得重要起来。 一、概念 对于一个基因来说，它所包含的碱基种类越多，则碱基复杂度越高；如果各种碱基的百分含量越接近一致，则碱基组成越平衡。 假设一个DNA片段，它的全部碱基都是A，AAAAAAAAAAAAAAAAAAAAAAAA，显然其碱基组成是极度不平衡的。 DNA碱基有4种：AGCT。所以碱基最平衡的情况就是：%A=%G=%C=%T=25%，比如这样的DNA片段：AGCTAGCTAGCTAGCTAGCTAGCTAGCTAGCT。 以上是从纵的方面讲的。对于二代测序，更重要的是横的方面。假设12个基因整整齐齐站成一排，第一个位置的12个碱基如果都是A，复杂度太低，严重不平衡；如果A和G各有6个，虽然平衡了，但是复杂度还是不够；如果AGCT各有3个，最复杂，也最平衡；如果A3个G4个C4个，T1个，虽然复杂，但是严重不平衡。 二、影响 4张滤色片，在4个波长处收集信号，然后合成，进行cluster定位及其他运算。如果缺少一种碱基，该波长的照片就是全黑的，没有信号，无法完成图片合并以及cluster定位，导致数据浪费。 需要特别注意碱基复杂度的二代测序应用:PCR产物测序，特别是用于鉴定细菌、真菌以及其他物种的16S rRNAPCR产物测序；小RNA测序；甲基化测序。 三、增加碱基复杂度的方法 文库： 把不同的文库混合在一起。 如果没有其他文库，那么掺入人基因组DNA文库、人外显子组文库或者PhiX标准品。这些都是已知碱基平衡的。 引物： 对于PCR产物来说，只要引物长度不同，就能自然错开，增加碱基复杂度。 采用多对序列不同的引物来完成扩增，然后将产物混合在一起。 Barcodes： 仔细挑选barcode组合，确保每个位置都有3-4种碱基且碱基分布均匀。 ##barcode 选择很多情况下，我们需要把多个样本混合在一起，在同一个通道(lane)里完成测序。像转录组测序、miRNA测序、lncRNA测序、ChIP测序等等，通常每个样本所需要的数据量都比较少，远少于HiSeq一个通道的产出能力，混合样本是非常常见。以转录组测序为例，一个样本测序60 M reads (8G PF data) ，就能够满足绝大部分研究所需。而HiSeq2500-PE125的一条通道，使用V4试剂，数据产出&gt;480 M reads。为了充分利用测序仪产能，节约成本，需要把7~8个RNA样本混合起来。 为了能够把测序数据按样本分离（de-multiplexing），在构建文库(library)的时候，需要用不同的标签序列(index, 也叫barcode)对文库进行标记。只有文库作了记号，数据才能区分。 Barcode的选择是一门技术活。如果barcode组合不佳，标签序列测序质量下降，部分或者全部标签碱基识别不正确，将导致部分数据无法归属到任何一个样本，成为undetermined数据，造成浪费。 一、如何判断barcode组合好坏 碱基平衡。好的barcode组合必须是“4种碱基达到平衡”的，或者说碱基复杂度高。具体就是：a. 在一组barcode的每一个位置，同时存在A、G、C、T四种碱基，不缺少任何一种碱基；b. 这4种碱基的比例接近，最好各1/4，分别为25%左右，没有任何一种碱基特别多或者特别少。 激光平衡。 受客观条件限制 a.试剂盒提供的barcode种类有限b.有些barcode已经被其他样本占用，导致可选的余地受限制，这就导致barcode组合经常无法达到理想的碱基平衡要求。退而求其次，要力保“红绿激光达到平衡”。 在所有型号的Illumina测序仪中，A和C两种碱基共用一种激光，由波长660 nm的红激光激发；G和T共用一种激光，由波长532 nm的绿激光激发。对于一组barcode的每一个位置，如果A＋C的总数与G＋T的总数相接近，可以在一定程度上弥补碱基不平衡的负面作用。 3、激光平衡是次优选择，不得已而为之。它虽然可以在一定程度上提高barcode测序质量，减少de-multiplexing出问题的可能性，但是并不是说，只要激光平衡了，测序数据的分离就一定不受影响。4、如果barcode组合碱基也不平衡，激光也不平衡，则de-multiplexing风险非常高。 二、Barcode组合举例 好的组合。 Illumina推荐的12个样本barcode组合如下。 编号 序列 01 ATC ACG 02 CGA TGT 03 TTA GGC 04 TGA CCA 05 ACA GTG 06 GCC AAT 07 CAG ATC 08 ACT TGA 09 GAT CAG 10 TAG CTT 11 GGC TAC 12 CTT GTA 以第一个位置（纵列）为例，A:G:C:T=3:3:3:3=1:1:1:1。实际上，该barcode组合每个位置的碱基比例都接近1:1，碱基平衡度近乎完美。 不好的组合 下面的组合有缺陷。比如说，第1个位置只有A和C两种碱基，A、C都属于红激光，导致绿激光没有信号，碱基和激光都不平衡。 AGT TCCACT GATACG AGCACT CCTCAA AAGCAA CCACAC CAG 三、Barcode碱基不平衡的后果 如果barcode组合的碱基组成不平衡，会导致测序进行到这些碱基时，软件对测序信号的处理出现障碍，不能准确地识别这些碱基(base-calling)，表现为QV值降低，%Q30曲线波动。 在这种情况下，运用生物信息软件对测序数据进行数据分离（de-multiplexing）出现困难，部分数据不能准确分离，成为undetermined 数据的一部分，造成undetermined数据增多，可分离的数据减少。 如果测序数据的总量很多，远大于全部样本数据量期望值的总和，则问题有可能不那么严重，全部或者大部分样本仍然可能分离到足够的数据量。 万一样本性质特殊，反应效率低；或者混合样本之间竞争和抑制严重，导致测序数据总量在期望值附近，余量很少；或者其中个别样本数据量特别少，这时如果undetermined数据比例过高，就会导致部分或者全部样本的数据量不够用。 混合样本补数据是一个非常麻烦的问题，成本极高。如果一组样本中只有个别样本需要补数据，由于文库是混合在一起的，其他样本也不得不跟着重测一次。这是困难之一。困难之二，如果数据缺口比较小，本来可以与其他样本混合，搭个便车，可是，进行第二次混合的时候，经常会遇到barcode冲突或者碱基不平衡，拼lane非常困难，往往要等很长时间，才有合适的机会。 四、实验证明de-multiplexing成功，该barcode组合今后是否一定好用 如果barcode组合碱基平衡，则无论样本怎么变，该组合一定好用。 如果barcode组合的碱基组成不理想，即使以前的实验证明好用，不等于今后一定好用。下一次测序效果可能好，也可能不好。 这是由于不同的项目样本不同，有可能导致两种后果： a.数据总量在期望值附近，余地不够多，de-multiplexing后部分样本数据量不够；b.如果新的样本本身也碱基不平衡，read 1测序质量很差，会影响到barcode和read2的测序质量。当然，情况b责任不在barcode，即使barcode很好，数据还是不够。 五、补救措施 如果满足以下两个条件： a. 混合样本的数据总量足够，只是由于barcode质量不好，导致de-multiplexing后部分或全部样本数据量不够；b. 排除QV值低的barcode碱基后，其余质量好的barcode碱基仍然足够用来区分全部样本； 那么，可以通过改变de-multiplexing算法来为每个样本获得尽量多的数据。比如去掉信号识别模糊的碱基，或者增加mismatch碱基的数目，重新运行de-multiplexing程序。 六、样本少于4种，不可能碱基平衡，怎么办 如果样本数少于4种，每一个位置的碱基最多只有3种，不可能碱基平衡，怎么办呢？这时一定要保证激光平衡。Illumina推荐了3种low-level pooling的barcode组合： 2个样本： :—:|:—-: #6|GCC AAT #12|CTT GTA 3个样本： :—-:|:—-: #4|TGACCA #6|GCCAAT #12|CTTGTA 6个样本： :—:|:—: #2|CGATGT #4|TGACCA #5|ACAGTG #6|GCCAAT #7|CAGATC #12|CTTGTA 这3种组合包含一个共同内核：6号和12号。6号和12号组合是百分百激光平衡的，每一个位置的碱基（纵列，即GC、CT、CT、AG、AT和TA）都分别属于不同的激光。只要barcode组合中包含6号和12号，就能满足最基本的要求，不至于颗粒无收。6号和12号是barcode组合的核心，不可或缺。]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>测序技术</tag>
        <tag>碱基平衡性</tag>
        <tag>barcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何选择入门编程语言]]></title>
    <url>%2F2015%2F01%2F22%2F2015-01-22-which-programming-language-should-I-learn-first.html</url>
    <content type="text"><![CDATA[这是一个很有意思的编程语言入门学习图谱，以不同人的目的为导向推荐不同程度的入门语言，还有一个最有意思的地方就是为每一个主流编程语言配上一个魔戒人物用以形象描述它的独特之处,非常有特点！点击图片看大图。]]></content>
      <categories>
        <category>编程技术</category>
        <category>信息图</category>
      </categories>
      <tags>
        <tag>入门学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何布局并管理好项目目录（2/2）]]></title>
    <url>%2F2015%2F01%2F14%2F2015-01-14-how-to-manage-the-project-data-2.html</url>
    <content type="text"><![CDATA[…书接上回… 不同类型的项目会有适合其特点的目录布局，很难说会有一个真正的标准，我这里特指的是基因组（或相关的）项目目录布局问题。 基因组项目目录结构的科学管理是一个非常重要的问题。如果不科学，东西一多更是成了灾难，很多时候你想找的东西并不是丢了，而是根本找不到了！有时更糟的是就算找到了，也会忘了是怎么来的，最后连自己都会怀疑它是不是真的就是正确的那一份！只得一轮又一轮地冥思苦想寻找着若隐若现的线索，甚至有时即使打算重做都不知道该怎么下手，效率真是极低，简直就是在浪费生命！这样下去根本没得玩，直到昨天我才意识到实际上要用程序设计的思想来管理每个项目的目录！还是A workflow for R这篇博文给我的灵感，它提出了一些规则： 透明（Transparency）：目录布局清晰明朗，逻辑清楚，整个结构一看就能明白。 易维护（Maintainability）：容易实行项目的修改和相关（如文件名、目录变换）调整。 模块化（Modularity）：任务之间都应该尽可能保持其独立性，每一个就只干好自己的事，不干涉其他任务，避免牵一发而动全身的情况发生，要有一一对应的关系，这样即便是需要修改也将会非常方便。 可移植（Portability）：项目可以很容易地移植到其他的平台或者系统中。 易复现（Reproducibility）：不论经过多久，都要能轻易重现原来的结果。 秒懂 (Efficiency)：无需多想就能明白项目执行过程中的相关细节，比如所要解决的每个问题是如何处理的和所用的工具是什么等等。 这几点完全说出了项目布局和目录管理所应达到的目的和状态，值得铭记于心多拿来参考参考。只是仅有理论，还是太过不着边际了点，有必要弄个例子来看看具体应该怎么做。 有那么一段时间以来，我的数据目录布局是这样的：虽然可以保持每次被调用的文件名字都是“sample.mat”弊病其实也是明显的，时间一久，真的很难想得起到底是哪个对哪个，每次都需要重新回忆。后来我看到了这篇文章，顿时觉得， 所以分享出来，不过我不会严格地按原文去翻译。]]></content>
      <categories>
        <category>规范和标准</category>
      </categories>
      <tags>
        <tag>项目管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何布局并管理好项目目录（1/2）]]></title>
    <url>%2F2015%2F01%2F13%2F2015-01-13-how-to-manage-the-project-data-1.html</url>
    <content type="text"><![CDATA[不同类型的项目会有适合其特点的目录布局，很难说会有一个真正的标准，我这里特指的是基因组（或相关的）项目目录布局问题。 基因组项目目录结构的科学管理是一个非常重要的问题。如果不科学，东西一多更是成了灾难，很多时候你想找的东西并不是丢了，而是根本找不到了！有时更糟的是就算找到了，也会忘了是怎么来的，最后连自己都会怀疑它是不是真的就是正确的那一份！只得一轮又一轮地冥思苦想寻找着若隐若现的线索，甚至有时即使打算重做都不知道该怎么下手，效率真是极低，简直就是在浪费生命！这样下去根本没得玩，直到昨天我才意识到实际上要用程序设计的思想来管理每个项目的目录！还是A workflow for R这篇博文给我的灵感，它提出了一些规则： 透明（Transparency）：目录布局清晰明朗，逻辑清楚，整个结构一看就能明白。 易维护（Maintainability）：容易实行项目的修改和相关（如文件名、目录变换）调整。 模块化（Modularity）：任务之间都应该尽可能保持其独立性，每一个就只干好自己的事，不干涉其他任务，避免牵一发而动全身的情况发生，要有一一对应的关系，这样即便是需要修改也将会非常方便。 可移植（Portability）：项目可以很容易地移植到其他的平台或者系统中。 易复现（Reproducibility）：不论经过多久，都要能轻易重现原来的结果。 秒懂 (Efficiency)：无需多想就能明白项目执行过程中的相关细节，比如所要解决的每个问题是如何处理的和所用的工具是什么等等。 这几点完全说出了项目布局和目录管理所应达到的目的和状态，值得铭记于心多拿来参考参考。只是仅有理论，还是太过不着边际了点，有必要弄个例子来看看具体应该怎么做。 …未完待续]]></content>
      <categories>
        <category>规范和标准</category>
      </categories>
      <tags>
        <tag>翻译</tag>
        <tag>项目管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[替换google字体，加快网站访问速度]]></title>
    <url>%2F2015%2F01%2F07%2F2015-01-07-Replace-google-fonts.html</url>
    <content type="text"><![CDATA[一段时间以来，这个博客的打开速度慢得出奇！本来只有很少的东西，ping的速度也在120ms左右，算是可以的了，怎么会这样！一开始我没有搞明白问题的根源。今天才突然醒悟到一定是google字体加载的问题！当时这个博客的主题是从yihui和Carl Chen那里抄来的（不用试了，这俩的页面是极难打开的了），因为懒，因为不认识网页语言，尤其是css，所以所有.css后缀什么的，我一概不去看！所以那几个css就成了我的一个暗区，现在问题很明显了，要修改的地方一定在那！目标锁定之后，那么着手处理吧！ 我页面的主要配置都在style.css和home.css，打开这两文件，搜索google，果然发现“fonts.googleapis.com”！那么剩下的就是要先找字体替换方案了，搜索一番后，发现原来360已经把整个的google字库下载下来放在了自己的服务器上了，这样就太好了！改起来也极其简单，只需要直接把所有的fonts.googleapis.com替换为fonts.useso.com,而且还不用做任何其他的修改，字体也还是原来的字体！ 完成后，发现这下页面打开的速度和以前相比根本就是两个网站！不禁感叹：“靠！怎么这么简单！” 参考 : http://www.cgtt.net/1323.html]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[修改二维码生成工具]]></title>
    <url>%2F2014%2F09%2F19%2F2014-09-19-Change-QRCode-Generator.html</url>
    <content type="text"><![CDATA[本来博客在改版后，短时间内，我是不想做修改了的，回国后，没办法，原来的用于生成二维码的代码必须得换！根本就是失效了。原因也是很明朗的，二维码生成代码用的API是google的，稳定，强大……但在墙面前显得跟土一样。有时候我总觉得，是不是因为有了墙，大家的技术反而都要强那么一点了——魔高一尺道高一丈！只是有些时候也不得不拜倒了。总之，为了速度，为了大家的良好体验，换了！google的跟踪代码，我也取消了，换成了百度，不然网页死活打不开的，这是题外话了。 总之货比三家，最后我做了个比较大众化的选择——联图网的服务接口！说是接口，听起来似乎很高大上的样子，其实也就是一小段代码罢了，无需紧张。下面就是主题了。 API接口调用代码如下： 123456&lt;!-- 网页自动生成二维码的代码 --&gt;&lt;script type="text/javascript"&gt; thisURL = document.URL; strwrite = "&lt;p align='center'&gt;&lt;img src='http://qr.liantu.com/api.php?w=120&amp;m=2&amp;text=" + thisURL + "' alt='QR Code'/&gt;（传送门）&lt;/p&gt;" document.write( strwrite ); &lt;/script&gt; 还和上次博客改版中提到的一样，把上面这段代码完全替换掉原来qrCodeGenerate.md中的内容就ok了。 为了方便大家看的明白，这里需要对上面这段代码中的一些重要参数做些说明： http://qr.liantu.com/api.php? 是联图网的QR code API接口地址； w：二维码图片宽度，根据需要自行修改，如w=120，表示生成一个120×120像素的正方形二维码图片； m：二维码静区（外边距），一般设为比较小的数，我这里设置为2； text：二维码数据信息， 我这里设置为对应页面的URL地址； alt：二维码图片描述， 这个就没什么讲究了。 总体的效果请看文章末尾的二维码图片。 参考资料： 免插件自动生成wordpress文章二维码图片 很感激这篇文章给我的启发。]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>网站插件</tag>
        <tag>google</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅述CNV——我怎么会是我]]></title>
    <url>%2F2014%2F08%2F16%2F2014-08-16-Slightly-Introduce-CNV.html</url>
    <content type="text"><![CDATA[什么是CNV CNV的全称是Copy Number Variantion ，但这里特指“人类基因组拷贝数变异”。我们知道人类基因组是由60亿个化学碱基（核苷酸）所构成的。这60亿个化学碱基一共组合成了23对（46条）染色体，在每一对染色体中都有两条姐妹染色单体，它们分别遗传自父亲和母亲。在这些染色体中包含着大约30000个编码基因。鉴于人类基因组是一个二倍体，那一般来说，我们会自然而然地认为每一个基因都应该恰好有2份拷贝，即姐妹染色单体上各有一份。但，实际的情况却并非如此。近期的研究也表明，基因组上有些长度较大的DNA序列片段——长度通常在1000bp至1Mbp，存在着反常的拷贝次数。这样的一种现象就称之为拷贝数变异，也就是英文的CNV。这种拷贝数变异会导致受其影响的基因表达失调。举个例子来说，比如我们一直以为在每个人中这些基因都只有2份拷贝，但是现在却会在有些人中看到它们只有1份，或是3份，甚至更多。更为罕见的情况是在有些人中甚至存在着两条染色单体中同时丢失某个基因的情况（见下图）。 大家从图中可以看到，大多数的基因确实只有2分拷贝，但也存在着反常的情形（红色高亮的部分）（译者：这个图只是示意而已，实际的情形可不一定是这样分布的）。 CNV为什么那么重要 明确了CNV是什么之后，接下来的话题便是要明白CNV为什么重要了。应该说正是由于DNA序列上的差异使得我们每个人在这个世界上都是独一无二的。当然了，这些独一无二的特性还包括了对不同疾病易感程度的差异和不平等性。以前的看法认为单核苷酸多态性（SNP）是DNA遗传差异的最主要和最重重要的来源。然而，近期的研究成果中我们却看到，人类基因组CNV变异的序列，在长度上至少已经是单核苷酸变异（SNP）总长的3倍（译者：原文虽然这么说，但其实并不能因此就简单的以为CNV在遗传差异上的贡献会是SNP的3倍，这还和它所能表现出来的效应相联系）。并且考虑到发生CNV的序列常常围绕在基因的周围，这很可能暗示了它们在人类疾病和药物反应中扮演着一个相当重要的角色。另外，能弄明白人类基因组CNV的发生机制也将有助于我们更好的了解自身基因组的进化历程，以便搞清楚“我为何不同，我之所以是我”的深层原因。 这里是原文。]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>变异检测</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多Github账号的使用问题]]></title>
    <url>%2F2014%2F08%2F10%2F2014-08-10-Multiple-Github-In-One-Machine.html</url>
    <content type="text"><![CDATA[同一台电脑，或者同一个（大型机）服务器账号下要使用多个Github账号，该咋办？ 如果是单用户(single-user)，很方便，默认拿id_rsa与你的github服务器的公钥对比；如果是多用户（multi-user）如user1,user2,那么就不能用在user2的身上了，这个时候就要配置一下了： 1、新建user2的SSH Key 12345#新建SSH key(若有需求，注意不要误删其他的id_rsa key)：$ cd ~/.ssh # 切换到~/.sshssh-keygen -t rsa -C mywork@email.com # 新建SSH key# 设置名称为id_rsa_hshujia (设置别的名字，防止因名字冲突而误删其他的id_rsa)Enter filein which to save the key (~/.ssh/id_rsa): id_rsa_hshujia 2、新密钥添加到SSH agent中 因为默认只读取id_rsa，为了让SSH识别新的私钥，需将其添加到SSH agent中： 添加至SSH agent1ssh-add ~/.ssh/id_rsa_hshujia 如果出现Could not open a connection to your authentication agent的错误，就试着用以下命令：1ssh-agent bash &amp;&amp; ssh-add ~/.ssh/id_rsa_hshujia 3、修改config文件 在~/.ssh目录下找到config文件，vi打开进行编辑，如果没有就创建：12# 创建configtouch config 然后按如下形式修改： 12345678910111213# 该文件用于配置私钥对应的服务器# Default github user(first@mail.com)**Host github.com HostName github.com User "user1" IdentityFile ~/.ssh/id_rsa # second user(second@mail.com) # 建一个github别名，新建的帐号使用这个别名做克隆和更新Host hshujia.github.com HostName github.com User "user2" IdentityFile ~/.ssh/id_rsa_hshujia 其规则就是：从上至下读取config的内容，在每个Host下寻找对应的私钥。这里将GitHub SSH仓库地址中的git@github.com替换成新建的Host别名如：hshujia.github.com，那么原地址是：git@github.com:user/Mywork.git，替换后应该是：git@hshujia.github.com:user/Mywork.git. 以下是我在（大型机）服务器上config文件的具体内容： 4、打开新生成的~/.ssh/id_rsa_hshujia.pub文件，将里面的内容添加到GitHub后台，此处默认大家懂得如何添加，就不详述了。 完成之后，在终端命令行中测试： 12ssh -T git@hshujia.github.com # 检验key是否已被成功添加到Github后台Hi hshujia! You&apos;ve successfully authenticated, but GitHub does not provide shell access. # 若能看到类似于这样的一句话就说明已经成功添加。 5、若以前是Global配置，则取消Global配置。 因为git pull or git push 的时候识别的是邮箱，多个github账号，就有多个邮箱，我们自然不能使用global的user.email了。 1234567# 1.取消global git config --global --unset user.namegit config --global --unset user.email# 2.在各个对应repo的目录下，设置项目repo自己的user.email和user namegit config user.email "xxxx@xx.com"git config user.name "xxx" 6、应用（例子） 写个Hello world 测试一下。打开登陆自己的账号，并新建一个repo，我这里命名为hello-world，创建完成之后，我在命令行中的具体操作如下: 1234567git initgit add README.mdgit config user.name hshujiagit config user.email hshujia@qq.comgit commit -m "first commit"git remote add origin git@hshujia.github.com:hshujia/hello-world.git # 注意修改git@后面的名字 git push -u origin master 这里唯一需要注意的就是多了两个git config用于告知该repo是属于哪一个user的；还有就是git remote add origin后面的host名字需要做相应的修改，具体的情况也已经在上面的代码中指明了。 参考资料 git初体验（七）多账户的使用 Git的多账号如何处理]]></content>
      <categories>
        <category>编程技术</category>
      </categories>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人类不是从猴子进化而来]]></title>
    <url>%2F2014%2F07%2F19%2F2014-07-19-Human-did-not-evolved-from-monkey.html</url>
    <content type="text"><![CDATA[问：如果人类是从猴子进化而来，那为什么猴子依然存在？ 答：我们不是从猴子进化而来的。我们（人和猴子）是从一个共同的祖先进化而来。 原文出处：sufuns 译文出处：科学公园–RhettZhang]]></content>
      <categories>
        <category>物种进化与迁徙</category>
      </categories>
      <tags>
        <tag>自然选择</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客大改版：添加评论，二维码生成，数学公式的显示，添加分析代码等]]></title>
    <url>%2F2014%2F07%2F18%2F2014-07-18-Change-blog-template.html</url>
    <content type="text"><![CDATA[目录 &nbsp;&nbsp;&nbsp;&nbsp;概述&nbsp;&nbsp;&nbsp;&nbsp;评论系统由Disqus改成了多说&nbsp;&nbsp;&nbsp;&nbsp;添加google analytics&nbsp;&nbsp;&nbsp;&nbsp;如何搞定数学公式显示问题&nbsp;&nbsp;&nbsp;&nbsp;如何让每个页面自动生成二维码&nbsp;&nbsp;&nbsp;&nbsp;如何添加“返回顶部”按钮“&nbsp;&nbsp;&nbsp;&nbsp;其他&nbsp;&nbsp;&nbsp;&nbsp;最后 概述 前几天把博客改版了，在Jekyll wiki上爬主题来回爬了好几遍，先是爬了Linghua Zhang,而后又直接爬了yihui和Carl Chen，不过做我这个版面主题的原始作者是yihui，随便一提这位是牛人，统计之都和COS论坛，以及中国R会议（第一届开始）都是他弄的，年纪轻轻且最近已经出了两本和R相关的书在亚马逊上卖了，恐怕国内（+很多国外）用过R的基本都知道！感兴趣的可以去他的主页扒扒。。。扯远了，说回我自己，虽然我也想改得更加不同一些，但一方面暂时还没啥时间，其次，此前未碰过任何与网页制作相关之事，HTML勉强能看，css和js就停留在听说过这两词的地步；然，最重要的是，我也看上了这个版面！所以门面修改一事先缓一缓吧，先在这里谢过各位作者大人！但是话也说回来，以上只要有任意一位作者介意，我也只能作罢，重新扒过。 OK，既然现在外在的部分还无力去动，那接下来，我说一下自己所做的一些内在改变。 1. 评论系统由Disqus改成了多说 我倒不是排斥Disqus，一开始我用的就是它，Disqus，国际化，版面简洁，管理容易，主页也生动，cool，我很喜欢！一个账号说遍天下，当然这一点上多说也一样。换掉它根本的原因还是在于伟大的‘墙’，Disqus只具有分享到Facebook和Twitter的功能，而这两货正常途径咱是上不了的。也罢，在国内的话多说用起来确实要更友好些，所以这次改版就重新选择了和国内社交网络联系在一起的多说。 这里我说一下自己是如何添加的。 （1）如果还没有绑定多说，那第一步需要做的就是到多说上绑定。多说是不需要做任何注册的，只要有QQ，微博，百度等账号就行，这一点相当方便，不然我又得增加一个账号，我都已经记不得自己在网络上到底注册过多少个账号了，恐怕各型各色几百个都有了！ （2）尔后，登陆多说主页点击“我要安装”，按提示走就行了。在站点地址这一栏填入自己的域名，再填上其他信息，按”创建” （3）创建了之后，在侧边框找到选择工具，选择通用代码，会看到多说提供了一段相关的代码，我们要做的事情很简单，直接把它复制并贴到你自己网页的代码中，然后，就没有然后了。。。诶诶，慢着呀！那应该贴进哪一个的网页里头啊？我这简单说明一下，比如，_layout（生成网站页面的相关网页绝大数都放这，jekyll会自己来加载）目录下我们一般都会创建defualt.html, page.html, post.html等这么几个文件。我假设post.html就是你用于博客的，而你也只想博客的页面才加多说评论框，那么代码就应该贴进post.html。虽然你可以放在与之间的任何位置，但考虑到页面主要内容的加载速度，最好还是把它靠后放，评论模块要那么急着加载干嘛，最好就只是在之前，下面我给了个示意。 添加多说的代码结构。 12345678910&lt;html&gt;&lt;head&gt;&lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body&gt;...&lt;!—多说代码--&gt;&#123; % include duoshuo.md % &#125; &lt;!-- 实际使用中 “&#123;” 和 “%”，以及“%”和”&#125;“ 之间的空格需要去掉--&gt;&lt;/body&gt;&lt;/html&gt; 如果除了post.html，你还打算将它添加到其他的页面中，也没问题，操作和上面类似。但整块整块的代码复制来复制去的，总觉得很麻烦，一旦想修改还得一个个去改，太吃力了！！我自己更喜欢的一种方式就是直接把多说的代码写到一个单独的文件中，比如就叫duoshuo.md，并放在_include目录下（这个目录按照jekyll的建议，就是让我们用来存放模块插件的），然后在需要多说的页面中直接调用{ % include duoshuo.md % }就行(本文为了显示的原因在{和%以及%和}之间加了一个空格，正式使用的时候注意去掉，下同)，这就犹如一个函数一般，修改也只需要改duoshuo.md这个文件，相当方便。 在设置&lt;div class=&quot;ds-thread&quot; data-thread-key=&quot;请将此处替换成文章在你的站点中的ID&quot; data-title=&quot;请替换成文章的标题&quot; data-url=&quot;请替换成文章的网址&quot;&gt;&lt;/div&gt;，这几个值的时候，我参考了这篇博文，它讲的很清楚，不过里面写成data-thread-key=”&lt;%= page.path %&gt;”，data-title=”&lt;%= page.title %&gt;”和data-url=”&lt;%= page.permalink %&gt;”的形式在我这并不能成功，不知道是否与我用markdown格式编辑有关。所以后来我改成&lt;div class=&quot;ds-thread&quot; data-thread-key=&quot;{ { page.path } }&quot; data-title=&quot;{ { page.title } }&quot; data-url=&quot;{ { page.permalink } }&quot;&gt;&lt;/div&gt;就成功了（本文为了显示的原因在{和{以及}和}之间加了一个空格，正式使用的时候注意去掉，下同）。 2. 添加google analytics 对于自己的网站，一个关心的事情就是网站的浏览量如何？是通过什么途径以什么方式流入流出的？别的先不多说，光是能时刻知道自己的网站在哪里、被用什么操作系统、多少人浏览、网页之间的流入流出是怎么样的等状态信息的本身就是一个很cool的事情，这跟玩游戏一样，相信没多少人会愿意把自己角色的血条和蓝条隐藏掉，然后在那瞎玩。所以我就开始琢磨着应该怎么搞定这样一个事情。后来知道了可以用google-analytics帮助记录和分析，同样需要登陆google账号，填入网站域名，获得对应该域名的google analytics网络跟踪代码，然后又是复制粘贴。改过一次评论系统之后，我就发现，这东西都是大同小异的，就那么几下板斧。当然了我还是把它单独写成一个模块放在_include下供调用。代码如下： 123456789101112&lt;!-- Google Analysis For website tracking --&gt;&lt;script&gt; (function(i,s,o,g,r,a,m)&#123;i['GoogleAnalyticsObject']=r;i[r]=i[r]||function()&#123; (i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) &#125;)(window,document,'script','//www.google-analytics.com/analytics.js','ga'); ga('create', 'UA-52659904-2', 'auto'); ga('require', 'displayfeatures'); ga('send', 'pageview');&lt;/script&gt; &lt;!-- Google Analysis end --&gt; 这一次我希望全站跟踪，default.html是我所有页面都会添加的网页，所以这个代码就放在default.html中了。关于GA代码应该放在文件中的哪个位置比较适合，我还是做了一下考虑的，参考了这篇文章,按照异步跟踪的方式添加： 1234567891011&lt;html&gt;&lt;head&gt;&lt;title&gt;&lt;/title&gt;...&lt;!—GA异步追踪代码--&gt;&#123; % include googleAnalysis.md % &#125; &lt;!-- 实际使用中 “&#123;” 和 “%”，以及“%”和”&#125;“ 之间的空格需要去掉--&gt;...&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 注意: 在国内使用google analytics 并不是一个明智的做法，它已被封，这会大大降低你的网页访问速度，体验极差，建议还是使用百度分析. 3. 如何搞定数学公式显示问题 makrdown是个好东西，jekyll+markdown做网站很容易！但麻烦的是，markdown不能支持LaTeX，我写个数学公式在那上面，它解析不了，只是原封不动的显示在那。。这一点真让人捉急！咋办？又是一番的google，后来还是找到了个好办法——MathJax，它是一个数学公式显示引擎，能够把LaTeX编辑的公式在网页上显示出来，不过它是在线解析的，公式如果比较多的话，显示速度会稍慢。要用MathJax，需要先把_config.yml中的makrdown解析器换成kramdown，不然用不了。我同样也是写成模块mathJax.md，然后由post.html调用(估计我也不会在除了博客之外的地方用到它)，为了加载速度，也放的靠后一些，刚好在多说模块上面，下面是我用的mathJax.md模块的代码，当然了，若有需要你也可以使用： 123456&lt;script type="text/x-mathjax-config"&gt; MathJax.Hub.Config(&#123; tex2jax: &#123;inlineMath: [['$','$'], ['\\(','\\)']]&#125; &#125;);&lt;/script&gt;&lt;script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"&gt;&lt;/script&gt; 成功添加了之后只需在公式前后用\$\$围起来，就可方便的编写公式了。如： 123$$\rho_i=\sum_j\chi(d_&#123;ij&#125;-d_c)$$ 显示的结果就是：$$\rho_i=\sumj\chi(d{ij}-d_c)$$ 4. 如何让每个页面自动生成二维码 二维码的作用其实也不必做多解释，最重要的就是方便。我之前并不知道可以用Google API为网站自动生成二维码一事，直到我成功了之后才恍然一悟，真是隔行如隔山。一开始我都是暴力解决：找个能生成二维码的网站，把自己每一篇博客的网址贴进去，点击生成二维码，然后再把这个生成的二维码图片下载下来，接着将它上传至图床，最后再将图片的链接添加到每篇博客的最后！这实在是。。。这个过程就算只是这样说起来都觉得十分费事，可以想象操作起来该有多费劲，而且一旦网站发生调整，就得重来一遍，实在低效！所以我一直寻思着该如何做才能让网页自己去产生二维码，一定要将它自动化！后来注意到了这个QR生成器，并且受到它share按钮中内容的启发，我当时就在想为什么它这样的一段代码（如下）在插入到网页中之后就能出来一个二维码呢？（当然这个二维码是指向它自己网站的） 12&lt;!--QR生成器的share代码--&gt;&lt;a href="https://www.the-qrcode-generator.com/"&gt;&lt;img src="http://chart.apis.google.com/chart?chs=200x200&amp;amp;cht=qr&amp;amp;chld=|1&amp;amp;chl=" alt="QR Code" /&gt;&lt;/a&gt; 在仔细观察后发现代码中url地址中后面的chs=200x200&amp;amp;cht=qr&amp;amp;chld=|1&amp;amp;chl=看起来很像某些参数，貌似是可以自己设置的，若真如此的话，又该怎么做呢？这次得感谢DIVCSS5这篇博文中的那一段代码结构，它完全让我明白了自己该如何改装上面的QR生成器代码让它能为我所用，给每一个网页都自动根据自己的网址产生正确的二维码，改了的代码如下： 123456&lt;!-- 网页自动生成二维码的代码 --&gt;&lt;script type="text/javascript"&gt; thisURL = document.URL; strwrite = "&lt;p align='center'&gt;&lt;img src='http://chart.apis.google.com/chart?chs=120x120&amp;amp;cht=qr&amp;amp;chld=|1&amp;amp;chl=" + thisURL + "' width='120' height='120' alt='QR Code'/&gt;（传送门）&lt;/p&gt;"; document.write( strwrite ); &lt;/script&gt; 这一段代码本身是独立的，不依赖于任何特定的网站拥有者，而且光看代码我们也能想象得到，这里头调用的就是Google API，所以如果有需要，也欢迎大家直接复制粘贴到自己的页面代码中。当然了，我还是把它独立写在了一个文件——qrCodeGenerate.md中并放在了_include目录下，然后再调用，就跟上面添加多说和google跟踪代码一个思路，不过这个倒没什么位置限制，就看各自喜欢放哪就放哪，只要是在与之间就行，但我还是建议放在博文内容加载模块之后，这样可以确保博文内容先加载。 注意: 后来在国内发现上面这个基于google的二维码，不但没用还大大影响了页面的加载速度！原因大家也都知道，具体的修改请参看这篇修改二维码生成工具博文。 5. 如何添加“返回顶部”按钮 “返回顶部”按钮（本博客右下角）的添加完全参照这个教程，作者写的相当详细！按照文章的信息我把’二’中的代码写入backtop.js，’三’中的信息写入backtop.css。然后将它们放在defualt.html中的与之间调用，示意如下： 12345678910&lt;html&gt;&lt;head&gt;...&lt;link rel="stylesheet" href="/media/css/backtop.css"&gt; &lt;!-- Back Top --&gt;&lt;script type="text/javascript" src="/media/js/backtop.js"&gt;&lt;/script&gt; &lt;!-- Back Top --&gt;...&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt; 6. 其他 （1）关于主页上的“FORUM”，本来有打算做成类似于论坛形式可以让大家就某个话题来进行讨论，但是发现要实现这个功能难度很大工作量不小！限于我自己能力有限，这个也得慢慢来，暂时将就变成了贴“鸡汤文”的板子了，o(╯□╰)o！ （2）google网站站长工具，上传了一份google提供的验证网页（它仅是验证之用），成功之后虽然可以将它删掉，但google官网的意思是建议留下。 （3）图床的选择。图床对于独立博客来说是一个重要又头疼的事情，目前我直接用点点博客的图片功能，使用外链。它的好处是免费，稳定，访问速度也快，要是真有一天点点不允许这样用的话，那我也不怕，实在不行就用七牛(注: 已改用七牛 )，或者想办法用Github作为图床。 7. 最后 OK！唠叨了不少总算把这篇文章写完了。接下来与博客搭建相关的内容先暂放一边了，我要回归主业的更新了。]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>网站插件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客搬家]]></title>
    <url>%2F2014%2F07%2F09%2F2014-07-09-Blog-Migration.html</url>
    <content type="text"><![CDATA[以前在cnblog上还遗留了几篇文章。当时还准备就在那上面安家的呢。不过，发了几篇博文之后，大伙多评论说，“哥们是不是发错地方了”之类的云云，还有一次惊讶了我一下的是，博文直接被管理员丢出了cnblog的首页，o(╯□╰)o。也罢了，毕竟博客园面向的是IT开发人员，发BT(Biology Technical)相关的东西也确实不好意思，既然门不当户不对，那就另觅出路呗。后来搞了搞，要整个独立博客还真有点费事，主机要花钱，IP要花钱。。。那会我还没听说过Jekyll，再加上自己平时比较忙也没什么时间，所以就一直搁着了，直到最近用了github，才在无意间发现了Jekyll+github的原来可以方便的搭建起独立博客来！这个中的具体情况我在上次的博文中提到了。 虽然碰到了不少困难，但也还是七手八手把个人网站搭起来了。现在算是有了块自己的地，接着自然也就是要配置一些“行当”了，一些旧的“家当”也是不能扔的。所以就趁热打铁，把以前发在cnblog上的几篇文章也一起给搬过来了。这搬的过程倒也不难（但比较遗憾的事评论貌似不好搬过来），这要感谢这位RichardUSTC写的一个格式转换程序，虽然如ta自己所言，格式转换有些瑕疵，但我觉得已经很好了！也就只是瑕疵，自己再稍微调整一下也就OK了！关键的目的还是在于能节省下不少时间，就这样顺利搬家了！多谢多谢！不过话也说回来，转换过程中有些图表和段落的问题还没解决好，在三代基因组测序技术原理简介这篇博文中尤为明显！算了等后面有时间再一一来解决吧！ 补充一下，以前我在cnblog上的网名是T&amp;S，后来想想还是换成YellowTree好了。]]></content>
      <categories>
        <category>Blog</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Science上发表的一个超赞聚类算法]]></title>
    <url>%2F2014%2F07%2F06%2F2014-07-06-Science-Public-Cluster.html</url>
    <content type="text"><![CDATA[作者(Alex Rodriguez, Alessandro Laio)提出了一种很简洁优美的聚类算法, 可以识别各种形状的类簇, 并且其超参数很容易确定. ##算法思想 该算法的假设是, 类簇的中心由一些局部密度比较低的点围绕, 并且这些点距离其他高局部密度的点的距离都比较大. 首先定义两个值: 局部密度$$\rho_i$$以及到高局部密度点的距离$$\delta_i$$: $$\rho_i=\sumj\chi(d{ij}-d_c)$$ 其中 $$\chi(x)= \begin{cases}1 &amp; if x&lt;0\ 0&amp; otherwise \end{cases}$$ $$d_c$$是一个截断距离，是一个超参数。所以$$\rho_i$$相当于距离点$$i$$的距离小于$$d_c$$的点的个数。由于该算法只对$$\rho_i$$的相对值敏感，所以对$$d_c$$的选择比较鲁棒，一种推荐的做法是选择$$d_c$$使得平均每个点的邻居数为所有点的1%-2%。 $$\deltai=\min{j:\rho_j&gt;\rhoi}(d{ij})$$ 对于密度最大的点，设置$$\delta_i=maxj(d{ij})$$。注意只有那些密度是局部或者全局最大的点才会有远大于正常的相邻点间距。 ##聚类过程 那些有着比较大的局部密度$$\rho_i$$和很大的$$\delta_i$$的点被认为是类簇的中心. 局部密度较小但是$$\delta_i$$较大的点是异常点.在确定了类簇中心之后, 所有其他点属于距离其最近的类簇中心所代表的类簇. 图例如下: 左图是所有点在二维空间的分布, 右图是以$$\rho$$为横坐标, 以$$\delta$$为纵坐标, 这种图称作决策图(decision tree). 可以看到, 1和10两个点的$$\rho_i$$和$$\delta_i$$都比较大, 作为类簇的中心点。 26, 27, 28三个点的$$\delta_i$$也比较大, 但是$$\rho_i$$较小, 所以是异常点。 ##聚类分析 在聚类分析中, 通常需要确定每个点划分给某个类簇的可靠性. 在该算法中, 可以首先为每个类簇定义一个边界区域(border region), 亦即划分给该类簇但是距离其他类簇的点的距离小于$$d_c$$的点。 然后为每个类簇找到其边界区域的局部密度最大的点, 令其局部密度为$$\rho_h$$该类簇中所有局部密度大于$$\rho_h$$的点被认为是类簇核心的一部分(亦即将该点划分给该类簇的可靠性很大), 其余的点被认为是该类簇的光晕(halo), 亦即可以认为是噪音. 图例如下: A图为生成数据的概率分布, B, C二图为分别从该分布中生成了4000, 1000个点. D, E分别是B, C两组数据的决策图(decision tree), 可以看到两组数据都只有五个点有比较大的$$\rho_i$$和很大的$$\delta_i$$。 这些点作为类簇的中心, 在确定了类簇的中心之后, 每个点被划分到各个类簇(彩色点), 或者是划分到类簇光晕(黑色点)。F图展示的是随着抽样点数量的增多, 聚类的错误率在逐渐下降, 说明该算法是鲁棒的。 最后展示一下该算法在各种数据分布上的聚类效果，非常漂亮。 参考文献: [1]. Clustering by fast search and find of density peak. Alex Rodriguez, Alessandro Laio 本文转载自Kemaswill’s Blog]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github+Jekyll建站]]></title>
    <url>%2F2014%2F07%2F05%2F2014-07-05-Build-MyOwn-blog-with-jekyll-and-githubpage.html</url>
    <content type="text"><![CDATA[这该如何说起。。。 关于搭建自己的网站，其实早有此心，只是。。。。。。靠，本想长篇大论一番，讲讲自己是如何搭建起这个网站的，讲讲自己是如何在对Jekyll，markdown，GithubPage，DNS解析等一无所知的情形下，怎样现学现卖，等等之类的，最后想想还是算了，网上的这些教程太多了，况且我也是参照着别人的经验做出来的，还是不装了。呵呵。 这里就只列一下之前看到的教程，权当作记录吧。 我自己其实是纯属意外才会真的动起来去用Jekyll+Github来建博客的，Jekyll和Github以前是有所听闻的，但一直不知道它们竟还能有这层关系！嗨，其实说来也是因为不知道，Github也还用了不到一个月的时间，要不是因为有一次很偶然的看到了代码仓库的setting之下有个Automatic page generator的按钮，一时好奇心起，想知道这玩意是个啥，于是一查，哟！原来如此！就这样我知道了GitHub Pages的神奇作用，发现了它完全符合我自己想要建个人网站的需求！除了能映射自己的域名之外，更重要的还是免费+无限空间，对于是静态还是动态网站于我来讲并不是很关心，我关心的是，我要能完全的把控它！接着也就自然而然的认识了Jekyll,markdown我在一开始使用Github的时候就知道了，所以这个没啥槛。 那么说干就干了，方向有了，怎么开始好呢！习惯性Google了一下，发现了这个使用Github Pages建独立博客,作者写的很好，也是这篇博文领我入门的。包括了如何设置DNSpod解析域名，添加CNAME，怎么添加A记录等等这些。再次千恩万谢！不过我重设DNSpod进行域名解析的时候，基本是瞬间生效，不需要如作者所言要等一天那么久，但不知这情况是否普遍。接下来又陆陆续续看了很多有关Jekyll+github的文章，基本上大同小异，也算不断填补自己的空白吧。 要特别提到的是今天看到的一篇迟来的博客一步步在GitHub上创建博客主页,讲的也是相当的清楚，而且作者写的一整个系列的一步步教程，对于新手而言非常具有参考价值，还对域名以及与其相关的一些概念，如A记录，CNAME，TTL之类的做了详细的解析，真心推荐新手读之，此外也看到作者的博客做的很漂亮。 其他的一些则如一步步构建Jekyll网站,Jekyll建站之旅等等之类的，也都不错。就不一一列出了！ OK，暂时先这样了！]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>github</tag>
        <tag>jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基因组变异检测概述]]></title>
    <url>%2F2013%2F09%2F28%2F2013-09-28-An-Introduction-of-genome-variant-detect.html</url>
    <content type="text"><![CDATA[首先，在开始之前我觉得有必要稍微科普缓冲一下，以便不使得不熟悉生物信息或基因组的客官们疑惑。1.基因组：每个人都有一个基因组，这里的&ldquo;基因组&rdquo;并不只是&ldquo;基因&rdquo;的集合，基因是控制性状的遗传单元（什么是性状呢？性状也可以狭义的理解为个体的各种外在和内在特征，比如头发和眼睛颜色，高矮胖瘦，抵抗力强等），但是基因组所指的其实是我们的所有遗传信息，而不单单只是一些外在和内在特征，也包含很多目前而言不明其功能性（或者被认为无功能）的DNA序列。 其实说白了就是整一个的DNA序列！因而，基因也只是基因组的一个子集。此外，需要特别指出的是，我们虽都为&ldquo;人&rdquo;，但人与人之间的基因组是不一样的（即是多态的），彼此之间都存在着一些差异，即使是和父母或是兄弟姐妹之间去比较。这些差异也是造成我们彼此之间为何如此这般不同的一个重要原因。而这些差异也是基因组多态性的来源。2.Reads：这里的reads是一个在基因组测序中使用的名词（对测序原理感兴趣的客官请猛戳：三代基因组测序技术原理简介），指的就是一段特定长度的DNA片段，这个长度取决于测序仪的读长。3. 变异是一个相对的概念，只有在彼此的比较中才有存在的意义。目前关于人类基因组变异的讨论，都是以&ldquo;人类基因组计划&rdquo;中所组装出来的人类基因组作为参照物。以下谈到的涉及比对过程所用的基因组指的就是这个人类参考基因组。4. 以下常出现&ldquo;序列&rdquo;，指的都是DNA序列片段。OK！简单的科普就此完毕，剩余的在后面碰到了再说明，以下进入正文。&nbsp;&nbsp; &nbsp; &nbsp; 摘要：人类基因组上的结构性变异研究对于基因组进化，群体多态性分析以及疾病易感性等方面的研究有着重要的意义。第二代短reads高通量测序技术的发展在带来了测序成本降低的同时，这种短读长的测序方式也给人类的变异检测带来了很大的挑战。这里我主要对当前常用的变异检测方法、软件以及他们各自的有确定做一个简要的小结。&nbsp; &nbsp; &nbsp;人类基因组上的变异主要分为三大类：1. 单核苷酸变异，（通常称为单核苷酸多态性，通俗的说法就是单个DNA碱基的不同，简称SNP）；2. 小的Indel（Insertion 和 Deletion的简），指的是在基因组的某个位置上所发生的小片段序列的插入或者删除，其长度通常在50bp以下（这个长度范围的变异可以利用Smith-Waterman 的比对算法来获得1,2）；3. 大的结构性变异，这种类型比较多，包括长度在50bp以上的长片段序列的插入或者删除、染色体倒位，染色体内部或染色体之间的序列易位，拷贝数变异，以及一些形式更为复杂的变异。为了和SNP变异作区分，第2和第3类变异通常也被称为基因组结构性变异（Structural variation，简称SV）。这里值得一提的是，研究人员对基因组的结构性变异发生兴趣，主要是由于这几年的研究发现：（1）虽然还未被广泛公认，但研究人员发现SV对基因组的影响比起SNP来说还要大3；（2）基因组上的SV比起SNP而言，似乎更能用于解释人类群体多样性的特征；（3）稀有且相同的一些结构性变异往往和疾病（包括一些癌症）的发生相关联甚至还是其致病的诱因4&ndash;6。不过应该注意的地方是，大多数的结构性变异并不真正与疾病的发生相关联，但是却确实与周围环境的响应或者其他的一些表型多态性相联系。&nbsp; &nbsp; &nbsp; 近年来，随着芯片技术（这里的芯片技术和IT领域所说的芯不是同一个概念，这里指的是一种用于抓获基因组特定序列片段的技术）和第二代高通量测序技术的发展，人类基因组上的结构性变异图谱才被真正全面而又集中地进行了研究。生物信息研究人员已针对这两种不同的技术开发了许多相对应的软件用于检测基因组的结构性变异。相比较而言，虽然成本较高，但是基于测序的方法要明显优于芯片的检测，其中最重要的一个方面是，高通量测序技术能够在单碱基精度之下对全基因组范围内所有类型的变异进行检测，而芯片技术实际上只对大片段的序列删除比较敏感。&nbsp; &nbsp; &nbsp; 接下来我将会对目前基于第二代测序技术的变异检测方法进行介绍。&nbsp; &nbsp; &nbsp; 在各大生物信息学期刊（包括Nature，Science，Cell等这些顶级期刊）上都有许多关于介绍变异检测方面的文章。这里我大致说一下四篇自己觉得在这方面比较重要的文章：综述&ldquo;Genome structural variation discovery and genotyping7&rdquo;和综述&ldquo;computational methods for discovering structural variation with next-generation sequencing&rdquo;，这两篇文章所探讨的主要是，如何根据实验上和计算上的途径来检测和发现基因组上的各种变异，特别是对检测SVs而已。另外两篇文章则是基于千人基因组计划的，他们描述的是如何利用trio家系全基因组测序的数据和群体低覆盖度的数据来做变异检测的生物信息学方法8,9。然而需要指出的是，对于千人基因组计划，他们基本上只关注于一些大片段的序列删除和一些特定的序列插入方面的检测，而忽视了很多基因组上其他形式的变异。关于这方面的局限性，一方面可能是由于生物信息检测方法上的不完善，另一方面可能也和千人基因组本身的数据特点有关，使得他们难以准确地获得更多的信息。&nbsp; &nbsp; &nbsp; 目前主要有4种检测基因组上结构性变异的策略，分别为：（1）Read pair（也称为Pair-end Mapping，简称PEM）；（2）Split read（简称SR）；（3）Read Depth（简称RD）和（4）基于de novo组装的方法（图1）。同时生物信息研究人员也已开发了众多根据以上4中策略中一种或者多种的软件用于结构性变异的检测。接下来我将对这四种策略以及他们各自的特点逐一进行介绍。图1&nbsp; &nbsp; &nbsp; 1.&nbsp;基于Pair-end Mapping（PEM）&nbsp; &nbsp; &nbsp;&nbsp;图2是PEM方法的一个主要分析框架，理论上来讲，PEM方法能够检测到的变异类型包括：序列删除（deletion），序列插入（insertion），序列转置（inversion），染色体内部和染色体外部的易位（intra- and inter-chromosome translocation），序列串联倍增（tandem duplications）和序列在基因组上的散在倍增（interspersed duplications）。这里有两个地方需要指出，第一，对于序列删除的检测，其所能检测到的片段长度受插入片段长度的标准差（SD）所影响（这里的插入片段长度指的是测序之前在构建DNA测序文库阶段，所选取的经由超声波打断的DNA片段长度，这些片段也称之为测序片段，这是实验过程中的操作，并不是指基因组的变异），并且越大的序列删除约容易被检测到，并且准确性也越高；第二，其所能检测的序列插入，长度只能在插入片段长度的范围内，并且最大长度也受限于测序的插入片段长度的标准差。目前，Breakdancer是应用PEM方法的软件，也是在使用变异检测方面用得最广泛的软件之一。其他类似的软件还包括：VariationHunter10, Spanner, PEMer11等等。但是，事实上整个过程并不像流程图中看起来的那么简单，而且绝大多数的软件都在检测复杂的序列结构方面（如序列易位和序列倍增）存在很大的困难。图2&nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; 2. Split Read（分裂read，简称SR）&nbsp; &nbsp; &nbsp; 对于这个方法，首先要求比对软件具备soft-clip reads的能力，如BWA 比对软件。我们知道目前illumina测序平台Pair-End测序的方法是对测序片段的两端来进行的，所以每次获得的都是来自同一个测序序列片段两端的一对read。当BWA成功地将这一对reads中的一条比对到参考序列上，而另一条却无法正常比上的时候，BWA会对这条read没能正常比上的read尝试在比对上的那条read附近使用更为宽松的Smith-Waterman局部比对策略搜索可能的比对位置。如果这条read只有一部分能够比上，那么BWA会对其进行soft-clip，而这里也往往是包含结构性变异的断点之处。Pindel12，这是目前唯一一个使用SR方法进行变异检测的软件。它在千人基因组计划和生物信息分析人员中被广泛使用。图1中也清楚地展示了Split reads的信号如何被用来进行结构性变异的检测。首先，在获得了单端唯一比对到基因组上的PE read之后，Pindel会将不能比上的那条read切开成2或者3小段，然后再分别重新按照用户所设置的最大序列删除长度去比对，并获得最终的比对位置和比对方向，而断点位置的确定则是根据soft-clipped的结果来获得。&nbsp; &nbsp; &nbsp; Pindel 理论上能够检测所有长度范围内的deletion，和小片段的insertion（长度在50bp以下），inversion，tandem duplication和一些large insertion。不过目前，作者并未公开发布关于检测lager insertion的原理。Split-reads的一个优势就在于，它们精确到单碱基。但是也和大多数的PEM方法一样，Pindel同样无法解决复杂结构性变异的情形。&nbsp; &nbsp; &nbsp; 3. Read Depth （read 覆盖深度，简称RD）&nbsp; &nbsp; &nbsp; 目前存在两种利用Read depth的信息检测大拷贝数变异（Copy number variation，包括丢失序列和序列重复倍增，简称CNV）的策略。一种是，通过检测样本在一个参考基因组上read的深度分布情况来检测CNV，适用于单样本；另一种则是通过和识别出比较两个样本中所存在的丢失和重复倍增区，以此来获得相对的CNV，适用于case-control模型的样本。这有点像CGH芯片。CNVnator使用的是第一种策略，同时也广泛地被用于检测大的CNV。当然还有一些比较冷门的软件，但是由于他们没有发表相应的文章，这里就不再列举了。CNV-seq使用的是第二个策略。基于其原理，RD的方法能够很好地用于检测一些大的deletion或者duplication事件，但是对于小的变异事件就无能为力了。&nbsp; &nbsp; &nbsp; 4. 基于De novo assembly&nbsp;&nbsp; &nbsp; &nbsp; 理论上来讲，de novo assembly 的方法应该要算是基因组变异检测上最有效的方法了。就目前来说，它能够提供（特别是）对于long insertion和复杂结构性变异的最好检测方法。现在虽然研究人员开发了很多基于第二代测序技术数据来进行组装的软件，但是组装却仍然是一件棘手的事情，特别是脊椎动物的组装则更是如此。其中最主要的原因在于，脊椎动物基因组上所存在的重复性序列和序列的杂合会严重影响组装的质量，除去资金成本，这也在很大程度上阻碍了利用组装的方法在基因组变异检测方面的应用。&nbsp; &nbsp; &nbsp; 小结：&nbsp; &nbsp; &nbsp; 通过对上面四种不同的变异检测策略的比较可以发现，小长度范围内的变异以及较长的deletion，目前都能够较好地检测出来，但对于大多数的long insertion和更复杂的结构性变异情况，当前的检测软件基本都没法还解决。Assembly应是当前全面获得基因组上各种变异的最好方法，但是目前的局限却也发生在Assembly本身，若是基因组没能装得好，后面的变异检测就更是无从说起。从目前的情况看，de novo assembly的方法并不能很快进入实际的应用。因此，暂且不提assembly，其余的三种策略都各有各的优势，从目前的结果看，并没有哪一款软件能够一次性地将基因组上的各种不同情况变异类型都获得。因此就目前短reads高通量测序技术来说，最合适的方案应是结合多个不同的策略，将结果合并在一起，这样可以最大限度地将FP降低。HugeSeq pipeline13在这方面做了一个比较好的总结，这个软件整合了BreakDancer, CNVnator, Pindel，BreakSeq以及GATK的结果。能够给出一个相对比较准确的变异检测结果。最后这句怎么看起来像是在帮别人卖广告o(╯□╰)o。1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DePristo, M. a et al. A framework for variation discovery and genotyping using next-generation DNA sequencing data. Nature genetics43, 491&ndash;8 (2011).2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Albers, C. a et al. Dindel: accurate indel calls from short-read data. Genome research21, 961&ndash;73 (2011).3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Conrad, D. F. et al. Europe PMC Funders Group Origins and functional impact of copy number variation in the human genome. 464, 704&ndash;712 (2012).4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Campbell, P. J. et al. Identification of somatically acquired rearrangements in cancer using genome-wide massively parallel paired-end sequencing. Nature genetics40, 722&ndash;9 (2008).5.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Berger, M. F. et al. The genomic complexity of primary human prostate cancer. Nature470, 214&ndash;20 (2011).6.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Stephens, P. J. et al. Massive genomic rearrangement acquired in a single catastrophic event during cancer development. Cell144, 27&ndash;40 (2011).7.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Alkan, C., Coe, B. P. &amp; Eichler, E. E. Genome structural variation discovery and genotyping. Nature reviews. Genetics12, 363&ndash;76 (2011).8.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mills, R. E. et al. Mapping copy number variation by population-scale genome sequencing. Nature470, 59&ndash;65 (2011).9.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Africa, W. A map of human genome variation from population-scale sequencing. Nature467, 1061&ndash;73 (2010).10.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hormozdiari, F., Alkan, C., Eichler, E. E. &amp; Sahinalp, S. C. Combinatorial algorithms for structural variation detection in high-throughput sequenced genomes. Genome research19, 1270&ndash;8 (2009).11.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Korbel, J. O. et al. PEMer: a computational framework with simulation-based error models for inferring genomic structural variants from massive paired-end sequencing data. Genome biology10, R23 (2009).12.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Ye, K., Schulz, M. H., Long, Q., Apweiler, R. &amp; Ning, Z. Pindel: a pattern growth approach to detect break points of large deletions and medium sized insertions from paired-end short reads. Bioinformatics (Oxford, England)25, 2865&ndash;71 (2009).13.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Lam, H. Y. K. et al. Detecting and annotating genetic variations using the HugeSeq pipeline. Nature biotechnology30, 226&ndash;9 (2012).&nbsp;]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>变异检测</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三代基因组测序技术原理简介]]></title>
    <url>%2F2013%2F08%2F02%2F2013-08-02-An-Introduction-of-NGS-Sequence.html</url>
    <content type="text"><![CDATA[摘要：从1977年第一代DNA测序技术（Sanger法）1，发展至今三十多年时间，测序技术已取得了相当大的发展，从第一代到第三代乃至第四代，测序读长从长到短，再从短到长。虽然就当前形势看来第二代短读长测序技术在全球测序市场上仍然占有着绝对的优势位置，但第三和第四代测序技术也已在这一两年的时间中快速发展着。测序技术的每一次变革，也都对基因组研究，疾病医疗研究，药物研发，育种等领域产生巨大的推动作用。在这里我主要对当前的测序技术以及它们的测序原理做一个简单的小结。 图1：测序技术的发展历程 生命体遗传信息的快速获得对于生命科学的研究有着十分重要的意义。以上图1（右键打开图片可查看大图，下同）所描述的是自沃森和克里克在1953年建立DNA双螺旋结构以来，整个测序技术的发展历程。第一代测序技术第一代DNA测序技术用的是1975年由桑格（Sanger）和考尔森（Coulson）开创的链终止法或者是1976-1977年由马克西姆（Maxam）和吉尔伯特（Gilbert）发明的化学法（链降解）. 并在1977年，桑格测定了第一个基因组序列，是噬菌体X174的，全长5375个碱基1。自此，人类获得了窥探生命遗传差异本质的能力，并以此为开端步入基因组学时代。研究人员在Sanger法的多年实践之中不断对其进行改进。在2001年，完成的首个人类基因组图谱就是以改进了的Sanger法为其测序基础，Sanger法核心原理是：由于ddNTP的2&rsquo;和3&rsquo;都不含羟基，其在DNA的合成过程中不能形成磷酸二酯键，因此可以用来中断DNA合成反应，在4个DNA合成反应体系中分别加入一定比例带有放射性同位素标记的ddNTP（分为：ddATP,ddCTP,ddGTP和ddTTP），通过凝胶电泳和放射自显影后可以根据电泳带的位置确定待测分子的DNA序列（图2）。这个网址为sanger测序法制作了一个小短片，形象而生动。值得注意的是，就在测序技术起步发展的这一时期中，除了Sanger法之外还出现了一些其他的测序技术，如焦磷酸测序法、链接酶法等。其中，焦磷酸测序法是后来Roche公司454技术所使用的测序方法2&ndash;4，而连接酶测序法是后来ABI公司SOLID技术使用的测序方法2,4，但他们的共同核心手段都是利用了Sanger1中的可中断DNA合成反应的dNTP。 图2：Sanger法测序原理 第二代测序技术总的说来，第一代测序技术的主要特点是测序读长可达1000bp，准确性高达99.999%，但其测序成本高，通量低等方面的缺点，严重影响了其真正大规模的应用。因而第一代测序技术并不是最理想的测序方法。经过不断的技术开发和改进，以Roche公司的454技术、illumina公司的Solexa，Hiseq技术和ABI公司的Solid技术为标记的第二代测序技术诞生了。第二代测序技术大大降低了测序成本的同时，还大幅提高了测序速度，并且保持了高准确性，以前完成一个人类基因组的测序需要3年时间，而使用二代测序技术则仅仅需要1周，但在序列读长方面比起第一代测序技术则要短很多。表1和图3对第一代和第二代测序技术各自的特点以及测序成本作了一个简单的比较5，以下我将对这三种主要的第二代测序技术的主要原理和特点作一个简单的介绍。&nbsp; 图3. 测序成本的变化 Illumine Illumina公司的Solexa和Hiseq应该说是目前全球使用量最大的第二代测序机器，这两个系列的技术核心原理是相同的2,4。这两个系列的机器采用的都是边合成边测序的方法，它的测序过程主要分为以下4步，如图4.&nbsp;&nbsp;&nbsp;&nbsp; （1）DNA待测文库构建利用超声波把待测的DNA样本打断成小片段，目前除了组装之外和一些其他的特殊要求之外，主要是打断成200-500bp长的序列片段，并在这些小片段的两端添加上不同的接头，构建出单链DNA文库。&nbsp;&nbsp;&nbsp;&nbsp; （2）FlowcellFlowcell是用于吸附流动DNA片段的槽道，当文库建好后，这些文库中的DNA在通过flowcell的时候会随机附着在flowcell表面的channel上。每个Flowcell有8个channel，每个channel的表面都附有很多接头，这些接头能和建库过程中加在DNA片段两端的接头相互配对（这就是为什么flowcell能吸附建库后的DNA的原因），并能支持DNA在其表面进行桥式PCR的扩增。&nbsp;&nbsp;&nbsp;&nbsp; （3）桥式PCR扩增与变性桥式PCR以Flowcell表面所固定的接头为模板，进行桥形扩增，如图4.a所示。经过不断的扩增和变性循环，最终每个DNA片段都将在各自的位置上集中成束，每一个束都含有单个DNA模板的很多分拷贝，进行这一过程的目的在于实现将碱基的信号强度放大，以达到测序所需的信号要求。&nbsp;（4）测序测序方法采用边合成边测序的方法。向反应体系中同时添加DNA聚合酶、接头引物和带有碱基特异荧光标记的4中dNTP（如同Sanger测序法）。这些dNTP的3&rsquo;-OH被化学方法所保护，因而每次只能添加一个dNTP。在dNTP被添加到合成链上后，所有未使用的游离dNTP和DNA聚合酶会被洗脱掉。接着，再加入激发荧光所需的缓冲液，用激光激发荧光信号，并有光学设备完成荧光信号的记录，最后利用计算机分析将光学信号转化为测序碱基。这样荧光信号记录完成后，再加入化学试剂淬灭荧光信号并去除dNTP 3&rsquo;-OH保护基团，以便能进行下一轮的测序反应。Illumina的这种测序技术每次只添加一个dNTP的特点能够很好的地解决同聚物长度的准确测量问题，它的主要测序错误来源是碱基的替换，目前它的测序错误率在1%-1.5%之间，测序周期以人类基因组重测序为例，30x测序深度大约为1周。&nbsp; 图4. Illumina测序流程 Roche 454 Roche 454测序系统是第一个商业化运营二代测序技术的平台。它的主要测序原理是（图5 abc）2：（1）DNA文库制备454测序系统的文件构建方式和illumina的不同，它是利用喷雾法将待测DNA打断成300-800bp长的小片段，并在片段两端加上不同的接头，或将待测DNA变性后用杂交引物进行PCR扩增，连接载体，构建单链DNA文库（图5a）。（2）Emulsion PCR （乳液PCR，其实是一个注水到油的独特过程）454当然DNA扩增过程也和illumina的截然不同，它将这些单链DNA结合在水油包被的直径约28um的磁珠上，并在其上面孵育、退火。乳液PCR最大的特点是可以形成数目庞大的独立反应空间以进行DNA扩增。其关键技术是&ldquo;注水到油&rdquo;（水包油），基本过程是在PCR反应前，将包含PCR所有反应成分的水溶液注入到高速旋转的矿物油表面，水溶液瞬间形成无数个被矿物油包裹的小水滴。这些小水滴就构成了独立的PCR反应空间。理想状态下，每个小水滴只含一个DNA模板和一个磁珠。这些被小水滴包被的磁珠表面含有与接头互补的DNA序列，因此这些单链DNA序列能够特异地结合在磁珠上。同时孵育体系中含有PCR反应试剂，所以保证了每个与磁珠结合的小片段都能独立进行PCR扩增，并且扩增产物仍可以结合到磁珠上。当反应完成后，可以破坏孵育体系并将带有DNA的磁珠富集下来。进过扩增，每个小片段都将被扩增约100万倍，从而达到下一步测序所要求的DNA量。（3）焦磷酸测序测序前需要先用一种聚合酶和单链结合蛋白处理带有DNA的磁珠，接着将磁珠放在一种PTP平板上。这种平板上特制有许多直径约为44um的小孔，每个小孔仅能容纳一个磁珠，通过这种方法来固定每个磁珠的位置，以便检测接下来的测序反应过程。 测序方法采用焦磷酸测序法，将一种比PTP板上小孔直径更小的磁珠放入小孔中，启动测序反应。测序反应以磁珠上大量扩增出的单链DNA为模板，每次反应加入一种dNTP进行合成反应。如果dNTP能与待测序列配对，则会在合成后释放焦磷酸基团。释放的焦磷酸基团会与反应体系中的ATP硫酸化学酶反应生成ATP。生成的ATP和荧光素酶共同氧化使测序反应中的荧光素分子并发出荧光，同时由PTP板另一侧的CCD照相机记录，最后通过计算机进行光信号处理而获得最终的测序结果。由于每一种dNTP在反应中产生的荧光颜色不同，因此可以根据荧光的颜色来判断被测分子的序列。反应结束后，游离的dNTP会在双磷酸酶的作用下降解ATP，从而导致荧光淬灭，以便使测序反应进入下一个循环。由于454测序技术中，每个测序反应都在PTP板上独立的小孔中进行，因而能大大降低相互间的干扰和测序偏差。454技术最大的优势在于其能获得较长的测序读长，当前454技术的平均读长可达400bp，并且454技术和illumina的Solexa和Hiseq技术不同，它最主要的一个缺点是无法准确测量同聚物的长度，如当序列中存在类似于PolyA的情况时，测序反应会一次加入多个T，而所加入的T的个数只能通过荧光强度推测获得，这就有可能导致结果不准确。也正是由于这一原因，454技术会在测序过程中引入插入和缺失的测序错误。&nbsp; 图5. Roche 454测序流程 Solid技术 Solid测序技术是ABI公司于2007年开始投入用于商业测序应用的仪器。它基于连接酶法，即利用DNA连接酶在连接过程之中测序（图6）2,4。它的原理是： 图6-a. Solid测序技术 （1）DNA文库构建 片段打断并在片段两端加上测序接头，连接载体，构建单链DNA文库。（2）Emulsion PCRSolid的PCR过程也和454的方法类似，同样采用小水滴emulsion PCR，但这些微珠比起454系统来说则要小得多，只有1um。在扩增的同时对扩增产物的3&rsquo;端进行修饰，这是为下一步的测序过程作的准备。3&rsquo;修饰的微珠会被沉积在一块玻片上。在微珠上样的过程中，沉积小室将每张玻片分成1个、4个或8个测序区域（图6-a）。Solid系统最大的优点就是每张玻片能容纳比454更高密度的微珠，在同一系统中轻松实现更高的通量。（3）连接酶测序这一步是Solid测序的独特之处。它并没有采用以前测序时所常用的DNA聚合酶，而是采用了连接酶。Solid连接反应的底物是8碱基单链荧光探针混合物，这里将其简单表示为：3&rsquo;-XXnnnzzz-5&rsquo;。连接反应中，这些探针按照碱基互补规则与单链DNA模板链配对。探针的5&rsquo;末端分别标记了CY5、Texas Red、CY3、6-FAM这4种颜色的荧光染料（图6-a）。这个8碱基单链荧光探针中，第1和第2位碱基（XX）上的碱基是确定的，并根据种类的不同在6-8位（zzz）上加上了不同的荧光标记。这是Solid的独特测序法，两个碱基确定一个荧光信号，相当于一次能决定两个碱基。这种测序方法也称之为两碱基测序法。当荧光探针能够与DNA模板链配对而连接上时，就会发出代表第1，2位碱基的荧光信号，图6-a和图6-b中的比色版所表示的是第1，2位碱基的不同组合与荧光颜色的关系。在记录下荧光信号后，通过化学方法在第5和第6位碱基之间进行切割，这样就能移除荧光信号，以便进行下一个位置的测序。不过值得注意的是，通过这种测序方法，每次测序的位置都相差5位。即第一次是第1、2位，第二次是第6、7位&hellip;&hellip;在测到末尾后，要将新合成的链变性，洗脱。接着用引物n-1进行第二轮测序。引物n-1与引物n的区别是，二者在与接头配对的位置上相差一个碱基（图6-a. 8）。也即是，通过引物n-1在引物n的基础上将测序位置往3&rsquo;端移动一个碱基位置，因而就能测定第0、1位和第5、6位&hellip;&hellip;第二轮测序完成，依此类推，直至第五轮测序，最终可以完成所有位置的碱基测序，并且每个位置的碱基均被检测了两次。该技术的读长在2&times;50bp，后续序列拼接同样比较复杂。由于双次检测，这一技术的原始测序准确性高达99.94%，而15x覆盖率时的准确性更是达到了99.999%，应该说是目前第二代测序技术中准确性最高的了。但在荧光解码阶段，鉴于其是双碱基确定一个荧光信号，因而一旦发生错误就容易产生连锁的解码错误。 图6-b. Solid测序技术 第三代测序技术测序技术在近两三年中又有新的里程碑。以PacBio公司的SMRT和Oxford Nanopore Technologies纳米孔单分子测序技术，被称之为第三代测序技术。与前两代相比，他们最大的特点就是单分子测序，测序过程无需进行PCR扩增。其中PacBio SMRT技术其实也应用了边合成边测序的思想5，并以SMRT芯片为测序载体。基本原理是： DNA聚合酶和模板结合,4色荧光标记 4 种碱基（即是dNTP）,在碱基配对阶段,不同碱基的加入,会发出不同光,根据光的波长与峰值可判断进入的碱基类型。同时这个 DNA 聚合酶是实现超长读长的关键之一,读长主要跟酶的活性保持有关,它主要受激光对其造成的损伤所影响。PacBio SMRT技术的一个关键是怎样将反应信号与周围游离碱基的强大荧光背景区别出来。他们利用的是ZMW（零模波导孔）原理：如同微波炉壁上可看到的很多密集小孔。小孔直径有考究,如果直径大于微波波长,能量就会在衍射效应的作用下穿透面板而泄露出来，从而与周围小孔相互干扰。如果孔径小于波长,能量不会辐射到周围，而是保持直线状态（光衍射的原理）,从而可起保护作用。同理,在一个反应管(SMRTCell:单分子实时反应孔)中有许多这样的圆形纳米小孔, 即 ZMW(零模波导孔),外径 100多纳米,比检测激光波长小(数百纳米),激光从底部打上去后不能穿透小孔进入上方溶液区,能量被限制在一个小范围(体积20X 10-21 L)里,正好足够覆盖需要检测的部分,使得信号仅来自这个小反应区域,孔外过多游离核苷酸单体依然留在黑暗中,从而实现将背景降到最低。另外，可以通过检测相邻两个碱基之间的测序时间，来检测一些碱基修饰情况，既如果碱基存在修饰，则通过聚合酶时的速度会减慢，相邻两峰之间的距离增大，可以通过这个来之间检测甲基化等信息（图7）。SMRT技术的测序速度很快，每秒约10个dNTP。但是，同时其测序错误率比较高（这几乎是目前单分子测序技术的通病），达到15%,但好在它的出错是随机的，并不会像第二代测序技术那样存在测序错误的偏向，因而可以通过多次测序来进行有效的纠错。 图7. PacBio SMRT测序原理 Oxford Nanopore Technologies公司所开发的纳米单分子测序技术与以往的测序技术皆不同，它是基于电信号而不是光信号的测序技术5。该技术的关键之一是，他们设计了一种特殊的纳米孔，孔内共价结合有分子接头。当DNA碱基通过纳米孔时，它们使电荷发生变化，从而短暂地影响流过纳米孔的电流强度（每种碱基所影响的电流变化幅度是不同的），灵敏的电子设备检测到这些变化从而鉴定所通过的碱基（图8）。该公司在去年基因组生物学技术进展年会(AGBT)上推出第一款商业化的纳米孔测序仪，引起了科学界的极大关注。纳米孔测序（和其他第三代测序技术）有望解决目前测序平台的不足，纳米孔测序的主要特点是：读长很长，大约在几十kb，甚至100 kb;错误率目前介于1%至4%，且是随机错误，而不是聚集在读取的两端;数据可实时读取;通量很高(30x人类基因组有望在一天内完成);起始DNA在测序过程中不被破坏;以及样品制备简单又便宜。理论上，它也能直接测序RNA。纳米孔单分子测序计算还有另一大特点，它能够直接读取出甲基化的胞嘧啶，而不必像传统方法那样对基因组进行bisulfite处理。这对于在基因组水平直接研究表观遗传相关现象有极大的帮助。并且改方法的测序准确性可达99.8%，而且一旦发现测序错误也能较容易地进行纠正。但目前似乎还没有应用该技术的相关报道。 图8. 纳米孔测序 其他测序技术目前还有一种基于半导体芯片的新一代革命性测序技术&mdash;&mdash;Ion Torrent6。该技术使用了一种布满小孔的高密度半导体芯片， 一个小孔就是一个测序反应池。当DNA聚合酶把核苷酸聚合到延伸中的DNA链上时，会释放出一个氢离子，反应池中的PH发生改变，位于池下的离子感受器感受到H+离子信号，H+离子信号再直接转化为数字信号，从而读出DNA序列（图9）。这一技术的发明人同时也是454测序技术的发明人之一&mdash;&mdash;Jonathan Rothberg，它的文库和样本制备跟454技术很像，甚至可以说就是454的翻版，只是测序过程中不是通过检测焦磷酸荧光显色，而是通过检测H+信号的变化来获得序列碱基信息。Ion Torrent相比于其他测序技术来说，不需要昂贵的物理成像等设备，因此，成本相对来说会低，体积也会比较小，同时操作也要更为简单，速度也相当快速，除了2天文库制作时间，整个上机测序可在2-3.5小时内完成，不过整个芯片的通量并不高，目前是10G左右，但非常适合小基因组和外显子验证的测序。&nbsp;&nbsp;&nbsp;&nbsp; 图9. Ion Torrent &nbsp;小结以上，对各代测序技术的原理做了简要的阐述，这三代测序技术的特点比较汇总在以下表1和表2中。其中测序成本，读长和通量是评估该测序技术先进与否的三个重要指标。第一代和第二代测序技术除了通量和成本上的差异之外，其测序核心原理（除Solid是边连接边测序之外）都是基于边合成边测序的思想。第二代测序技术的优点是成本较之一代大大下降，通量大大提升，但缺点是所引入PCR过程会在一定程度上增加测序的错误率，并且具有系统偏向性，同时读长也比较短。第三代测序技术是为了解决第二代所存在的缺点而开发的，它的根本特点是单分子测序，不需要任何PCR的过程，这是为了能有效避免因PCR偏向性而导致的系统错误，同时提高读长，并要保持二代技术的高通量，低成本的优点。 表1：测序技术的比较第X代公司平台名称测序方法检测方法大约读长(碱基数)优点相对局限性第一代ABI/生命技术公司3130xL-3730xL桑格-毛细管电泳测序法荧光/光学600-1000高读长，准确度一次性达标率高，能很好处理重复序列和多聚序列通量低；样品制备成本高，使之难以做大量的平行测序第一代贝克曼GeXP遗传分析系统桑格-毛细管电泳测序法荧光/光学600-1000高读长，准确度一次性达标率高，能很好处理重复序列和多聚序列；易小型化通量低；单个样品的制备成本相对较高第二代Roche/454基因组测序仪FLX系统焦磷酸测序法光学230-400在第二代中最高读长；比第一代的测序通量大样品制备较难；难于处理重复和同种碱基多聚区域；试剂冲洗带来错误累积；仪器昂贵第二代IlluminaHiSeq2000,HiSeq2500/MiSeq可逆链终止物和合成测序法荧光/光学2x150很高测序通量仪器昂贵；用于数据删节和分析的费用很高第二代ABI/Solid5500xlSolid系统连接测序法荧光/光学25-35很高测序通量；在广为接受的几种第二代平台中，所要拼接出人类基因组的试剂成本最低测序运行时间长；读长短，造成成本高，数据分析困难和基因组拼接困难；仪器昂贵第二代赫利克斯Heliscope单分子合成测序法荧光/光学25-30高通量；在第二代中属于单分子性质的测序技术读长短，推高了测序成本，降低了基因组拼接的质量；仪器非常昂贵第三代太平洋生物科学公司PacBio RS实时单分子DNA测序荧光/光学~1000高平均读长，比第一代的测序时间降低；不需要扩增；最长单个读长接近3000碱基并不能高效地将DNA聚合酶加到测序阵列中；准确性一次性达标的机会低（81-83%）；DNA聚合酶在阵列中降解；总体上每个碱基测序成本高（仪器昂贵）；第三代全基因组学公司GeXP遗传分析系统复合探针锚杂交和连接技术荧光/光学10在第三代中通量最高；在所有测序技术中，用于拼接一个人基因组的试剂成本最低；每个测序步骤独立，使错误的累积变得最低低读长；&nbsp;模板制备妨碍长重复序列区域测序；样品制备费事；尚无商业化供应的仪器第三代Ion Torrent/生命技术公司个人基因组测序仪（PGM）&nbsp;合成测序法以离子敏感场效应晶体管检测pH值变化100-200对核酸碱基的掺入可直接测定；在自然条件下进行DNA合成（不需要使用修饰过的碱基）一步步的洗脱过程可导致错误累积；阅读高重复和同种多聚序列时有潜在困难；第三代牛津纳米孔公司&nbsp;gridION纳米孔外切酶测序电流尚未定量有潜力达到高读长；可以成本生产纳米孔；无需荧光标记或光学手段切断的核苷酸可能被读错方向；难于生产出带多重平行孔的装置&nbsp;&nbsp;表2：主流测序机器的成本测序比较 以下图10展示了当前全球测序仪的分布情况。图中的几个热点区主要分布在中国的深圳（主要是华大），南欧，西欧和美国。 图10. 测序仪全球分布 参考文献 1. Sanger, F. &amp; Nicklen, S. DNA sequencing with chain-terminating. 74, 5463&ndash;5467 (1977).2. Mardis, E. R. Next-generation DNA sequencing methods. Annual review of genomics and human genetics 9, 387&ndash;402 (2008).3. Shendure, J. &amp; Ji, H. Next-generation DNA sequencing. Nature biotechnology 26, 1135&ndash;45 (2008).4. Metzker, M. L. Sequencing technologies - the next generation. Nature reviews. Genetics 11, 31&ndash;46 (2010).5. Niedringhaus, T. P., Milanova, D., Kerby, M. B., Snyder, M. P. &amp; Barron, A. E. Landscape of Next-Generation Sequencing Technologies. 4327&ndash;4341 (2011).6. Rothberg, J. M. et al. An integrated semiconductor device enabling non-optical genome sequencing. Nature 475, 348&ndash;52 (2011).&nbsp;]]></content>
      <categories>
        <category>生物信息</category>
        <category>基因组学</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>测序技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[测序量估计]]></title>
    <url>%2F2013%2F07%2F11%2F2013-07-11-Sequence-estimate.html</url>
    <content type="text"><![CDATA[考虑这样一个问题，如果要保证基因组上95%的区域其覆盖深度在30x以上的话，那么最低的平均测序深度应该是多少? 关于测序量的估计，对于做生物信息的人来讲应算是家常便饭了，多数时候我们都能直接根据以往项目的经验来获得，或是说的更具体些，在变异检测中一般要有25x以上的覆盖度才能得到一个比较靠谱的结果，于是以此为目的给出测序量的估计值；当然少数情况下也会有直接拍脑袋拍出一个值来的疯狂行为，不过嘛，虽说是拍脑袋，但也不是随便拍的，拍脑袋的背景靠的是身后丰富的经验。相对更好一些的估计方式就是直接模拟数据，不过总是用模拟数据还是让人觉得麻烦，最好是能不用花多少时间，也不用做很多的计算就能脱口给出。我想在这里说一下这种情况下我的解法。当然了并不一定完全准确，仅作交流，欢迎各位拍砖。 闲话说完，回到上面的问题，在不通过数据模拟也不拍脑袋的情况下，要如何才能估算出一个合理的值呢？其实在作出任何推断之前我们都应当要先有一个合理的前提假设，或者说是理论依据来作为后续分析的基础。我们都知道短序列测序的一个特点是，在理论情况下位点被覆盖到的深度符合泊松分布（测序没什么问题的话，实际的情形也相差不多），但实际上在这种情况下用正态分布来考虑也是合理的，作为一个估计值，误差也是能够接受的，这是我们的基础。之所以想用正态分布来考虑，是因为正态分布有许多方便于计算的性质。其中一个很有用的法则，就是68-95-99法则，意思就是距离均值一个标准差的区域围起来的面积大约是总体的68%，2个标准差的区域范围的面积是总体的95%，3个标准差区域范围占到了总体的99%，如果你自己想要验证这一法则也并不困难，只需做些积分就能算出来，但这里就不做计算了。如下图，均值用$\mu$表示，标准差用$\sigma$表示。 现在事情就很简单了，从图中我们可以看出，只要30x深度的位置在$-2\sigma$以下，那么就能达到理论的要求。要得到这一结果，问题就只剩下一个了，此时我们只需要知道测序深度分布的标准差就能粗略估计出此时我们所需要的最低平均测序深度。虽然这个标准差跟许多因素有关，这里以illumina公司的Hiseq系列测序仪为例子，依照以往基因组重测序的经验，$\sigma$约等于10x。那么，简单算一下，此刻，理论上我们只需要测50x就可以使得基因组上有97.7%的区域其覆盖深度在30x以上了，注意这里不是95%了，因为我们的区域实际上是$[-2\sigma, +\infty)$，而不是$[-2\sigma,+2\sigma]$! 再除掉一些边边角角的误差，50x这个值在这里应当是合理的了. 以上计算都是以正态分布为基础而做出的估计。当然了，如果一定要用泊松分布去推算也可以，只是运算起来会麻烦很多。此外，如果是不同系列或是不同公司的测序仪，&sigma;就不一定是10了。]]></content>
      <categories>
        <category>生物信息</category>
      </categories>
      <tags>
        <tag>NGS</tag>
        <tag>测序技术</tag>
        <tag>估计</tag>
      </tags>
  </entry>
</search>
